{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da55e5a3-cdd0-4b19-a405-6d25853d33f9",
   "metadata": {
    "id": "da55e5a3-cdd0-4b19-a405-6d25853d33f9"
   },
   "source": [
    "**Load Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83038a2e-883a-4da6-97bd-c5371f13c182",
   "metadata": {
    "id": "83038a2e-883a-4da6-97bd-c5371f13c182"
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from utility.utils import EarlyStopping, WarmUpLR, most_recent_folder, most_recent_weights, best_acc_weights, last_epoch\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, Subset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Analysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc13d1-90ff-4301-8dac-07977aaaddcf",
   "metadata": {
    "id": "bbdc13d1-90ff-4301-8dac-07977aaaddcf"
   },
   "source": [
    "**Seed & Device Setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b8539ad-3c66-4c94-abb2-fcc33f97d3ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b8539ad-3c66-4c94-abb2-fcc33f97d3ca",
    "outputId": "ed6e73ba-1056-4f62-fc55-15b56acfe3c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # GPU 여러 개 사용할 경우 사용\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b5e51",
   "metadata": {
    "id": "a35b5e51"
   },
   "source": [
    "**Wandb Setting & Set Hyperparameters (수정 구간)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce35ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "d5ce35ad",
    "outputId": "26c52ce7-bcef-496e-fd7d-d9ea23426c27"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\" : 128,\n",
    "    \"num_epochs\" : 300,\n",
    "    \"learning_rate\" : 0.1,\n",
    "    \"momentum\" : 0.9,\n",
    "    \"weight_decay\" : 5e-4,\n",
    "    \"nesterov\" : True,\n",
    "    \"gamma\" : 0.4,\n",
    "    \"warm\" : 1,\n",
    "    \"patience\" : 150,\n",
    "    \"plateau_patience\" : 15,\n",
    "    \"milestones\" : [120, 190, 220, 260, 280],\n",
    "    \"scheduler\" : \"MultiStepLR\",\n",
    "    \"pin_memory\" : True,\n",
    "\n",
    "    \"resume\" : False,\n",
    "    \"model_name\" : \"pyramidnet_cutmix\",\n",
    "    \"stratified_data\" : True,\n",
    "    \"alpha\": 48,\n",
    "    \"depth\": 110\n",
    "}\n",
    "if config[\"resume\"] == True:\n",
    "    config[\"warm\"] = 0\n",
    "model_name = config[\"model_name\"]\n",
    "wandb.init(project=\"CIFAR-100_Classification\", name=config[\"model_name\"], config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d1e41f",
   "metadata": {
    "id": "a8d1e41f"
   },
   "outputs": [],
   "source": [
    "fine_to_superclass = {\n",
    "    4: 0, 30: 0, 55: 0, 72: 0, 95: 0,       # aquatic mammals\n",
    "    1: 1, 32: 1, 67: 1, 73: 1, 91: 1,       # fish\n",
    "    54: 2, 62: 2, 70: 2, 82: 2, 92: 2,      # flowers\n",
    "    9: 3, 10: 3, 16: 3, 28: 3, 61: 3,       # food containers\n",
    "    0: 4, 51: 4, 53: 4, 57: 4, 83: 4,       # fruit and vegetables\n",
    "    22: 5, 39: 5, 40: 5, 86: 5, 87: 5,      # household electrical devices\n",
    "    5: 6, 20: 6, 25: 6, 84: 6, 94: 6,       # household furniture\n",
    "    6: 7, 7: 7, 14: 7, 18: 7, 24: 7,        # insects\n",
    "    3: 8, 42: 8, 43: 8, 88: 8, 97: 8,       # large carnivores\n",
    "    12: 9, 17: 9, 37: 9, 68: 9, 76: 9,      # large man-made outdoor things\n",
    "    23: 10, 33: 10, 49: 10, 60: 10, 71: 10, # large natural outdoor scenes\n",
    "    15: 11, 19: 11, 21: 11, 31: 11, 38: 11, # large omnivores and herbivores\n",
    "    34: 12, 63: 12, 64: 12, 66: 12, 75: 12, # medium-sized mammals\n",
    "    26: 13, 45: 13, 77: 13, 79: 13, 99: 13, # non-insect invertebrates\n",
    "    2: 14, 11: 14, 35: 14, 46: 14, 98: 14,  # people\n",
    "    27: 15, 29: 15, 44: 15, 78: 15, 93: 15, # reptiles\n",
    "    36: 16, 50: 16, 65: 16, 74: 16, 80: 16, # small mammals\n",
    "    47: 17, 52: 17, 56: 17, 59: 17, 96: 17, # trees\n",
    "    8: 18, 13: 18, 48: 18, 58: 18, 90: 18,  # vehicles 1\n",
    "    41: 19, 69: 19, 81: 19, 85: 19, 89: 19  # vehicles 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e04e253-49c0-4c37-8260-5e02f2c09452",
   "metadata": {
    "id": "3e04e253-49c0-4c37-8260-5e02f2c09452"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZKl6oKBffJYA",
   "metadata": {
    "id": "ZKl6oKBffJYA"
   },
   "source": [
    "**Data Augementation (수정 구간)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93758c0c-b1d2-4ab9-afb4-bdd60a04dbd3",
   "metadata": {
    "id": "93758c0c-b1d2-4ab9-afb4-bdd60a04dbd3"
   },
   "outputs": [],
   "source": [
    "CIFAR100_TRAIN_MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
    "CIFAR100_TRAIN_STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
    "\n",
    "train_val_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomRotation(degrees=15),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ade0a7-66ce-48f4-96b0-f4719beaf090",
   "metadata": {
    "id": "13ade0a7-66ce-48f4-96b0-f4719beaf090"
   },
   "source": [
    "**Splitting the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09b4a6e5-9118-449a-bfb8-d05f9d339304",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09b4a6e5-9118-449a-bfb8-d05f9d339304",
    "outputId": "f8325035-cd21-4715-b7a6-1e0d8d0dfcd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_val_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_val_transform)\n",
    "val_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=test_transform)\n",
    "test_data = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c9ab44-2b89-4d17-b90d-f42d1e8fe426",
   "metadata": {
    "id": "82c9ab44-2b89-4d17-b90d-f42d1e8fe426"
   },
   "outputs": [],
   "source": [
    "# Stratified 방식으로 train 데이터를 train/val로 나누기\n",
    "if config[\"stratified_data\"]:\n",
    "  labels = train_val_data.targets\n",
    "  stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "  for train_index, val_index in stratified_split.split(train_val_data.data, labels):\n",
    "      train_data = Subset(train_val_data, train_index)\n",
    "      val_data = Subset(val_data, val_index)\n",
    "else:\n",
    "  # RandomSampler 방식으로 train 데이터를 train/val로 나누기\n",
    "  num_train = len(train_val_data)\n",
    "  indices = list(range(num_train))\n",
    "  split = int(np.floor(0.2 * num_train))  # validation 데이터를 20%로 설정\n",
    "\n",
    "  np.random.shuffle(indices)\n",
    "  train_idx, val_idx = indices[split:], indices[:split]\n",
    "\n",
    "  train_sampler = SubsetRandomSampler(train_idx)\n",
    "  val_sampler = SubsetRandomSampler(val_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce676124-c073-4aec-a000-cb061ac3e10a",
   "metadata": {
    "id": "ce676124-c073-4aec-a000-cb061ac3e10a"
   },
   "source": [
    "**Define Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "824ddf7d-d749-456a-b717-e257489431b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "824ddf7d-d749-456a-b717-e257489431b9",
    "outputId": "8bfd89af-a57a-441c-9a14-d549ff2ef41d"
   },
   "outputs": [],
   "source": [
    "if config[\"stratified_data\"]:\n",
    "  train_loader = DataLoader(train_data, batch_size=config[\"batch_size\"], shuffle=True, num_workers=4, pin_memory=config[\"pin_memory\"])\n",
    "  val_loader = DataLoader(val_data, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=config[\"pin_memory\"])\n",
    "  test_loader = DataLoader(test_data, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=config[\"pin_memory\"])\n",
    "else:\n",
    "  train_loader = DataLoader(train_val_data, batch_size=config[\"batch_size\"], sampler=train_sampler, num_workers=4, pin_memory=config[\"pin_memory\"])\n",
    "  val_loader = DataLoader(train_val_data, batch_size=config[\"batch_size\"], shuffle=False, sampler=val_sampler, num_workers=4, pin_memory=config[\"pin_memory\"])\n",
    "  test_loader = DataLoader(test_data, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=config[\"pin_memory\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e300067-8688-4cb1-b34b-a9dcd504c860",
   "metadata": {
    "id": "0e300067-8688-4cb1-b34b-a9dcd504c860"
   },
   "source": [
    "**Model Initialize (수정 부분)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f588f85-942a-4180-9a04-eb8ad7f4401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CutMix에서 반환되는 label은 criterion을 계산할 때 \n",
    "# 두 개의 label을 (1 - lam) 비율로 가중치를 곱해 더하도록 수정\n",
    "def cutmix_criterion(outputs, labels):\n",
    "    if isinstance(labels, tuple):\n",
    "        labels1, labels2, lam = labels\n",
    "        loss = lam * F.cross_entropy(outputs, labels1) + (1 - lam) * F.cross_entropy(outputs, labels2)\n",
    "    else:\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a58ae19-69f4-49a5-b704-d878caa5d7c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a58ae19-69f4-49a5-b704-d878caa5d7c7",
    "outputId": "70f5d330-681b-4ffd-de0b-16232491cdc4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use: cuda:0\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0005, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'initial_lr': 0.1, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]}]\n"
     ]
    }
   ],
   "source": [
    "from models import resnet, wideresnet\n",
    "from models import pyramidNet\n",
    "\n",
    "print(\"use:\", device)\n",
    "\n",
    "# 모델 초기화\n",
    "model = pyramidNet.pyramidnet(config[\"depth\"], config[\"alpha\"])\n",
    "net = model\n",
    "# 모델을 GPU로 이동\n",
    "net.to(device)\n",
    "\n",
    "# print(summary(net, (3,224,224)))\n",
    "optimizer = optim.SGD(net.parameters(), lr=config[\"learning_rate\"], momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"], nesterov=config[\"nesterov\"])\n",
    "\n",
    "if config[\"scheduler\"] == \"MultiStepLR\":\n",
    "    train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, config[\"milestones\"], gamma=config[\"gamma\"]) #learning rate decay\n",
    "elif config[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "    train_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", factor=config[\"gamma\"], patience=config[\"plateau_patience\"])\n",
    "else:\n",
    "    print(\"No scheduler!!\")\n",
    "iter_per_epoch = len(train_loader)\n",
    "if config[\"warm\"]:\n",
    "    warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * config[\"warm\"])\n",
    "early_stopping = EarlyStopping(patience=config[\"patience\"], verbose=True)\n",
    "\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa195cf0-826c-44cd-88f2-a22403660dce",
   "metadata": {},
   "source": [
    "**CutMix 추가부분**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c2b0f5f-cc55-4db3-9450-809293898cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix(batch, alpha=1.0):\n",
    "    input, labels = batch\n",
    "    indices = torch.randperm(input.size(0))\n",
    "    shuffled_input = input[indices]\n",
    "    shuffled_labels = labels[indices]\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "\n",
    "    image_h, image_w = input.shape[2:]\n",
    "    cx = np.random.uniform(0, image_w)\n",
    "    cy = np.random.uniform(0, image_h)\n",
    "    w = image_w * np.sqrt(1 - lam)\n",
    "    h = image_h * np.sqrt(1 - lam)\n",
    "    x0 = int(np.round(max(cx - w / 2, 0)))\n",
    "    x1 = int(np.round(min(cx + w / 2, image_w)))\n",
    "    y0 = int(np.round(max(cy - h / 2, 0)))\n",
    "    y1 = int(np.round(min(cy + h / 2, image_h)))\n",
    "\n",
    "    input[:, :, y0:y1, x0:x1] = shuffled_input[:, :, y0:y1, x0:x1]\n",
    "    labels = (labels, shuffled_labels, lam)\n",
    "\n",
    "    return input, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49f37a-e104-42df-a9a6-6ba473a9881f",
   "metadata": {
    "id": "8b49f37a-e104-42df-a9a6-6ba473a9881f"
   },
   "source": [
    "**Model Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96ba42f4-6da7-4f0c-983b-b8b82da34879",
   "metadata": {
    "id": "96ba42f4-6da7-4f0c-983b-b8b82da34879"
   },
   "outputs": [],
   "source": [
    "# 모델 학습 함수\n",
    "def train_model(net, trainloader, optimizer, epoch):\n",
    "    net.train()\n",
    "    start = time.time()\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    superclass_correct = 0\n",
    "    superclass_total = 0\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        labels, inputs = labels.to(device), inputs.to(device)\n",
    "\n",
    "        if use_cutmix:\n",
    "            inputs, labels = cutmix((inputs, labels), alpha=1.0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 모델 예측\n",
    "        outputs = net(inputs)\n",
    "        loss = cutmix_criterion(outputs=outputs, labels=labels)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        labels1, labels2, lam = labels\n",
    "\n",
    "        top1_correct += (lam * torch.sum((preds == labels1).int()).item() + (1 - lam) * torch.sum((preds == labels2).int()).item())\n",
    "\n",
    "\n",
    "        _, top5_preds = outputs.topk(5, 1, True, True)\n",
    "        top5_correct += (lam * torch.sum((top5_preds.eq(labels1.view(-1, 1).expand_as(top5_preds))).int()).item() + \n",
    "                         (1 - lam) * torch.sum((top5_preds.eq(labels2.view(-1, 1).expand_as(top5_preds))).int()).item())\n",
    "        \n",
    "        total += labels1.size(0)\n",
    "\n",
    "        # Superclass 정확도 계산 (CutMix 적용)\n",
    "        super_preds = torch.tensor([fine_to_superclass[p.item()] for p in preds], dtype=torch.long)\n",
    "        super_labels1 = torch.tensor([fine_to_superclass[t.item()] for t in labels1], dtype=torch.long)\n",
    "        super_labels2 = torch.tensor([fine_to_superclass[t.item()] for t in labels2], dtype=torch.long)\n",
    "        \n",
    "        superclass_correct += (lam * torch.sum(super_preds == super_labels1).item() + \n",
    "                               (1 - lam) * torch.sum(super_preds == super_labels2).item())\n",
    "        superclass_total += super_labels1.size(0)\n",
    "\n",
    "        # 역전파 및 최적화\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        n_iter = (epoch - 1) * len(train_loader) + batch_idx + 1\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # 30번째 배치마다 상태 출력\n",
    "        if (batch_idx + 1) % 30 == 0:\n",
    "            print(f\"Batch [{batch_idx+1}/{len(trainloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # LR에 warmup 되는 게 안 보이지만 실제론 이루어지고 있음. batch로 돌 때 이루어짐.\n",
    "        if epoch <= config[\"warm\"]:\n",
    "            warmup_scheduler.step()\n",
    "\n",
    "\n",
    "    # Epoch당 평균 손실 계산 및 출력\n",
    "    epoch_loss = running_loss / total\n",
    "\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    finish = time.time()\n",
    "\n",
    "    # 세부 클래스 및 슈퍼 클래스 정확도 계산\n",
    "    top1_acc = top1_correct / total\n",
    "    top5_acc = top5_correct / total\n",
    "    superclass_acc = superclass_correct / superclass_total\n",
    "    accuracy = [top1_acc, top5_acc, superclass_acc]\n",
    "\n",
    "    print(\"Train set: Epoch: {}, Average loss:{:.4f}, LR: {:.6f} Top-1 Accuracy: {:.4f}, Top-5 Accuracy: {:.4f}, SuperClass Accuracy: {:.4f}, Time consumed:{:.2f}s\".format(\n",
    "        epoch,\n",
    "        epoch_loss,\n",
    "        optimizer.param_groups[0]['lr'],\n",
    "        top1_acc,\n",
    "        top5_acc,\n",
    "        superclass_acc,\n",
    "        finish - start\n",
    "    ))\n",
    "\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "def eval_training(net, testloader, epoch):\n",
    "    net.eval()\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    superclass_correct = 0\n",
    "    superclass_total = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            if isinstance(labels, tuple):\n",
    "                labels1, labels2, lam = labels\n",
    "                top1_correct += (lam * torch.sum((preds == labels1).int()).item() + (1 - lam) * torch.sum((preds == labels2).int()).item())\n",
    "    \n",
    "                _, top5_preds = outputs.topk(5, 1, True, True)\n",
    "                top5_correct += (lam * torch.sum((top5_preds.eq(labels1.view(-1, 1).expand_as(top5_preds))).int()).item() + \n",
    "                                 (1 - lam) * torch.sum((top5_preds.eq(labels2.view(-1, 1).expand_as(top5_preds))).int()).item())\n",
    "            else:\n",
    "                top1_correct += torch.sum(preds == labels).item()\n",
    "                \n",
    "                _, top5_preds = outputs.topk(5, 1, True, True)\n",
    "                top5_correct += torch.sum(top5_preds.eq(labels.view(-1, 1).expand_as(top5_preds))).item()\n",
    "    \n",
    "            total += labels.size(0)\n",
    "            test_loss += cutmix_criterion(outputs, labels)\n",
    "\n",
    "            # Superclass accuracy (CutMix 적용)\n",
    "            if isinstance(labels, tuple):\n",
    "                super_preds = torch.tensor([fine_to_superclass[p.item()] for p in preds], dtype=torch.long)\n",
    "                super_labels1 = torch.tensor([fine_to_superclass[t.item()] for t in labels1], dtype=torch.long)\n",
    "                super_labels2 = torch.tensor([fine_to_superclass[t.item()] for t in labels2], dtype=torch.long)\n",
    "                \n",
    "                superclass_correct += (lam * torch.sum(super_preds == super_labels1).item() + \n",
    "                                       (1 - lam) * torch.sum(super_preds == super_labels2).item())\n",
    "                superclass_total += super_labels1.size(0)\n",
    "            else:\n",
    "                super_preds = torch.tensor([fine_to_superclass[p.item()] for p in preds], dtype=torch.long)\n",
    "                super_labels = torch.tensor([fine_to_superclass[t.item()] for t in labels], dtype=torch.long)\n",
    "                \n",
    "                superclass_correct += torch.sum(super_preds == super_labels).item()\n",
    "                superclass_total += super_labels.size(0)\n",
    "\n",
    "    finish = time.time()\n",
    "    average_loss = test_loss / total\n",
    "\n",
    "    # 세부 클래스 및 슈퍼 클래스 정확도 계산\n",
    "    top1_acc = top1_correct / total\n",
    "    top5_acc = top5_correct / total\n",
    "    superclass_acc = superclass_correct / superclass_total\n",
    "    accuracy = [top1_acc, top5_acc, superclass_acc]\n",
    "\n",
    "    print(\"Test set: Epoch: {}, Average loss:{:.4f}, Top-1 Accuracy: {:.4f}, Top-5 Accuracy: {:.4f}, SuperClass Accuracy: {:.4f}, Time consumed:{:.2f}s\".format(\n",
    "        epoch,\n",
    "        average_loss,\n",
    "        top1_acc,\n",
    "        top5_acc,\n",
    "        superclass_acc,\n",
    "        finish - start\n",
    "    ))\n",
    "    print()\n",
    "\n",
    "    return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e079b97-a215-4b55-aedd-c1297773426d",
   "metadata": {
    "id": "7e079b97-a215-4b55-aedd-c1297773426d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found best acc weights file:runs/pyramidnet_cutmix/savepoints/Wednesday_09_October_2024_12h_36m_55s/pyramidnet_cutmix-205-best.pth\n",
      "load best training file to test acc...\n",
      "Test set: Epoch: 0, Average loss:0.0117, Top-1 Accuracy: 0.6186, Top-5 Accuracy: 0.8850, SuperClass Accuracy: 0.7477, Time consumed:23.03s\n",
      "\n",
      "best acc is 0.01\n",
      "loading weights file runs/pyramidnet_cutmix/savepoints/Wednesday_09_October_2024_12h_36m_55s/pyramidnet_cutmix-205-best.pth to resume training......\n"
     ]
    }
   ],
   "source": [
    "DATE_FORMAT = '%A_%d_%B_%Y_%Hh_%Mm_%Ss'\n",
    "TIME_NOW = datetime.now().strftime(DATE_FORMAT)\n",
    "\n",
    "if config[\"resume\"]:\n",
    "    recent_folder = most_recent_folder(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\"), DATE_FORMAT)\n",
    "    if not recent_folder:\n",
    "        raise Exception(\"no recent folder were found\")\n",
    "\n",
    "    checkpoint_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder)\n",
    "else:\n",
    "    checkpoint_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", TIME_NOW)\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, '{net}-{epoch}-{type}.pth')\n",
    "\n",
    "best_acc = 0.0\n",
    "epoch = 0\n",
    "weights_path = False\n",
    "\n",
    "if config[\"resume\"]:\n",
    "    best_weights = best_acc_weights(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder))\n",
    "    if best_weights:\n",
    "        weights_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder, best_weights)\n",
    "        print('found best acc weights file:{}'.format(weights_path))\n",
    "        print('load best training file to test acc...')\n",
    "        net.load_state_dict(torch.load(weights_path))\n",
    "        _, best_acc = eval_training(net, test_loader, epoch)\n",
    "        print(\"best acc is {:0.2f}\".format(best_acc[0]))\n",
    "\n",
    "    recent_weights_file = most_recent_weights(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder))\n",
    "    if not recent_weights_file:\n",
    "        raise Exception(\"no recent weights file were found\")\n",
    "    weights_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder, recent_weights_file)\n",
    "    print(\"loading weights file {} to resume training......\".format(weights_path))\n",
    "    net.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "    resume_epoch = last_epoch(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder))\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"learning_rate\"], momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"], nesterov=config[\"nesterov\"])\n",
    "\n",
    "    if config[\"scheduler\"] == \"MultiStepLR\":\n",
    "        train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, config[\"milestones\"], gamma=config[\"gamma\"]) #learning rate decay\n",
    "    elif config[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "        train_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", factor=config[\"gamma\"], patience=config[\"plateau_patience\"])\n",
    "    else:\n",
    "        print(\"No scheduler!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2e2e73-edc7-4368-910e-e6a71133dd81",
   "metadata": {
    "id": "4a2e2e73-edc7-4368-910e-e6a71133dd81"
   },
   "source": [
    "**Per-Epoch Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c463ddea-8731-4e48-8ba1-23e2961b164a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c463ddea-8731-4e48-8ba1-23e2961b164a",
    "outputId": "c22e6d65-3707-478e-ceb6-b56881f0c945",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/300 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [30/313], Loss: 3.3871\n",
      "Batch [60/313], Loss: 2.3762\n",
      "Batch [90/313], Loss: 2.7042\n",
      "Batch [120/313], Loss: 2.5588\n",
      "Batch [150/313], Loss: 2.8380\n",
      "Batch [180/313], Loss: 1.6006\n",
      "Batch [210/313], Loss: 2.3573\n",
      "Batch [240/313], Loss: 2.0872\n",
      "Batch [270/313], Loss: 3.0832\n",
      "Batch [300/313], Loss: 2.2601\n",
      "Train set: Epoch: 206, Average loss:2.8575, LR: 0.000000 Top-1 Accuracy: 0.3382, Top-5 Accuracy: 0.6205, SuperClass Accuracy: 0.4448, Time consumed:292.58s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|████████████████████████████▏            | 206/300 [05:15<02:23,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 206, Average loss:0.0117, Top-1 Accuracy: 0.6223, Top-5 Accuracy: 0.8867, SuperClass Accuracy: 0.7564, Time consumed:22.74s\n",
      "\n",
      "saving weights file to runs/pyramidnet_cutmix/savepoints/Wednesday_09_October_2024_12h_36m_55s/pyramidnet_cutmix-206-best.pth\n",
      "Batch [30/313], Loss: 2.5026\n",
      "Batch [60/313], Loss: 2.5503\n",
      "Batch [90/313], Loss: 4.0575\n",
      "Batch [120/313], Loss: 2.6646\n",
      "Batch [150/313], Loss: 3.0923\n",
      "Batch [180/313], Loss: 3.4716\n",
      "Batch [210/313], Loss: 3.3966\n",
      "Batch [240/313], Loss: 2.0999\n",
      "Batch [270/313], Loss: 2.4564\n",
      "Batch [300/313], Loss: 3.1548\n",
      "Train set: Epoch: 207, Average loss:2.7957, LR: 0.000000 Top-1 Accuracy: 0.3530, Top-5 Accuracy: 0.6360, SuperClass Accuracy: 0.4627, Time consumed:338.80s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|████████████████████████████▎            | 207/300 [11:17<06:13,  4.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 207, Average loss:0.0115, Top-1 Accuracy: 0.6229, Top-5 Accuracy: 0.8867, SuperClass Accuracy: 0.7559, Time consumed:22.76s\n",
      "\n",
      "saving weights file to runs/pyramidnet_cutmix/savepoints/Wednesday_09_October_2024_12h_36m_55s/pyramidnet_cutmix-207-best.pth\n",
      "Batch [30/313], Loss: 2.8707\n",
      "Batch [60/313], Loss: 3.2983\n",
      "Batch [90/313], Loss: 2.2811\n",
      "Batch [120/313], Loss: 2.9062\n",
      "Batch [150/313], Loss: 3.1974\n",
      "Batch [180/313], Loss: 3.1085\n",
      "Batch [210/313], Loss: 2.0469\n",
      "Batch [240/313], Loss: 3.2323\n",
      "Batch [270/313], Loss: 2.9525\n",
      "Batch [300/313], Loss: 2.5595\n",
      "Train set: Epoch: 208, Average loss:2.7812, LR: 0.000000 Top-1 Accuracy: 0.3545, Top-5 Accuracy: 0.6385, SuperClass Accuracy: 0.4646, Time consumed:347.23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|████████████████████████████▍            | 208/300 [17:23<11:34,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 208, Average loss:0.0114, Top-1 Accuracy: 0.6225, Top-5 Accuracy: 0.8891, SuperClass Accuracy: 0.7566, Time consumed:19.66s\n",
      "\n",
      "Batch [30/313], Loss: 3.2260\n",
      "Batch [60/313], Loss: 2.9827\n",
      "Batch [90/313], Loss: 2.0926\n",
      "Batch [120/313], Loss: 3.1589\n",
      "Batch [150/313], Loss: 2.5797\n",
      "Batch [180/313], Loss: 3.0923\n",
      "Batch [210/313], Loss: 2.5308\n",
      "Batch [240/313], Loss: 3.3128\n",
      "Batch [270/313], Loss: 3.5614\n",
      "Batch [300/313], Loss: 2.5716\n",
      "Train set: Epoch: 209, Average loss:2.8085, LR: 0.000000 Top-1 Accuracy: 0.3515, Top-5 Accuracy: 0.6335, SuperClass Accuracy: 0.4610, Time consumed:332.44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████▌            | 209/300 [23:19<18:41, 12.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 209, Average loss:0.0115, Top-1 Accuracy: 0.6230, Top-5 Accuracy: 0.8873, SuperClass Accuracy: 0.7573, Time consumed:22.90s\n",
      "\n",
      "saving weights file to runs/pyramidnet_cutmix/savepoints/Wednesday_09_October_2024_12h_36m_55s/pyramidnet_cutmix-209-best.pth\n",
      "EarlyStopping counter: 1 out of 150\n",
      "Batch [30/313], Loss: 3.1001\n",
      "Batch [60/313], Loss: 2.4744\n",
      "Batch [90/313], Loss: 2.5690\n",
      "Batch [120/313], Loss: 1.8831\n",
      "Batch [150/313], Loss: 2.9961\n",
      "Batch [180/313], Loss: 2.9051\n",
      "Batch [210/313], Loss: 2.7257\n",
      "Batch [240/313], Loss: 3.1979\n",
      "Batch [270/313], Loss: 2.9834\n",
      "Batch [300/313], Loss: 3.3693\n",
      "Train set: Epoch: 210, Average loss:2.7958, LR: 0.000000 Top-1 Accuracy: 0.3555, Top-5 Accuracy: 0.6374, SuperClass Accuracy: 0.4657, Time consumed:329.11s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████▋            | 210/300 [29:11<28:17, 18.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 210, Average loss:0.0118, Top-1 Accuracy: 0.6191, Top-5 Accuracy: 0.8863, SuperClass Accuracy: 0.7535, Time consumed:22.98s\n",
      "\n",
      "EarlyStopping counter: 2 out of 150\n",
      "Batch [30/313], Loss: 2.2347\n",
      "Batch [60/313], Loss: 2.7620\n",
      "Batch [90/313], Loss: 1.7385\n",
      "Batch [120/313], Loss: 2.7052\n",
      "Batch [150/313], Loss: 2.8652\n",
      "Batch [180/313], Loss: 3.2362\n",
      "Batch [210/313], Loss: 2.2420\n",
      "Batch [240/313], Loss: 3.1434\n",
      "Batch [270/313], Loss: 2.2375\n",
      "Batch [300/313], Loss: 3.1126\n",
      "Train set: Epoch: 211, Average loss:2.7772, LR: 0.000000 Top-1 Accuracy: 0.3582, Top-5 Accuracy: 0.6410, SuperClass Accuracy: 0.4695, Time consumed:357.46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████▊            | 211/300 [35:30<42:14, 28.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 211, Average loss:0.0116, Top-1 Accuracy: 0.6249, Top-5 Accuracy: 0.8869, SuperClass Accuracy: 0.7594, Time consumed:21.24s\n",
      "\n",
      "saving weights file to runs/pyramidnet_cutmix/savepoints/Wednesday_09_October_2024_12h_36m_55s/pyramidnet_cutmix-211-best.pth\n",
      "EarlyStopping counter: 3 out of 150\n",
      "Batch [30/313], Loss: 2.0052\n",
      "Batch [60/313], Loss: 2.7879\n",
      "Batch [90/313], Loss: 3.0547\n",
      "Batch [120/313], Loss: 3.0511\n",
      "Batch [150/313], Loss: 3.0522\n",
      "Batch [180/313], Loss: 2.6013\n",
      "Batch [210/313], Loss: 2.7042\n",
      "Batch [240/313], Loss: 3.1259\n",
      "Batch [270/313], Loss: 3.0427\n",
      "Batch [300/313], Loss: 2.6886\n",
      "Train set: Epoch: 212, Average loss:2.8039, LR: 0.000000 Top-1 Accuracy: 0.3504, Top-5 Accuracy: 0.6360, SuperClass Accuracy: 0.4610, Time consumed:335.06s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|████████████████████████████▉            | 212/300 [41:27<59:31, 40.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 212, Average loss:0.0117, Top-1 Accuracy: 0.6223, Top-5 Accuracy: 0.8880, SuperClass Accuracy: 0.7570, Time consumed:22.45s\n",
      "\n",
      "EarlyStopping counter: 4 out of 150\n",
      "Batch [30/313], Loss: 3.0282\n",
      "Batch [60/313], Loss: 2.9220\n",
      "Batch [90/313], Loss: 2.8026\n",
      "Batch [120/313], Loss: 3.3549\n",
      "Batch [150/313], Loss: 1.6080\n",
      "Batch [180/313], Loss: 2.1568\n",
      "Batch [210/313], Loss: 2.8581\n",
      "Batch [240/313], Loss: 2.5262\n",
      "Batch [270/313], Loss: 3.1851\n",
      "Batch [300/313], Loss: 2.2480\n",
      "Train set: Epoch: 213, Average loss:2.7978, LR: 0.000000 Top-1 Accuracy: 0.3493, Top-5 Accuracy: 0.6368, SuperClass Accuracy: 0.4597, Time consumed:354.78s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████████████████████████▋           | 213/300 [47:45<1:23:16, 57.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 213, Average loss:0.0116, Top-1 Accuracy: 0.6234, Top-5 Accuracy: 0.8886, SuperClass Accuracy: 0.7575, Time consumed:23.02s\n",
      "\n",
      "EarlyStopping counter: 5 out of 150\n",
      "Batch [30/313], Loss: 1.4899\n",
      "Batch [60/313], Loss: 2.0064\n",
      "Batch [90/313], Loss: 3.1633\n",
      "Batch [120/313], Loss: 3.3914\n",
      "Batch [150/313], Loss: 3.2359\n",
      "Batch [180/313], Loss: 3.0854\n",
      "Batch [210/313], Loss: 3.3859\n",
      "Batch [240/313], Loss: 3.2282\n",
      "Batch [270/313], Loss: 1.8539\n",
      "Batch [300/313], Loss: 2.7971\n",
      "Train set: Epoch: 214, Average loss:2.8286, LR: 0.000000 Top-1 Accuracy: 0.3458, Top-5 Accuracy: 0.6288, SuperClass Accuracy: 0.4563, Time consumed:320.29s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████████████████████████▊           | 214/300 [53:28<1:49:32, 76.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 214, Average loss:0.0115, Top-1 Accuracy: 0.6244, Top-5 Accuracy: 0.8883, SuperClass Accuracy: 0.7577, Time consumed:22.31s\n",
      "\n",
      "EarlyStopping counter: 6 out of 150\n",
      "Batch [30/313], Loss: 2.3451\n",
      "Batch [60/313], Loss: 2.9397\n",
      "Batch [90/313], Loss: 3.0925\n",
      "Batch [120/313], Loss: 1.5443\n",
      "Batch [150/313], Loss: 2.9272\n",
      "Batch [180/313], Loss: 2.2676\n",
      "Batch [210/313], Loss: 3.2567\n",
      "Batch [240/313], Loss: 3.0475\n",
      "Batch [270/313], Loss: 2.8383\n",
      "Batch [300/313], Loss: 3.2636\n",
      "Train set: Epoch: 215, Average loss:2.8165, LR: 0.000000 Top-1 Accuracy: 0.3465, Top-5 Accuracy: 0.6334, SuperClass Accuracy: 0.4566, Time consumed:357.44s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████████████████████████▏          | 215/300 [59:48<2:25:38, 102.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 215, Average loss:0.0116, Top-1 Accuracy: 0.6191, Top-5 Accuracy: 0.8848, SuperClass Accuracy: 0.7537, Time consumed:22.73s\n",
      "\n",
      "EarlyStopping counter: 7 out of 150\n",
      "Batch [30/313], Loss: 2.7942\n",
      "Batch [60/313], Loss: 2.3632\n",
      "Batch [90/313], Loss: 1.5877\n",
      "Batch [120/313], Loss: 2.8755\n",
      "Batch [150/313], Loss: 2.7821\n",
      "Batch [180/313], Loss: 3.0242\n",
      "Batch [210/313], Loss: 2.5171\n",
      "Batch [240/313], Loss: 3.2394\n",
      "Batch [270/313], Loss: 2.1938\n",
      "Batch [300/313], Loss: 2.1497\n",
      "Train set: Epoch: 216, Average loss:2.8277, LR: 0.000000 Top-1 Accuracy: 0.3445, Top-5 Accuracy: 0.6269, SuperClass Accuracy: 0.4543, Time consumed:325.42s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|█████████████████████████▉          | 216/300 [1:05:36<3:01:49, 129.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 216, Average loss:0.0115, Top-1 Accuracy: 0.6222, Top-5 Accuracy: 0.8882, SuperClass Accuracy: 0.7549, Time consumed:22.56s\n",
      "\n",
      "EarlyStopping counter: 8 out of 150\n",
      "Batch [30/313], Loss: 3.0939\n",
      "Batch [60/313], Loss: 2.2824\n",
      "Batch [90/313], Loss: 1.6925\n",
      "Batch [120/313], Loss: 3.3833\n",
      "Batch [150/313], Loss: 3.0708\n",
      "Batch [180/313], Loss: 2.4018\n",
      "Batch [210/313], Loss: 3.1968\n",
      "Batch [240/313], Loss: 2.0052\n",
      "Batch [270/313], Loss: 2.5101\n",
      "Batch [300/313], Loss: 2.8962\n",
      "Train set: Epoch: 217, Average loss:2.8075, LR: 0.000000 Top-1 Accuracy: 0.3465, Top-5 Accuracy: 0.6331, SuperClass Accuracy: 0.4560, Time consumed:345.89s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|██████████████████████████          | 217/300 [1:11:45<3:44:41, 162.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 217, Average loss:0.0117, Top-1 Accuracy: 0.6242, Top-5 Accuracy: 0.8875, SuperClass Accuracy: 0.7569, Time consumed:22.97s\n",
      "\n",
      "EarlyStopping counter: 9 out of 150\n",
      "Batch [30/313], Loss: 2.8332\n",
      "Batch [60/313], Loss: 2.9553\n",
      "Batch [90/313], Loss: 2.8796\n",
      "Batch [120/313], Loss: 2.6201\n",
      "Batch [150/313], Loss: 3.3853\n",
      "Batch [180/313], Loss: 3.0745\n",
      "Batch [210/313], Loss: 2.8958\n",
      "Batch [240/313], Loss: 3.4697\n",
      "Batch [270/313], Loss: 2.9721\n",
      "Batch [300/313], Loss: 1.9998\n",
      "Train set: Epoch: 218, Average loss:2.8853, LR: 0.000000 Top-1 Accuracy: 0.3360, Top-5 Accuracy: 0.6182, SuperClass Accuracy: 0.4439, Time consumed:359.83s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|██████████████████████████▏         | 218/300 [1:18:07<4:31:01, 198.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 218, Average loss:0.0117, Top-1 Accuracy: 0.6211, Top-5 Accuracy: 0.8874, SuperClass Accuracy: 0.7558, Time consumed:22.80s\n",
      "\n",
      "EarlyStopping counter: 10 out of 150\n",
      "Batch [30/313], Loss: 2.9473\n",
      "Batch [60/313], Loss: 2.1088\n",
      "Batch [90/313], Loss: 2.9777\n",
      "Batch [120/313], Loss: 2.8658\n",
      "Batch [150/313], Loss: 3.0235\n",
      "Batch [180/313], Loss: 2.8055\n",
      "Batch [210/313], Loss: 2.0854\n",
      "Batch [240/313], Loss: 3.3646\n",
      "Batch [270/313], Loss: 2.8817\n",
      "Batch [300/313], Loss: 3.2181\n",
      "Train set: Epoch: 219, Average loss:2.7829, LR: 0.000000 Top-1 Accuracy: 0.3563, Top-5 Accuracy: 0.6399, SuperClass Accuracy: 0.4684, Time consumed:333.65s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|██████████████████████████▎         | 219/300 [1:24:04<5:07:58, 228.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 219, Average loss:0.0115, Top-1 Accuracy: 0.6220, Top-5 Accuracy: 0.8884, SuperClass Accuracy: 0.7563, Time consumed:22.65s\n",
      "\n",
      "EarlyStopping counter: 11 out of 150\n",
      "Batch [30/313], Loss: 2.7935\n",
      "Batch [60/313], Loss: 3.6092\n",
      "Batch [90/313], Loss: 3.6649\n"
     ]
    }
   ],
   "source": [
    "# CutMix 사용 여부 결정\n",
    "use_cutmix = True\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(1, config[\"num_epochs\"] + 1)):\n",
    "    if epoch > config[\"warm\"]:\n",
    "        if config[\"scheduler\"] == \"MultiStepLR\":\n",
    "            train_scheduler.step()\n",
    "        elif config[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "            train_scheduler.step(test_loss)\n",
    "\n",
    "    if config[\"resume\"]:\n",
    "        if epoch <= resume_epoch:\n",
    "            continue\n",
    "\n",
    "    epoch_loss, train_accuracy = train_model(net, train_loader, optimizer, epoch=epoch)\n",
    "    test_loss, test_accuracy = eval_training(net, val_loader, epoch)\n",
    "\n",
    "    if best_acc[0] < test_accuracy[0]:\n",
    "        if weights_path:  # 모델의 용량 이슈로 새로운 best 모델이 갱신되면 이전 best 모델 삭제\n",
    "            os.remove(weights_path)\n",
    "        weights_path = checkpoint_path.format(net=config[\"model_name\"], epoch=epoch, type=\"best\")\n",
    "        print(\"saving weights file to {}\".format(weights_path))\n",
    "        torch.save(net.state_dict(), weights_path)\n",
    "        best_acc = test_accuracy\n",
    "\n",
    "    early_stopping(test_loss, net)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "        \"train_loss\": epoch_loss,\n",
    "        \"train top-1 accuracy\": train_accuracy[0],\n",
    "        \"train top-5 accuracy\": train_accuracy[1],\n",
    "        \"train super-class accuracy\": train_accuracy[2],\n",
    "        \"val_loss\": test_loss,\n",
    "        \"valid top-1 accuracy\": test_accuracy[0],\n",
    "        \"valid top-5 accuracy\": test_accuracy[1],\n",
    "        \"valid super-class accuracy\": test_accuracy[2]\n",
    "    })\n",
    "\n",
    "end_time = time.time()\n",
    "wandb.log({\"train_time\": end_time - start_time})\n",
    "\n",
    "# Result\n",
    "print(f\"Result of best {model_name} = Epoch : {epoch}   Loss : {test_loss:.4f}   Top-1 Accuracy : {best_acc[0]*100:.4f}%  Top-5 Accuracy : {best_acc[1]*100:.4f}%   Super Accuracy : {best_acc[2]*100:.4f}%   Total_Accuracy : {sum(best_acc)*100:.4f}    Time : {end_time - start_time:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d3e551-4ee7-4a56-8ca9-a669e98ea815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end_time = time.time()\n",
    "# wandb.log({\"train_time\": end_time - start_time})\n",
    "\n",
    "# # Result\n",
    "# print(f\" Result of ResNet = Epoch : {epoch}   Loss : {test_loss:.4f}   Top-1 Accuracy : {test_accuracy[0]*100:.4f}%  Top-5 Accuracy : {test_accuracy[1]*100:.4f}%   Super Accuracy : {test_accuracy[2]*100:.4f}%   Total_Accuracy : {sum(test_accuracy)*100:.4f}    Time : {end_time - start_time:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p5YcRgjxvB92",
   "metadata": {
    "id": "p5YcRgjxvB92"
   },
   "source": [
    "**Best Model Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VM1-xrA_3Ug6",
   "metadata": {
    "id": "VM1-xrA_3Ug6"
   },
   "outputs": [],
   "source": [
    "def all_accuracy(net, test_loader, device):\n",
    "    net2.eval()\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    superclass_correct = 0\n",
    "    superclass_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (images, labels) in enumerate(test_loader):\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net2(images)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            top1_correct += torch.sum(preds == labels).item()\n",
    "\n",
    "            _, top5_preds = outputs.topk(5, 1, True, True)\n",
    "            top5_correct += torch.sum(top5_preds.eq(labels.view(-1, 1).expand_as(top5_preds))).item()\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Superclass accuracy\n",
    "            super_preds = torch.tensor([fine_to_superclass[p.item()] for p in preds], dtype=torch.long)\n",
    "            super_labels = torch.tensor([fine_to_superclass[t.item()] for t in labels], dtype=torch.long)\n",
    "            superclass_correct += torch.sum(super_preds == super_labels).item()\n",
    "            superclass_total += super_labels.size(0)\n",
    "\n",
    "    # 세부 클래스 및 슈퍼 클래스 정확도 계산\n",
    "    top1_acc = top1_correct / total\n",
    "    top5_acc = top5_correct / total\n",
    "    superclass_acc = superclass_correct / superclass_total\n",
    "    accuracy = [top1_acc, top5_acc, superclass_acc]\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rVneR6Xh_XfY",
   "metadata": {
    "id": "rVneR6Xh_XfY"
   },
   "outputs": [],
   "source": [
    "net2 = model\n",
    "net2.to(device)\n",
    "net2.eval()\n",
    "\n",
    "# 이 코드로 best 모델이 load 되지 않을 경우 아래 코드에서 경로 직접 지정\n",
    "recent_folder = most_recent_folder(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\"), DATE_FORMAT)\n",
    "if not recent_folder:\n",
    "    raise Exception(\"no recent folder were found\")\n",
    "\n",
    "best_weights = best_acc_weights(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder))\n",
    "if best_weights:\n",
    "    weights_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder, best_weights)\n",
    "    print('found best acc weights file:{}'.format(weights_path))\n",
    "    print('load best training file to test acc...')\n",
    "    net2.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "acc = all_accuracy(net2, test_loader, device)\n",
    "print(\"Top-1 accuracy: {:.4f}\".format(acc[0]))\n",
    "print(\"Top-5 accuracy: {:.4f}\".format(acc[1]))\n",
    "print(\"Super-Class accuracy: {:.4f}\".format(acc[2]))\n",
    "\n",
    "wandb.log({\n",
    "    \"Test Top-1 accuracy\": acc[0],\n",
    "    \"Test Top-5 accuracy\": acc[1],\n",
    "    \"Test Super-Class accuracy\": acc[2],\n",
    "    \"Total Score\": sum(acc)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3exQ4RCI34-1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3exQ4RCI34-1",
    "outputId": "6c819a3d-91ce-48d0-e7f1-495bed210b91"
   },
   "outputs": [],
   "source": [
    "# # 위 코드에서 에러가 발생해 경로 지정이 제대로 되지 않을 경우 사용\n",
    "\n",
    "# net2 = model\n",
    "# net2.to(device)\n",
    "# net2.load_state_dict(torch.load(\"runs/wide_resnet/savepoints/Tuesday_01_October_2024_13h_15m_52s/wide_resnet-229-best.pth\"))\n",
    "\n",
    "# acc = all_accuracy(net2, test_loader, device)\n",
    "# print(\"Top-1 accuracy: {:.4f}\".format(acc[0]))\n",
    "# print(\"Top-5 accuracy: {:.4f}\".format(acc[1]))\n",
    "# print(\"Super-Class accuracy: {:.4f}\".format(acc[2]))\n",
    "# wandb.log({\n",
    "#     \"Test Top-1 accuracy\": acc[0],\n",
    "#     \"Test Top-5 accuracy\": acc[1],\n",
    "#     \"Test Super-Class accuracy\": acc[2],\n",
    "#     \"Total Score\": sum(acc)\n",
    "#     })\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc7f408",
   "metadata": {
    "id": "cdc7f408"
   },
   "source": [
    "**Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8404d10-a77a-4544-96d1-70cb306e2a05",
   "metadata": {
    "id": "d8404d10-a77a-4544-96d1-70cb306e2a05"
   },
   "outputs": [],
   "source": [
    "import wandb.sklearn\n",
    "\n",
    "classes = train_val_data.classes\n",
    "coarse_classes = [\n",
    "    'aquatic mammals', 'fish', 'flowers', 'food containers', 'fruit and vegetables', 'household electrical devices',\n",
    "    'household furniture', 'insects', 'large carnivores', 'large man-made outdoor things',\n",
    "    'large natural outdoor scenes', 'large omnivores and herbivores', 'medium-sized mammals',\n",
    "    'non-insect invertebrates', 'people', 'reptiles', 'small mammals', 'trees', 'vehicles 1', 'vehicles 2'\n",
    "]\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_probs = []\n",
    "\n",
    "for x, y in torch.utils.data.DataLoader(dataset=test_data, batch_size=config[\"batch_size\"]):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    z = net2(x)\n",
    "    _, yhat = torch.max(z, 1)\n",
    "    probs = z.softmax(dim=1).cpu().detach().numpy()\n",
    "\n",
    "    pred = yhat.data.cpu().numpy()\n",
    "    \n",
    "    super_pred = [coarse_classes[fine_to_superclass[p]] for p in pred]\n",
    "    y_pred.extend(super_pred)\n",
    "\n",
    "    labels = y.data.cpu().numpy()\n",
    "    super_labels = [coarse_classes[fine_to_superclass[l]] for l in labels]\n",
    "    y_true.extend(super_labels)\n",
    "\n",
    "    y_probs.extend(probs)\n",
    "    \n",
    "cf_matrix = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix, index=coarse_classes,\n",
    "                     columns=coarse_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='.2f', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix of {config[\"model_name\"]} (CIFAR-100 Superclass)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Real')\n",
    "\n",
    "conf_matrix_img = wandb.Image(plt.gcf())\n",
    "plt.close()\n",
    "\n",
    "class_report = classification_report(y_true, y_pred, labels=coarse_classes, zero_division=0)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.text(0.01, 0.05, str(class_report), {'fontsize': 12}, fontproperties='monospace')  # monospaced font\n",
    "plt.axis('off')  # 축 제거\n",
    "plt.tight_layout()\n",
    "\n",
    "class_report_img = wandb.Image(plt.gcf())\n",
    "plt.close()\n",
    "\n",
    "wandb.log({\n",
    "    \"Confusion Matrix\": conf_matrix_img,\n",
    "    \"Classification Report\": class_report_img\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9076a5-82f4-4ec9-9903-5f20c093aa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
