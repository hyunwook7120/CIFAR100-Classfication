{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da55e5a3-cdd0-4b19-a405-6d25853d33f9",
   "metadata": {
    "id": "da55e5a3-cdd0-4b19-a405-6d25853d33f9"
   },
   "source": [
    "**Load Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83038a2e-883a-4da6-97bd-c5371f13c182",
   "metadata": {
    "id": "83038a2e-883a-4da6-97bd-c5371f13c182"
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from utility.utils import EarlyStopping, WarmUpLR, most_recent_folder, most_recent_weights, best_acc_weights, last_epoch\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, Subset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Analysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc13d1-90ff-4301-8dac-07977aaaddcf",
   "metadata": {
    "id": "bbdc13d1-90ff-4301-8dac-07977aaaddcf"
   },
   "source": [
    "**Seed & Device Setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b8539ad-3c66-4c94-abb2-fcc33f97d3ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b8539ad-3c66-4c94-abb2-fcc33f97d3ca",
    "outputId": "ed6e73ba-1056-4f62-fc55-15b56acfe3c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # GPU 여러 개 사용할 경우 사용\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b5e51",
   "metadata": {
    "id": "a35b5e51"
   },
   "source": [
    "**Wandb Setting & Set Hyperparameters (수정 구간)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5ce35ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "d5ce35ad",
    "outputId": "26c52ce7-bcef-496e-fd7d-d9ea23426c27"
   },
   "outputs": [],
   "source": [
    "# Scheduler 종류\n",
    "# \"ReduceLROnPlateau\", \"MultiStepLR\"\n",
    "\n",
    "config = {\n",
    "    \"batch_size\" : 128,\n",
    "    \"num_epochs\" : 250,\n",
    "    \"learning_rate\" : 0.1,\n",
    "    \"momentum\" : 0.9,\n",
    "    \"weight_decay\" : 5e-4,\n",
    "    \"nesterov\" : False,\n",
    "    \"gamma\" : 0.2,\n",
    "    \"warm\" : 1,\n",
    "    \"patience\" : 150,\n",
    "    \"plateau_patience\" : 15,\n",
    "    \"milestones\" : [100, 150, 200],\n",
    "    \"pin_memory\" : True,\n",
    "\n",
    "    \"resume\" : True,\n",
    "    \"model_name\" : \"resnet_18_cutmix\",\n",
    "    \"stratified_data\" : True,\n",
    "    \"scheduler\" : \"MultiStepLR\",\n",
    "}\n",
    "if config[\"resume\"] == True:\n",
    "    config[\"warm\"] = 0\n",
    "model_name = config[\"model_name\"]\n",
    "# wandb.init(project=\"CIFAR-100_Classification\", name=config[\"model_name\"], config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8d1e41f",
   "metadata": {
    "id": "a8d1e41f"
   },
   "outputs": [],
   "source": [
    "fine_to_superclass = {\n",
    "    4: 0, 30: 0, 55: 0, 72: 0, 95: 0,       # aquatic mammals\n",
    "    1: 1, 32: 1, 67: 1, 73: 1, 91: 1,       # fish\n",
    "    54: 2, 62: 2, 70: 2, 82: 2, 92: 2,      # flowers\n",
    "    9: 3, 10: 3, 16: 3, 28: 3, 61: 3,       # food containers\n",
    "    0: 4, 51: 4, 53: 4, 57: 4, 83: 4,       # fruit and vegetables\n",
    "    22: 5, 39: 5, 40: 5, 86: 5, 87: 5,      # household electrical devices\n",
    "    5: 6, 20: 6, 25: 6, 84: 6, 94: 6,       # household furniture\n",
    "    6: 7, 7: 7, 14: 7, 18: 7, 24: 7,        # insects\n",
    "    3: 8, 42: 8, 43: 8, 88: 8, 97: 8,       # large carnivores\n",
    "    12: 9, 17: 9, 37: 9, 68: 9, 76: 9,      # large man-made outdoor things\n",
    "    23: 10, 33: 10, 49: 10, 60: 10, 71: 10, # large natural outdoor scenes\n",
    "    15: 11, 19: 11, 21: 11, 31: 11, 38: 11, # large omnivores and herbivores\n",
    "    34: 12, 63: 12, 64: 12, 66: 12, 75: 12, # medium-sized mammals\n",
    "    26: 13, 45: 13, 77: 13, 79: 13, 99: 13, # non-insect invertebrates\n",
    "    2: 14, 11: 14, 35: 14, 46: 14, 98: 14,  # people\n",
    "    27: 15, 29: 15, 44: 15, 78: 15, 93: 15, # reptiles\n",
    "    36: 16, 50: 16, 65: 16, 74: 16, 80: 16, # small mammals\n",
    "    47: 17, 52: 17, 56: 17, 59: 17, 96: 17, # trees\n",
    "    8: 18, 13: 18, 48: 18, 58: 18, 90: 18,  # vehicles 1\n",
    "    41: 19, 69: 19, 81: 19, 85: 19, 89: 19  # vehicles 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e04e253-49c0-4c37-8260-5e02f2c09452",
   "metadata": {
    "id": "3e04e253-49c0-4c37-8260-5e02f2c09452"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZKl6oKBffJYA",
   "metadata": {
    "id": "ZKl6oKBffJYA"
   },
   "source": [
    "**Data Augementation (수정 구간)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93758c0c-b1d2-4ab9-afb4-bdd60a04dbd3",
   "metadata": {
    "id": "93758c0c-b1d2-4ab9-afb4-bdd60a04dbd3"
   },
   "outputs": [],
   "source": [
    "CIFAR100_TRAIN_MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
    "CIFAR100_TRAIN_STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
    "\n",
    "train_val_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomRotation(degrees=15),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ade0a7-66ce-48f4-96b0-f4719beaf090",
   "metadata": {
    "id": "13ade0a7-66ce-48f4-96b0-f4719beaf090"
   },
   "source": [
    "**Splitting the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09b4a6e5-9118-449a-bfb8-d05f9d339304",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09b4a6e5-9118-449a-bfb8-d05f9d339304",
    "outputId": "f8325035-cd21-4715-b7a6-1e0d8d0dfcd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_val_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_val_transform)\n",
    "val_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=test_transform)\n",
    "test_data = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c9ab44-2b89-4d17-b90d-f42d1e8fe426",
   "metadata": {
    "id": "82c9ab44-2b89-4d17-b90d-f42d1e8fe426"
   },
   "outputs": [],
   "source": [
    "# Stratified 방식으로 train 데이터를 train/val로 나누기\n",
    "if config[\"stratified_data\"]:\n",
    "  labels = train_val_data.targets\n",
    "  stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "  for train_index, val_index in stratified_split.split(train_val_data.data, labels):\n",
    "      train_data = Subset(train_val_data, train_index)\n",
    "      val_data = Subset(val_data, val_index)\n",
    "else:\n",
    "  # RandomSampler 방식으로 train 데이터를 train/val로 나누기\n",
    "  num_train = len(train_val_data)\n",
    "  indices = list(range(num_train))\n",
    "  split = int(np.floor(0.2 * num_train))  # validation 데이터를 20%로 설정\n",
    "\n",
    "  np.random.shuffle(indices)\n",
    "  train_idx, val_idx = indices[split:], indices[:split]\n",
    "\n",
    "  train_sampler = SubsetRandomSampler(train_idx)\n",
    "  val_sampler = SubsetRandomSampler(val_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce676124-c073-4aec-a000-cb061ac3e10a",
   "metadata": {
    "id": "ce676124-c073-4aec-a000-cb061ac3e10a"
   },
   "source": [
    "**Define Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "824ddf7d-d749-456a-b717-e257489431b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "824ddf7d-d749-456a-b717-e257489431b9",
    "outputId": "8bfd89af-a57a-441c-9a14-d549ff2ef41d"
   },
   "outputs": [],
   "source": [
    "if config[\"stratified_data\"]:\n",
    "  train_loader = DataLoader(train_data, batch_size=config[\"batch_size\"], shuffle=True, num_workers=4, pin_memory=config[\"pin_memory\"])\n",
    "  val_loader = DataLoader(val_data, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=config[\"pin_memory\"])\n",
    "  test_loader = DataLoader(test_data, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=config[\"pin_memory\"])\n",
    "else:\n",
    "  train_loader = DataLoader(train_val_data, batch_size=config[\"batch_size\"], sampler=train_sampler, num_workers=4, pin_memory=config[\"pin_memory\"])\n",
    "  val_loader = DataLoader(train_val_data, batch_size=config[\"batch_size\"], shuffle=False, sampler=val_sampler, num_workers=4, pin_memory=config[\"pin_memory\"])\n",
    "  test_loader = DataLoader(test_data, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=config[\"pin_memory\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e300067-8688-4cb1-b34b-a9dcd504c860",
   "metadata": {
    "id": "0e300067-8688-4cb1-b34b-a9dcd504c860"
   },
   "source": [
    "**Model Initialize (수정 부분)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f588f85-942a-4180-9a04-eb8ad7f4401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CutMix에서 반환되는 label은 criterion을 계산할 때 \n",
    "# 두 개의 label을 (1 - lam) 비율로 가중치를 곱해 더하도록 수정\n",
    "def cutmix_criterion(outputs, labels):\n",
    "    if isinstance(labels, tuple):\n",
    "        labels1, labels2, lam = labels\n",
    "        loss = lam * F.cross_entropy(outputs, labels1) + (1 - lam) * F.cross_entropy(outputs, labels2)\n",
    "    else:\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7a58ae19-69f4-49a5-b704-d878caa5d7c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a58ae19-69f4-49a5-b704-d878caa5d7c7",
    "outputId": "70f5d330-681b-4ffd-de0b-16232491cdc4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use: cuda:0\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0005, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'initial_lr': 0.1, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]}]\n"
     ]
    }
   ],
   "source": [
    "from models import resnet\n",
    "\n",
    "print(\"use:\", device)\n",
    "\n",
    "# 모델 초기화\n",
    "model = resnet.resnet18()\n",
    "net = model\n",
    "# 모델을 GPU로 이동\n",
    "net.to(device)\n",
    "\n",
    "# print(summary(net, (3,224,224)))\n",
    "optimizer = optim.SGD(net.parameters(), lr=config[\"learning_rate\"], momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"], nesterov=config[\"nesterov\"])\n",
    "\n",
    "if config[\"scheduler\"] == \"MultiStepLR\":\n",
    "    train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, config[\"milestones\"], gamma=config[\"gamma\"]) #learning rate decay\n",
    "elif config[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "    train_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", factor=config[\"gamma\"], patience=config[\"plateau_patience\"])\n",
    "else:\n",
    "    print(\"No scheduler!!\")\n",
    "iter_per_epoch = len(train_loader)\n",
    "if config[\"warm\"]:\n",
    "    warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * config[\"warm\"])\n",
    "early_stopping = EarlyStopping(patience=config[\"patience\"], verbose=True)\n",
    "\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa195cf0-826c-44cd-88f2-a22403660dce",
   "metadata": {},
   "source": [
    "**CutMix 추가부분**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c2b0f5f-cc55-4db3-9450-809293898cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutmix(batch, alpha=1.0):\n",
    "    input, labels = batch\n",
    "    indices = torch.randperm(input.size(0))\n",
    "    shuffled_input = input[indices]\n",
    "    shuffled_labels = labels[indices]\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "\n",
    "    image_h, image_w = input.shape[2:]\n",
    "    cx = np.random.uniform(0, image_w)\n",
    "    cy = np.random.uniform(0, image_h)\n",
    "    w = image_w * np.sqrt(1 - lam)\n",
    "    h = image_h * np.sqrt(1 - lam)\n",
    "    x0 = int(np.round(max(cx - w / 2, 0)))\n",
    "    x1 = int(np.round(min(cx + w / 2, image_w)))\n",
    "    y0 = int(np.round(max(cy - h / 2, 0)))\n",
    "    y1 = int(np.round(min(cy + h / 2, image_h)))\n",
    "\n",
    "    input[:, :, y0:y1, x0:x1] = shuffled_input[:, :, y0:y1, x0:x1]\n",
    "    labels = (labels, shuffled_labels, lam)\n",
    "\n",
    "    return input, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49f37a-e104-42df-a9a6-6ba473a9881f",
   "metadata": {
    "id": "8b49f37a-e104-42df-a9a6-6ba473a9881f"
   },
   "source": [
    "**Model Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96ba42f4-6da7-4f0c-983b-b8b82da34879",
   "metadata": {
    "id": "96ba42f4-6da7-4f0c-983b-b8b82da34879"
   },
   "outputs": [],
   "source": [
    "# 모델 학습 함수\n",
    "def train_model(net, trainloader, optimizer, epoch):\n",
    "    net.train()\n",
    "    start = time.time()\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    superclass_correct = 0\n",
    "    superclass_total = 0\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        labels, inputs = labels.to(device), inputs.to(device)\n",
    "\n",
    "        if use_cutmix:\n",
    "            inputs, labels = cutmix((inputs, labels), alpha=1.0)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 모델 예측\n",
    "        outputs = net(inputs)\n",
    "        loss = cutmix_criterion(outputs=outputs, labels=labels)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        labels1, labels2, lam = labels\n",
    "\n",
    "        top1_correct += (lam * torch.sum((preds == labels1).int()).item() + (1 - lam) * torch.sum((preds == labels2).int()).item())\n",
    "\n",
    "\n",
    "        _, top5_preds = outputs.topk(5, 1, True, True)\n",
    "        top5_correct += (lam * torch.sum((top5_preds.eq(labels1.view(-1, 1).expand_as(top5_preds))).int()).item() + \n",
    "                         (1 - lam) * torch.sum((top5_preds.eq(labels2.view(-1, 1).expand_as(top5_preds))).int()).item())\n",
    "        \n",
    "        total += labels1.size(0)\n",
    "\n",
    "        # Superclass 정확도 계산 (CutMix 적용)\n",
    "        super_preds = torch.tensor([fine_to_superclass[p.item()] for p in preds], dtype=torch.long)\n",
    "        super_labels1 = torch.tensor([fine_to_superclass[t.item()] for t in labels1], dtype=torch.long)\n",
    "        super_labels2 = torch.tensor([fine_to_superclass[t.item()] for t in labels2], dtype=torch.long)\n",
    "        \n",
    "        superclass_correct += (lam * torch.sum(super_preds == super_labels1).item() + \n",
    "                               (1 - lam) * torch.sum(super_preds == super_labels2).item())\n",
    "        superclass_total += super_labels1.size(0)\n",
    "\n",
    "        # 역전파 및 최적화\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        n_iter = (epoch - 1) * len(train_loader) + batch_idx + 1\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # 30번째 배치마다 상태 출력\n",
    "        if (batch_idx + 1) % 30 == 0:\n",
    "            print(f\"Batch [{batch_idx+1}/{len(trainloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # LR에 warmup 되는 게 안 보이지만 실제론 이루어지고 있음. batch로 돌 때 이루어짐.\n",
    "        if epoch <= config[\"warm\"]:\n",
    "            warmup_scheduler.step()\n",
    "\n",
    "\n",
    "    # Epoch당 평균 손실 계산 및 출력\n",
    "    epoch_loss = running_loss / total\n",
    "\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    finish = time.time()\n",
    "\n",
    "    # 세부 클래스 및 슈퍼 클래스 정확도 계산\n",
    "    top1_acc = top1_correct / total\n",
    "    top5_acc = top5_correct / total\n",
    "    superclass_acc = superclass_correct / superclass_total\n",
    "    accuracy = [top1_acc, top5_acc, superclass_acc]\n",
    "\n",
    "    print(\"Train set: Epoch: {}, Average loss:{:.4f}, LR: {:.6f} Top-1 Accuracy: {:.4f}, Top-5 Accuracy: {:.4f}, SuperClass Accuracy: {:.4f}, Time consumed:{:.2f}s\".format(\n",
    "        epoch,\n",
    "        epoch_loss,\n",
    "        optimizer.param_groups[0]['lr'],\n",
    "        top1_acc,\n",
    "        top5_acc,\n",
    "        superclass_acc,\n",
    "        finish - start\n",
    "    ))\n",
    "\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "def eval_training(net, testloader, epoch):\n",
    "    net.eval()\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    superclass_correct = 0\n",
    "    superclass_total = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            if isinstance(labels, tuple):\n",
    "                labels1, labels2, lam = labels\n",
    "                top1_correct += (lam * torch.sum((preds == labels1).int()).item() + (1 - lam) * torch.sum((preds == labels2).int()).item())\n",
    "    \n",
    "                _, top5_preds = outputs.topk(5, 1, True, True)\n",
    "                top5_correct += (lam * torch.sum((top5_preds.eq(labels1.view(-1, 1).expand_as(top5_preds))).int()).item() + \n",
    "                                 (1 - lam) * torch.sum((top5_preds.eq(labels2.view(-1, 1).expand_as(top5_preds))).int()).item())\n",
    "            else:\n",
    "                top1_correct += torch.sum(preds == labels).item()\n",
    "                \n",
    "                _, top5_preds = outputs.topk(5, 1, True, True)\n",
    "                top5_correct += torch.sum(top5_preds.eq(labels.view(-1, 1).expand_as(top5_preds))).item()\n",
    "    \n",
    "            total += labels.size(0)\n",
    "            test_loss += cutmix_criterion(outputs, labels)\n",
    "\n",
    "            # Superclass accuracy (CutMix 적용)\n",
    "            if isinstance(labels, tuple):\n",
    "                super_preds = torch.tensor([fine_to_superclass[p.item()] for p in preds], dtype=torch.long)\n",
    "                super_labels1 = torch.tensor([fine_to_superclass[t.item()] for t in labels1], dtype=torch.long)\n",
    "                super_labels2 = torch.tensor([fine_to_superclass[t.item()] for t in labels2], dtype=torch.long)\n",
    "                \n",
    "                superclass_correct += (lam * torch.sum(super_preds == super_labels1).item() + \n",
    "                                       (1 - lam) * torch.sum(super_preds == super_labels2).item())\n",
    "                superclass_total += super_labels1.size(0)\n",
    "            else:\n",
    "                super_preds = torch.tensor([fine_to_superclass[p.item()] for p in preds], dtype=torch.long)\n",
    "                super_labels = torch.tensor([fine_to_superclass[t.item()] for t in labels], dtype=torch.long)\n",
    "                \n",
    "                superclass_correct += torch.sum(super_preds == super_labels).item()\n",
    "                superclass_total += super_labels.size(0)\n",
    "\n",
    "    finish = time.time()\n",
    "    average_loss = test_loss / total\n",
    "\n",
    "    # 세부 클래스 및 슈퍼 클래스 정확도 계산\n",
    "    top1_acc = top1_correct / total\n",
    "    top5_acc = top5_correct / total\n",
    "    superclass_acc = superclass_correct / superclass_total\n",
    "    accuracy = [top1_acc, top5_acc, superclass_acc]\n",
    "\n",
    "    print(\"Test set: Epoch: {}, Average loss:{:.4f}, Top-1 Accuracy: {:.4f}, Top-5 Accuracy: {:.4f}, SuperClass Accuracy: {:.4f}, Time consumed:{:.2f}s\".format(\n",
    "        epoch,\n",
    "        average_loss,\n",
    "        top1_acc,\n",
    "        top5_acc,\n",
    "        superclass_acc,\n",
    "        finish - start\n",
    "    ))\n",
    "    print()\n",
    "\n",
    "    return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e079b97-a215-4b55-aedd-c1297773426d",
   "metadata": {
    "id": "7e079b97-a215-4b55-aedd-c1297773426d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found best acc weights file:runs\\resnet_18_base\\savepoints\\Sunday_06_October_2024_13h_21m_09s\\resnet_18_base-237-best.pth\n",
      "load best training file to test acc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhw03\\AppData\\Local\\Temp\\ipykernel_16720\\2803652068.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(weights_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set: Epoch: 0, Average loss:0.0077, Top-1 Accuracy: 0.7576, Top-5 Accuracy: 0.9286, SuperClass Accuracy: 0.8466, Time consumed:12.76s\n",
      "\n",
      "best acc is Top-1:0.76 Top-5:0.93 Super:0.85 Total:2.53\n",
      "loading weights file runs\\resnet_18_base\\savepoints\\Sunday_06_October_2024_13h_21m_09s\\resnet_18_base-237-best.pth to resume training......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhw03\\AppData\\Local\\Temp\\ipykernel_16720\\2803652068.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(weights_path))\n"
     ]
    }
   ],
   "source": [
    "DATE_FORMAT = '%A_%d_%B_%Y_%Hh_%Mm_%Ss'\n",
    "TIME_NOW = datetime.now().strftime(DATE_FORMAT)\n",
    "\n",
    "if config[\"resume\"]:\n",
    "    recent_folder = most_recent_folder(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\"), DATE_FORMAT)\n",
    "    if not recent_folder:\n",
    "        raise Exception(\"no recent folder were found\")\n",
    "\n",
    "    checkpoint_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder)\n",
    "else:\n",
    "    checkpoint_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", TIME_NOW)\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, '{net}-{epoch}-{type}.pth')\n",
    "\n",
    "best_acc = 0.0\n",
    "epoch = 0\n",
    "weights_path = False\n",
    "\n",
    "if config[\"resume\"]:\n",
    "    best_weights = best_acc_weights(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder))\n",
    "    if best_weights:\n",
    "        weights_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder, best_weights)\n",
    "        print('found best acc weights file:{}'.format(weights_path))\n",
    "        print('load best training file to test acc...')\n",
    "        net.load_state_dict(torch.load(weights_path))\n",
    "        _, best_acc = eval_training(net, test_loader, epoch)\n",
    "        print(\"best acc is Top-1:{:0.2f} Top-5:{:0.2f} Super:{:0.2f} Total:{:0.2f}\".format(best_acc[0], best_acc[1], best_acc[2], sum(best_acc)))\n",
    "\n",
    "    recent_weights_file = most_recent_weights(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder))\n",
    "    if not recent_weights_file:\n",
    "        raise Exception(\"no recent weights file were found\")\n",
    "    weights_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder, recent_weights_file)\n",
    "    print(\"loading weights file {} to resume training......\".format(weights_path))\n",
    "    net.load_state_dict(torch.load(weights_path))\n",
    "    resume_epoch = last_epoch(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder))\n",
    "\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"learning_rate\"], momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"], nesterov=config[\"nesterov\"])\n",
    "\n",
    "    if config[\"scheduler\"] == \"MultiStepLR\":\n",
    "        train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, config[\"milestones\"], gamma=config[\"gamma\"]) #learning rate decay\n",
    "    elif config[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "        train_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", factor=config[\"gamma\"], patience=config[\"plateau_patience\"])\n",
    "    else:\n",
    "        print(\"No scheduler!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2e2e73-edc7-4368-910e-e6a71133dd81",
   "metadata": {
    "id": "4a2e2e73-edc7-4368-910e-e6a71133dd81"
   },
   "source": [
    "**Per-Epoch Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c463ddea-8731-4e48-8ba1-23e2961b164a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c463ddea-8731-4e48-8ba1-23e2961b164a",
    "outputId": "c22e6d65-3707-478e-ceb6-b56881f0c945",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [30/313], Loss: 5.4668\n",
      "Batch [60/313], Loss: 4.9003\n",
      "Batch [90/313], Loss: 4.7327\n",
      "Batch [120/313], Loss: 4.6481\n",
      "Batch [150/313], Loss: 4.7037\n",
      "Batch [180/313], Loss: 4.5755\n",
      "Batch [210/313], Loss: 4.5462\n",
      "Batch [240/313], Loss: 4.6507\n",
      "Batch [270/313], Loss: 4.5341\n",
      "Batch [300/313], Loss: 4.6861\n",
      "Train set: Epoch: 238, Average loss:4.8501, LR: 0.000800 Top-1 Accuracy: 0.0259, Top-5 Accuracy: 0.0960, SuperClass Accuracy: 0.0900, Time consumed:37.10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 237/250 [00:40<00:02,  5.91it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     17\u001b[0m epoch_loss, train_accuracy \u001b[38;5;241m=\u001b[39m train_model(net, train_loader, optimizer, epoch\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[1;32m---> 18\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43meval_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_acc[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m test_accuracy[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m weights_path:  \u001b[38;5;66;03m# 모델의 용량 이슈로 새로운 best 모델이 갱신되면 이전 best 모델 삭제\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 100\u001b[0m, in \u001b[0;36meval_training\u001b[1;34m(net, testloader, epoch)\u001b[0m\n\u001b[0;32m     97\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m testloader:\n\u001b[0;32m    101\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    102\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m net(images)\n",
      "File \u001b[1;32mc:\\Users\\jhw03\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:440\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jhw03\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:388\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jhw03\\anaconda3\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1038\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1031\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1032\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1038\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\jhw03\\anaconda3\\envs\\dl\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jhw03\\anaconda3\\envs\\dl\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jhw03\\anaconda3\\envs\\dl\\lib\\multiprocessing\\context.py:327\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jhw03\\anaconda3\\envs\\dl\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\jhw03\\anaconda3\\envs\\dl\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# CutMix 사용 여부 결정\n",
    "use_cutmix = True\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(1, config[\"num_epochs\"] + 1)):\n",
    "    if epoch > config[\"warm\"]:\n",
    "        if config[\"scheduler\"] == \"MultiStepLR\":\n",
    "            train_scheduler.step()\n",
    "        elif config[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "            train_scheduler.step(test_loss)\n",
    "\n",
    "    if config[\"resume\"]:\n",
    "        if epoch <= resume_epoch:\n",
    "            continue\n",
    "\n",
    "    epoch_loss, train_accuracy = train_model(net, train_loader, optimizer, epoch=epoch)\n",
    "    test_loss, test_accuracy = eval_training(net, val_loader, epoch)\n",
    "\n",
    "    if best_acc[0] < test_accuracy[0]:\n",
    "        if weights_path:  # 모델의 용량 이슈로 새로운 best 모델이 갱신되면 이전 best 모델 삭제\n",
    "            os.remove(weights_path)\n",
    "        weights_path = checkpoint_path.format(net=config[\"model_name\"], epoch=epoch, type=\"best\")\n",
    "        print(\"saving weights file to {}\".format(weights_path))\n",
    "        torch.save(net.state_dict(), weights_path)\n",
    "        best_acc = test_accuracy\n",
    "\n",
    "    early_stopping(test_loss, net)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "        \"train_loss\": epoch_loss,\n",
    "        \"train top-1 accuracy\": train_accuracy[0],\n",
    "        \"train top-5 accuracy\": train_accuracy[1],\n",
    "        \"train super-class accuracy\": train_accuracy[2],\n",
    "        \"val_loss\": test_loss,\n",
    "        \"valid top-1 accuracy\": test_accuracy[0],\n",
    "        \"valid top-5 accuracy\": test_accuracy[1],\n",
    "        \"valid super-class accuracy\": test_accuracy[2]\n",
    "    })\n",
    "\n",
    "end_time = time.time()\n",
    "wandb.log({\"train_time\": end_time - start_time})\n",
    "\n",
    "# Result\n",
    "print(f\"Result of best {model_name} = Epoch : {epoch}   Loss : {test_loss:.4f}   Top-1 Accuracy : {best_acc[0]*100:.4f}%  Top-5 Accuracy : {best_acc[1]*100:.4f}%   Super Accuracy : {best_acc[2]*100:.4f}%   Total_Accuracy : {sum(best_acc)*100:.4f}    Time : {end_time - start_time:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d3e551-4ee7-4a56-8ca9-a669e98ea815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# end_time = time.time()\n",
    "# wandb.log({\"train_time\": end_time - start_time})\n",
    "\n",
    "# # Result\n",
    "# print(f\" Result of ResNet = Epoch : {epoch}   Loss : {test_loss:.4f}   Top-1 Accuracy : {test_accuracy[0]*100:.4f}%  Top-5 Accuracy : {test_accuracy[1]*100:.4f}%   Super Accuracy : {test_accuracy[2]*100:.4f}%   Total_Accuracy : {sum(test_accuracy)*100:.4f}    Time : {end_time - start_time:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p5YcRgjxvB92",
   "metadata": {
    "id": "p5YcRgjxvB92"
   },
   "source": [
    "**Best Model Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "VM1-xrA_3Ug6",
   "metadata": {
    "id": "VM1-xrA_3Ug6"
   },
   "outputs": [],
   "source": [
    "def all_accuracy(net, test_loader, device):\n",
    "    net2.eval()\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    superclass_correct = 0\n",
    "    superclass_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (images, labels) in enumerate(test_loader):\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net2(images)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            top1_correct += torch.sum(preds == labels).item()\n",
    "\n",
    "            _, top5_preds = outputs.topk(5, 1, True, True)\n",
    "            top5_correct += torch.sum(top5_preds.eq(labels.view(-1, 1).expand_as(top5_preds))).item()\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Superclass accuracy\n",
    "            super_preds = torch.tensor([fine_to_superclass[p.item()] for p in preds], dtype=torch.long)\n",
    "            super_labels = torch.tensor([fine_to_superclass[t.item()] for t in labels], dtype=torch.long)\n",
    "            superclass_correct += torch.sum(super_preds == super_labels).item()\n",
    "            superclass_total += super_labels.size(0)\n",
    "\n",
    "    # 세부 클래스 및 슈퍼 클래스 정확도 계산\n",
    "    top1_acc = top1_correct / total\n",
    "    top5_acc = top5_correct / total\n",
    "    superclass_acc = superclass_correct / superclass_total\n",
    "    accuracy = [top1_acc, top5_acc, superclass_acc]\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "rVneR6Xh_XfY",
   "metadata": {
    "id": "rVneR6Xh_XfY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found best acc weights file:runs/resnet_18_cutmix/savepoints/Wednesday_09_October_2024_08h_02m_49s/resnet_18_cutmix-234-best.pth\n",
      "load best training file to test acc...\n",
      "Top-1 accuracy: 0.7822\n",
      "Top-5 accuracy: 0.9444\n",
      "Super-Class accuracy: 0.8677\n"
     ]
    }
   ],
   "source": [
    "net2 = model\n",
    "net2.to(device)\n",
    "net2.eval()\n",
    "\n",
    "# 이 코드로 best 모델이 load 되지 않을 경우 아래 코드에서 경로 직접 지정\n",
    "recent_folder = most_recent_folder(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\"), DATE_FORMAT)\n",
    "if not recent_folder:\n",
    "    raise Exception(\"no recent folder were found\")\n",
    "\n",
    "best_weights = best_acc_weights(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder))\n",
    "if best_weights:\n",
    "    weights_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder, best_weights)\n",
    "    print('found best acc weights file:{}'.format(weights_path))\n",
    "    print('load best training file to test acc...')\n",
    "    net2.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "acc = all_accuracy(net2, test_loader, device)\n",
    "print(\"Top-1 accuracy: {:.4f}\".format(acc[0]))\n",
    "print(\"Top-5 accuracy: {:.4f}\".format(acc[1]))\n",
    "print(\"Super-Class accuracy: {:.4f}\".format(acc[2]))\n",
    "\n",
    "wandb.log({\n",
    "    \"Test Top-1 accuracy\": acc[0],\n",
    "    \"Test Top-5 accuracy\": acc[1],\n",
    "    \"Test Super-Class accuracy\": acc[2],\n",
    "    \"Total Score\": sum(acc)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3exQ4RCI34-1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3exQ4RCI34-1",
    "outputId": "6c819a3d-91ce-48d0-e7f1-495bed210b91"
   },
   "outputs": [],
   "source": [
    "# # 위 코드에서 에러가 발생해 경로 지정이 제대로 되지 않을 경우 사용\n",
    "\n",
    "# net2 = model\n",
    "# net2.to(device)\n",
    "# net2.load_state_dict(torch.load(\"runs/wide_resnet/savepoints/Tuesday_01_October_2024_13h_15m_52s/wide_resnet-229-best.pth\"))\n",
    "\n",
    "# acc = all_accuracy(net2, test_loader, device)\n",
    "# print(\"Top-1 accuracy: {:.4f}\".format(acc[0]))\n",
    "# print(\"Top-5 accuracy: {:.4f}\".format(acc[1]))\n",
    "# print(\"Super-Class accuracy: {:.4f}\".format(acc[2]))\n",
    "# wandb.log({\n",
    "#     \"Test Top-1 accuracy\": acc[0],\n",
    "#     \"Test Top-5 accuracy\": acc[1],\n",
    "#     \"Test Super-Class accuracy\": acc[2],\n",
    "#     \"Total Score\": sum(acc)\n",
    "#     })\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc7f408",
   "metadata": {
    "id": "cdc7f408"
   },
   "source": [
    "**Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8404d10-a77a-4544-96d1-70cb306e2a05",
   "metadata": {
    "id": "d8404d10-a77a-4544-96d1-70cb306e2a05"
   },
   "outputs": [],
   "source": [
    "import wandb.sklearn\n",
    "\n",
    "classes = train_val_data.classes\n",
    "coarse_classes = [\n",
    "    'aquatic mammals', 'fish', 'flowers', 'food containers', 'fruit and vegetables', 'household electrical devices',\n",
    "    'household furniture', 'insects', 'large carnivores', 'large man-made outdoor things',\n",
    "    'large natural outdoor scenes', 'large omnivores and herbivores', 'medium-sized mammals',\n",
    "    'non-insect invertebrates', 'people', 'reptiles', 'small mammals', 'trees', 'vehicles 1', 'vehicles 2'\n",
    "]\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_probs = []\n",
    "\n",
    "for x, y in torch.utils.data.DataLoader(dataset=test_data, batch_size=config[\"batch_size\"]):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    z = net2(x)\n",
    "    _, yhat = torch.max(z, 1)\n",
    "    probs = z.softmax(dim=1).cpu().detach().numpy()\n",
    "\n",
    "    pred = yhat.data.cpu().numpy()\n",
    "    \n",
    "    super_pred = [coarse_classes[fine_to_superclass[p]] for p in pred]\n",
    "    y_pred.extend(super_pred)\n",
    "\n",
    "    labels = y.data.cpu().numpy()\n",
    "    super_labels = [coarse_classes[fine_to_superclass[l]] for l in labels]\n",
    "    y_true.extend(super_labels)\n",
    "\n",
    "    y_probs.extend(probs)\n",
    "    \n",
    "cf_matrix = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix, index=coarse_classes,\n",
    "                     columns=coarse_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='.2f', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix of {config[\"model_name\"]} (CIFAR-100 Superclass)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Real')\n",
    "\n",
    "conf_matrix_img = wandb.Image(plt.gcf())\n",
    "plt.close()\n",
    "\n",
    "class_report = classification_report(y_true, y_pred, labels=coarse_classes, zero_division=0)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.text(0.01, 0.05, str(class_report), {'fontsize': 12}, fontproperties='monospace')  # monospaced font\n",
    "plt.axis('off')  # 축 제거\n",
    "plt.tight_layout()\n",
    "\n",
    "class_report_img = wandb.Image(plt.gcf())\n",
    "plt.close()\n",
    "\n",
    "wandb.log({\n",
    "    \"Confusion Matrix\": conf_matrix_img,\n",
    "    \"Classification Report\": class_report_img\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c9076a5-82f4-4ec9-9903-5f20c093aa6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Super-Class accuracy</td><td>▁</td></tr><tr><td>Test Top-1 accuracy</td><td>▁</td></tr><tr><td>Test Top-5 accuracy</td><td>▁</td></tr><tr><td>Total Score</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>learning_rate</td><td>█████████████▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train super-class accuracy</td><td>▁▃▄▄▅▅▅▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>train top-1 accuracy</td><td>▁▁▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇███████</td></tr><tr><td>train top-5 accuracy</td><td>▁▂▃▃▄▄▄▅▄▅▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▆▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█▇▇▆▆▅▅▅▅▅▄▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_time</td><td>▁</td></tr><tr><td>val_loss</td><td>███▇▇▇▆▆▆▇▇▂▂▂▃▃▃▂▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid super-class accuracy</td><td>▁▂▃▃▅▄▅▅▆▅▅▅▆▆▇▇▇▇▇▇▇▇▇█████████████████</td></tr><tr><td>valid top-1 accuracy</td><td>▁▃▄▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▄▇████████████████████</td></tr><tr><td>valid top-5 accuracy</td><td>▁▂▃▄▅▄▅▅▅▅▅▅▄▅▅▅▅▆▅▅▇▇▇▇▇███████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Super-Class accuracy</td><td>0.8677</td></tr><tr><td>Test Top-1 accuracy</td><td>0.7822</td></tr><tr><td>Test Top-5 accuracy</td><td>0.9444</td></tr><tr><td>Total Score</td><td>2.5943</td></tr><tr><td>epoch</td><td>250</td></tr><tr><td>learning_rate</td><td>0.0008</td></tr><tr><td>train super-class accuracy</td><td>0.67903</td></tr><tr><td>train top-1 accuracy</td><td>0.64913</td></tr><tr><td>train top-5 accuracy</td><td>0.90906</td></tr><tr><td>train_loss</td><td>1.31176</td></tr><tr><td>train_time</td><td>13707.90608</td></tr><tr><td>val_loss</td><td>0.00686</td></tr><tr><td>valid super-class accuracy</td><td>0.8632</td></tr><tr><td>valid top-1 accuracy</td><td>0.7755</td></tr><tr><td>valid top-5 accuracy</td><td>0.9471</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resnet_18_cutmix</strong> at: <a href='https://wandb.ai/hyunwook7120-hanyang-university/CIFAR-100_Classification/runs/iw6t6xpr' target=\"_blank\">https://wandb.ai/hyunwook7120-hanyang-university/CIFAR-100_Classification/runs/iw6t6xpr</a><br/> View project at: <a href='https://wandb.ai/hyunwook7120-hanyang-university/CIFAR-100_Classification' target=\"_blank\">https://wandb.ai/hyunwook7120-hanyang-university/CIFAR-100_Classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241009_080000-iw6t6xpr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
