{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da55e5a3-cdd0-4b19-a405-6d25853d33f9",
   "metadata": {},
   "source": [
    "**Load Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83038a2e-883a-4da6-97bd-c5371f13c182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import torchvision.models as models\n",
    "from utility.early_stopping import EarlyStopping\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, SubsetRandomSampler\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc13d1-90ff-4301-8dac-07977aaaddcf",
   "metadata": {},
   "source": [
    "**Seed Setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b8539ad-3c66-4c94-abb2-fcc33f97d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb9cd2-1723-4ab5-8dd9-6419f42ddf4f",
   "metadata": {},
   "source": [
    "**Device Setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a29bcf9-99b3-4908-a05d-80f158d4de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2957fbc7-d25b-4e3e-b201-b9e357d58826",
   "metadata": {},
   "source": [
    "**Set Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b910701-145a-408d-9309-37816c55b997",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e04e253-49c0-4c37-8260-5e02f2c09452",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93758c0c-b1d2-4ab9-afb4-bdd60a04dbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomCrop(32, padding=4), \n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ade0a7-66ce-48f4-96b0-f4719beaf090",
   "metadata": {},
   "source": [
    "**Splitting the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09b4a6e5-9118-449a-bfb8-d05f9d339304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "val_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=val_transform)\n",
    "test_data = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82c9ab44-2b89-4d17-b90d-f42d1e8fe426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터를 train/val로 나누기\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(0.2 * num_train))  # validation 데이터를 20%로 설정\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "train_idx, val_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce676124-c073-4aec-a000-cb061ac3e10a",
   "metadata": {},
   "source": [
    "**Define Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "824ddf7d-d749-456a-b717-e257489431b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=batch_size, sampler=train_sampler, num_workers=2)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, sampler=val_sampler, shuffle=False, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e300067-8688-4cb1-b34b-a9dcd504c860",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a58ae19-69f4-49a5-b704-d878caa5d7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use: cuda:0\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,728\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "              ReLU-3         [-1, 64, 224, 224]               0\n",
      "            Conv2d-4         [-1, 64, 224, 224]          36,864\n",
      "       BatchNorm2d-5         [-1, 64, 224, 224]             128\n",
      "              ReLU-6         [-1, 64, 224, 224]               0\n",
      "            Conv2d-7         [-1, 64, 224, 224]          36,864\n",
      "       BatchNorm2d-8         [-1, 64, 224, 224]             128\n",
      "        BasicBlock-9         [-1, 64, 224, 224]               0\n",
      "           Conv2d-10         [-1, 64, 224, 224]          36,864\n",
      "      BatchNorm2d-11         [-1, 64, 224, 224]             128\n",
      "             ReLU-12         [-1, 64, 224, 224]               0\n",
      "           Conv2d-13         [-1, 64, 224, 224]          36,864\n",
      "      BatchNorm2d-14         [-1, 64, 224, 224]             128\n",
      "       BasicBlock-15         [-1, 64, 224, 224]               0\n",
      "           Conv2d-16        [-1, 128, 112, 112]          73,728\n",
      "      BatchNorm2d-17        [-1, 128, 112, 112]             256\n",
      "             ReLU-18        [-1, 128, 112, 112]               0\n",
      "           Conv2d-19        [-1, 128, 112, 112]         147,456\n",
      "      BatchNorm2d-20        [-1, 128, 112, 112]             256\n",
      "           Conv2d-21        [-1, 128, 112, 112]           8,192\n",
      "      BatchNorm2d-22        [-1, 128, 112, 112]             256\n",
      "       BasicBlock-23        [-1, 128, 112, 112]               0\n",
      "           Conv2d-24        [-1, 128, 112, 112]         147,456\n",
      "      BatchNorm2d-25        [-1, 128, 112, 112]             256\n",
      "             ReLU-26        [-1, 128, 112, 112]               0\n",
      "           Conv2d-27        [-1, 128, 112, 112]         147,456\n",
      "      BatchNorm2d-28        [-1, 128, 112, 112]             256\n",
      "       BasicBlock-29        [-1, 128, 112, 112]               0\n",
      "           Conv2d-30          [-1, 256, 56, 56]         294,912\n",
      "      BatchNorm2d-31          [-1, 256, 56, 56]             512\n",
      "             ReLU-32          [-1, 256, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]         589,824\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "           Conv2d-35          [-1, 256, 56, 56]          32,768\n",
      "      BatchNorm2d-36          [-1, 256, 56, 56]             512\n",
      "       BasicBlock-37          [-1, 256, 56, 56]               0\n",
      "           Conv2d-38          [-1, 256, 56, 56]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 56, 56]             512\n",
      "             ReLU-40          [-1, 256, 56, 56]               0\n",
      "           Conv2d-41          [-1, 256, 56, 56]         589,824\n",
      "      BatchNorm2d-42          [-1, 256, 56, 56]             512\n",
      "       BasicBlock-43          [-1, 256, 56, 56]               0\n",
      "           Conv2d-44          [-1, 512, 28, 28]       1,179,648\n",
      "      BatchNorm2d-45          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-46          [-1, 512, 28, 28]               0\n",
      "           Conv2d-47          [-1, 512, 28, 28]       2,359,296\n",
      "      BatchNorm2d-48          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-49          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-50          [-1, 512, 28, 28]           1,024\n",
      "       BasicBlock-51          [-1, 512, 28, 28]               0\n",
      "           Conv2d-52          [-1, 512, 28, 28]       2,359,296\n",
      "      BatchNorm2d-53          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-54          [-1, 512, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]       2,359,296\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "       BasicBlock-57          [-1, 512, 28, 28]               0\n",
      "AdaptiveAvgPool2d-58            [-1, 512, 1, 1]               0\n",
      "           Linear-59                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,220,132\n",
      "Trainable params: 11,220,132\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 667.63\n",
      "Params size (MB): 42.80\n",
      "Estimated Total Size (MB): 711.01\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from models import resnet\n",
    "\n",
    "print(\"use:\", device)\n",
    "\n",
    "# 모델 초기화\n",
    "# net = Net()\n",
    "net = resnet.resnet18()\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "net.to(device)\n",
    "\n",
    "print(summary(net, (3,224,224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4617a666-d3ad-4e82-b31f-419020476333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]}]\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49f37a-e104-42df-a9a6-6ba473a9881f",
   "metadata": {},
   "source": [
    "**Model Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "004a5573-52ba-4f9e-84e6-2a7d195f015c",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./runs/resnet_18/tensorboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a7cf9f6-dd9a-45c2-b4cc-c4170b3a2d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96ba42f4-6da7-4f0c-983b-b8b82da34879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 함수\n",
    "def train_model(model, trainloader, criterion, optimizer, num_epochs=25):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "            labels = labels.type(torch.LongTensor).to(device)  # CPU에서 long type tensor로 변환\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 모델 예측\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # 역전파 및 최적화\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # 30번째 배치마다 상태 출력\n",
    "            if (batch_idx + 1) % 30 == 0:\n",
    "                print(f\"Batch [{batch_idx+1}/{len(trainloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Epoch당 평균 손실 계산 및 출력\n",
    "        epoch_loss = running_loss / len(trainloader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping 체크\n",
    "        early_stopping(epoch_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9895f2f8-54c6-4882-ac5e-4a80611df282",
   "metadata": {},
   "source": [
    "**Model Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b995f7c8-7723-4053-83b6-2ae2336a9727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 및 테스트 함수 (superclass 예측 포함)\n",
    "def test_model(model, testloader, criterion, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # 모델 예측\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "\n",
    "            # 예측 결과 저장 및 정확도 계산\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (pred == labels).sum().item()\n",
    "\n",
    "            # TensorBoard에 테스트 손실 및 정확도 기록\n",
    "            writer.add_scalar(\"Test Loss\", test_loss / len(testloader.dataset), epoch)\n",
    "            writer.add_scalar(\"Test Accuracy\", correct / len(testloader.dataset), epoch)\n",
    "\n",
    "    # 평균 손실 및 정확도 계산\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    accuracy = correct / len(testloader.dataset)\n",
    "    # test_accuracies.append(accuracy)\n",
    "\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2e2e73-edc7-4368-910e-e6a71133dd81",
   "metadata": {},
   "source": [
    "**Per-Epoch Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff6115a-12c7-4a72-94f1-661d4d620b4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [30/313], Loss: 4.5262\n",
      "Batch [60/313], Loss: 4.3721\n",
      "Batch [90/313], Loss: 4.3078\n",
      "Batch [120/313], Loss: 4.1325\n",
      "Batch [150/313], Loss: 3.9929\n",
      "Batch [180/313], Loss: 4.0319\n",
      "Batch [210/313], Loss: 4.0776\n",
      "Batch [240/313], Loss: 3.7826\n",
      "Batch [270/313], Loss: 3.7975\n",
      "Batch [300/313], Loss: 3.7420\n",
      "Epoch [1/20], Loss: 3.3091\n",
      "Validation loss decreased (inf --> 3.309097).  Saving model ...\n",
      "Batch [30/313], Loss: 3.7722\n",
      "Batch [60/313], Loss: 3.7182\n",
      "Batch [90/313], Loss: 3.4911\n",
      "Batch [120/313], Loss: 3.9340\n",
      "Batch [150/313], Loss: 3.7466\n",
      "Batch [180/313], Loss: 3.7299\n",
      "Batch [210/313], Loss: 3.6129\n",
      "Batch [240/313], Loss: 3.3678\n",
      "Batch [270/313], Loss: 3.5710\n",
      "Batch [300/313], Loss: 3.5438\n",
      "Epoch [2/20], Loss: 2.8866\n",
      "Validation loss decreased (3.309097 --> 2.886587).  Saving model ...\n",
      "Batch [30/313], Loss: 3.4983\n",
      "Batch [60/313], Loss: 3.2011\n",
      "Batch [90/313], Loss: 3.2579\n",
      "Batch [120/313], Loss: 3.2911\n",
      "Batch [150/313], Loss: 3.3275\n",
      "Batch [180/313], Loss: 3.1907\n",
      "Batch [210/313], Loss: 3.1176\n",
      "Batch [240/313], Loss: 3.2534\n",
      "Batch [270/313], Loss: 3.0047\n",
      "Batch [300/313], Loss: 3.1455\n",
      "Epoch [3/20], Loss: 2.6535\n",
      "Validation loss decreased (2.886587 --> 2.653528).  Saving model ...\n",
      "Batch [30/313], Loss: 3.1609\n",
      "Batch [60/313], Loss: 3.1377\n",
      "Batch [90/313], Loss: 3.3406\n",
      "Batch [120/313], Loss: 3.1287\n",
      "Batch [150/313], Loss: 3.0598\n",
      "Batch [180/313], Loss: 3.0949\n",
      "Batch [210/313], Loss: 3.0832\n",
      "Batch [240/313], Loss: 2.9544\n",
      "Batch [270/313], Loss: 3.0156\n",
      "Batch [300/313], Loss: 2.9548\n",
      "Epoch [4/20], Loss: 2.4523\n",
      "Validation loss decreased (2.653528 --> 2.452317).  Saving model ...\n",
      "Batch [30/313], Loss: 2.6183\n",
      "Batch [60/313], Loss: 2.9878\n",
      "Batch [90/313], Loss: 2.8058\n",
      "Batch [120/313], Loss: 2.9222\n",
      "Batch [150/313], Loss: 2.6145\n",
      "Batch [180/313], Loss: 2.8409\n",
      "Batch [210/313], Loss: 2.7634\n",
      "Batch [240/313], Loss: 2.5940\n",
      "Batch [270/313], Loss: 2.5814\n",
      "Batch [300/313], Loss: 2.6665\n",
      "Epoch [5/20], Loss: 2.2654\n",
      "Validation loss decreased (2.452317 --> 2.265403).  Saving model ...\n",
      "Batch [30/313], Loss: 2.7121\n",
      "Batch [60/313], Loss: 2.5925\n",
      "Batch [90/313], Loss: 2.6636\n",
      "Batch [120/313], Loss: 2.6678\n",
      "Batch [150/313], Loss: 2.7295\n",
      "Batch [180/313], Loss: 2.6393\n",
      "Batch [210/313], Loss: 2.6062\n",
      "Batch [240/313], Loss: 2.3795\n",
      "Batch [270/313], Loss: 2.6824\n",
      "Batch [300/313], Loss: 2.6999\n",
      "Epoch [6/20], Loss: 2.1111\n",
      "Validation loss decreased (2.265403 --> 2.111126).  Saving model ...\n",
      "Batch [30/313], Loss: 2.9069\n",
      "Batch [60/313], Loss: 2.6371\n",
      "Batch [90/313], Loss: 2.3616\n",
      "Batch [120/313], Loss: 2.8237\n",
      "Batch [150/313], Loss: 2.4506\n",
      "Batch [180/313], Loss: 2.5848\n",
      "Batch [210/313], Loss: 2.4917\n",
      "Batch [240/313], Loss: 2.3666\n",
      "Batch [270/313], Loss: 2.1931\n",
      "Batch [300/313], Loss: 2.5566\n",
      "Epoch [7/20], Loss: 1.9869\n",
      "Validation loss decreased (2.111126 --> 1.986857).  Saving model ...\n",
      "Batch [30/313], Loss: 2.4582\n",
      "Batch [60/313], Loss: 2.1449\n",
      "Batch [90/313], Loss: 2.3089\n",
      "Batch [120/313], Loss: 2.3963\n",
      "Batch [150/313], Loss: 2.5644\n",
      "Batch [180/313], Loss: 2.2281\n",
      "Batch [210/313], Loss: 2.3803\n",
      "Batch [240/313], Loss: 2.0761\n",
      "Batch [270/313], Loss: 2.2879\n",
      "Batch [300/313], Loss: 2.3352\n",
      "Epoch [8/20], Loss: 1.8657\n",
      "Validation loss decreased (1.986857 --> 1.865724).  Saving model ...\n",
      "Batch [30/313], Loss: 2.1000\n",
      "Batch [60/313], Loss: 2.2066\n",
      "Batch [90/313], Loss: 2.1566\n",
      "Batch [120/313], Loss: 2.0771\n",
      "Batch [150/313], Loss: 2.1145\n",
      "Batch [180/313], Loss: 2.1754\n",
      "Batch [210/313], Loss: 2.1522\n",
      "Batch [240/313], Loss: 2.1204\n",
      "Batch [270/313], Loss: 2.3308\n",
      "Batch [300/313], Loss: 2.0815\n",
      "Epoch [9/20], Loss: 1.7619\n",
      "Validation loss decreased (1.865724 --> 1.761855).  Saving model ...\n",
      "Batch [30/313], Loss: 2.1213\n",
      "Batch [60/313], Loss: 2.2149\n",
      "Batch [90/313], Loss: 2.1584\n",
      "Batch [120/313], Loss: 2.1115\n",
      "Batch [150/313], Loss: 1.8198\n",
      "Batch [180/313], Loss: 2.0887\n",
      "Batch [210/313], Loss: 2.0647\n",
      "Batch [240/313], Loss: 1.9482\n",
      "Batch [270/313], Loss: 2.0429\n",
      "Batch [300/313], Loss: 2.2063\n",
      "Epoch [10/20], Loss: 1.6675\n",
      "Validation loss decreased (1.761855 --> 1.667529).  Saving model ...\n",
      "Batch [30/313], Loss: 1.8954\n",
      "Batch [60/313], Loss: 2.0775\n",
      "Batch [90/313], Loss: 1.7957\n",
      "Batch [120/313], Loss: 1.8667\n",
      "Batch [150/313], Loss: 1.7633\n",
      "Batch [180/313], Loss: 2.1200\n",
      "Batch [210/313], Loss: 1.8459\n",
      "Batch [240/313], Loss: 1.9824\n",
      "Batch [270/313], Loss: 2.1004\n",
      "Batch [300/313], Loss: 1.8686\n",
      "Epoch [11/20], Loss: 1.5770\n",
      "Validation loss decreased (1.667529 --> 1.577018).  Saving model ...\n",
      "Batch [30/313], Loss: 2.0765\n",
      "Batch [60/313], Loss: 1.8561\n",
      "Batch [90/313], Loss: 2.0504\n",
      "Batch [120/313], Loss: 1.7850\n",
      "Batch [150/313], Loss: 1.9097\n",
      "Batch [180/313], Loss: 1.7575\n",
      "Batch [210/313], Loss: 2.0663\n",
      "Batch [240/313], Loss: 1.6322\n",
      "Batch [270/313], Loss: 1.7840\n",
      "Batch [300/313], Loss: 1.9581\n",
      "Epoch [12/20], Loss: 1.5096\n",
      "Validation loss decreased (1.577018 --> 1.509578).  Saving model ...\n",
      "Batch [30/313], Loss: 1.6521\n",
      "Batch [60/313], Loss: 1.6166\n",
      "Batch [90/313], Loss: 1.8701\n",
      "Batch [120/313], Loss: 1.9121\n",
      "Batch [150/313], Loss: 1.9268\n",
      "Batch [180/313], Loss: 1.7113\n",
      "Batch [210/313], Loss: 1.7672\n",
      "Batch [240/313], Loss: 1.6370\n",
      "Batch [270/313], Loss: 1.6819\n",
      "Batch [300/313], Loss: 1.6569\n",
      "Epoch [13/20], Loss: 1.4294\n",
      "Validation loss decreased (1.509578 --> 1.429365).  Saving model ...\n",
      "Batch [30/313], Loss: 1.6751\n",
      "Batch [60/313], Loss: 1.5986\n",
      "Batch [90/313], Loss: 1.6972\n",
      "Batch [120/313], Loss: 1.5702\n",
      "Batch [150/313], Loss: 1.7002\n",
      "Batch [180/313], Loss: 1.7103\n",
      "Batch [210/313], Loss: 1.4962\n",
      "Batch [240/313], Loss: 1.5937\n",
      "Batch [270/313], Loss: 1.8558\n",
      "Batch [300/313], Loss: 1.8053\n",
      "Epoch [14/20], Loss: 1.3608\n",
      "Validation loss decreased (1.429365 --> 1.360835).  Saving model ...\n",
      "Batch [30/313], Loss: 1.3993\n",
      "Batch [60/313], Loss: 1.4608\n",
      "Batch [90/313], Loss: 1.7053\n",
      "Batch [120/313], Loss: 1.5639\n",
      "Batch [150/313], Loss: 1.5618\n",
      "Batch [180/313], Loss: 1.7079\n",
      "Batch [210/313], Loss: 1.6103\n",
      "Batch [240/313], Loss: 1.4439\n",
      "Batch [270/313], Loss: 1.5999\n",
      "Batch [300/313], Loss: 1.5383\n",
      "Epoch [15/20], Loss: 1.3057\n",
      "Validation loss decreased (1.360835 --> 1.305699).  Saving model ...\n",
      "Batch [30/313], Loss: 1.3553\n",
      "Batch [60/313], Loss: 1.4562\n",
      "Batch [90/313], Loss: 1.6307\n",
      "Batch [120/313], Loss: 1.4391\n",
      "Batch [150/313], Loss: 1.4833\n",
      "Batch [180/313], Loss: 1.5266\n",
      "Batch [210/313], Loss: 1.3325\n",
      "Batch [240/313], Loss: 1.5444\n",
      "Batch [270/313], Loss: 1.5830\n",
      "Batch [300/313], Loss: 1.5596\n",
      "Epoch [16/20], Loss: 1.2465\n",
      "Validation loss decreased (1.305699 --> 1.246487).  Saving model ...\n",
      "Batch [30/313], Loss: 1.5929\n",
      "Batch [60/313], Loss: 1.6147\n",
      "Batch [90/313], Loss: 1.4109\n",
      "Batch [120/313], Loss: 1.4391\n",
      "Batch [150/313], Loss: 1.4206\n",
      "Batch [180/313], Loss: 1.2570\n",
      "Batch [210/313], Loss: 1.6590\n",
      "Batch [240/313], Loss: 1.6256\n",
      "Batch [270/313], Loss: 1.6108\n",
      "Batch [300/313], Loss: 1.4189\n",
      "Epoch [17/20], Loss: 1.1916\n",
      "Validation loss decreased (1.246487 --> 1.191613).  Saving model ...\n",
      "Batch [30/313], Loss: 1.3616\n",
      "Batch [60/313], Loss: 1.7172\n",
      "Batch [90/313], Loss: 1.3308\n",
      "Batch [120/313], Loss: 1.5969\n",
      "Batch [150/313], Loss: 1.3782\n",
      "Batch [180/313], Loss: 1.6916\n",
      "Batch [210/313], Loss: 1.4090\n",
      "Batch [240/313], Loss: 1.4183\n",
      "Batch [270/313], Loss: 1.1819\n",
      "Batch [300/313], Loss: 1.4463\n",
      "Epoch [18/20], Loss: 1.1468\n",
      "Validation loss decreased (1.191613 --> 1.146830).  Saving model ...\n",
      "Batch [30/313], Loss: 1.4861\n",
      "Batch [60/313], Loss: 1.2753\n",
      "Batch [90/313], Loss: 1.2440\n",
      "Batch [120/313], Loss: 1.3178\n",
      "Batch [150/313], Loss: 1.2904\n",
      "Batch [180/313], Loss: 1.2742\n",
      "Batch [210/313], Loss: 1.4438\n",
      "Batch [240/313], Loss: 1.5818\n",
      "Batch [270/313], Loss: 1.3006\n",
      "Batch [300/313], Loss: 1.6266\n",
      "Epoch [19/20], Loss: 1.1013\n",
      "Validation loss decreased (1.146830 --> 1.101258).  Saving model ...\n",
      "Batch [30/313], Loss: 1.4173\n",
      "Batch [60/313], Loss: 1.4011\n",
      "Batch [90/313], Loss: 1.4261\n",
      "Batch [120/313], Loss: 1.2068\n",
      "Batch [150/313], Loss: 1.1956\n",
      "Batch [180/313], Loss: 1.1306\n",
      "Batch [210/313], Loss: 1.2671\n",
      "Batch [240/313], Loss: 1.3732\n",
      "Batch [270/313], Loss: 1.3696\n",
      "Batch [300/313], Loss: 1.3574\n",
      "Epoch [20/20], Loss: 1.0508\n",
      "Validation loss decreased (1.101258 --> 1.050791).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██                                       | 1/20 [03:27<1:05:41, 207.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Result = Epoch : 1   Loss : 0.34637238609313964   Accuracy : 0.10594\n",
      "Batch [30/313], Loss: 0.9555\n",
      "Batch [60/313], Loss: 1.3797\n",
      "Batch [90/313], Loss: 1.2160\n",
      "Batch [120/313], Loss: 1.2401\n",
      "Batch [150/313], Loss: 1.3002\n",
      "Batch [180/313], Loss: 1.3143\n",
      "Batch [210/313], Loss: 1.1792\n",
      "Batch [240/313], Loss: 1.1234\n",
      "Batch [270/313], Loss: 1.4045\n",
      "Batch [300/313], Loss: 1.1439\n",
      "Epoch [1/20], Loss: 1.0086\n",
      "Validation loss decreased (1.050791 --> 1.008620).  Saving model ...\n",
      "Batch [30/313], Loss: 1.1336\n",
      "Batch [60/313], Loss: 1.1741\n",
      "Batch [90/313], Loss: 1.3026\n",
      "Batch [120/313], Loss: 1.1133\n",
      "Batch [150/313], Loss: 1.3527\n",
      "Batch [180/313], Loss: 1.1990\n",
      "Batch [210/313], Loss: 1.1628\n",
      "Batch [240/313], Loss: 1.1515\n",
      "Batch [270/313], Loss: 1.0891\n",
      "Batch [300/313], Loss: 1.4908\n",
      "Epoch [2/20], Loss: 0.9722\n",
      "Validation loss decreased (1.008620 --> 0.972217).  Saving model ...\n",
      "Batch [30/313], Loss: 1.1984\n",
      "Batch [60/313], Loss: 1.1090\n",
      "Batch [90/313], Loss: 1.4002\n",
      "Batch [120/313], Loss: 1.0272\n",
      "Batch [150/313], Loss: 1.0806\n",
      "Batch [180/313], Loss: 1.3877\n",
      "Batch [210/313], Loss: 1.2139\n",
      "Batch [240/313], Loss: 0.7320\n",
      "Batch [270/313], Loss: 1.0379\n",
      "Batch [300/313], Loss: 1.2786\n",
      "Epoch [3/20], Loss: 0.9276\n",
      "Validation loss decreased (0.972217 --> 0.927621).  Saving model ...\n",
      "Batch [30/313], Loss: 1.0203\n",
      "Batch [60/313], Loss: 0.9421\n",
      "Batch [90/313], Loss: 1.1471\n",
      "Batch [120/313], Loss: 1.2767\n",
      "Batch [150/313], Loss: 1.2320\n",
      "Batch [180/313], Loss: 0.9034\n",
      "Batch [210/313], Loss: 1.1192\n",
      "Batch [240/313], Loss: 1.1523\n",
      "Batch [270/313], Loss: 1.2120\n",
      "Batch [300/313], Loss: 1.2835\n",
      "Epoch [4/20], Loss: 0.8909\n",
      "Validation loss decreased (0.927621 --> 0.890868).  Saving model ...\n",
      "Batch [30/313], Loss: 0.9704\n",
      "Batch [60/313], Loss: 1.0344\n",
      "Batch [90/313], Loss: 1.0515\n",
      "Batch [120/313], Loss: 1.1073\n",
      "Batch [150/313], Loss: 1.1819\n",
      "Batch [180/313], Loss: 1.0655\n",
      "Batch [210/313], Loss: 0.8740\n",
      "Batch [240/313], Loss: 1.1048\n",
      "Batch [270/313], Loss: 1.1496\n",
      "Batch [300/313], Loss: 1.2160\n",
      "Epoch [5/20], Loss: 0.8595\n",
      "Validation loss decreased (0.890868 --> 0.859548).  Saving model ...\n",
      "Batch [30/313], Loss: 1.1077\n",
      "Batch [60/313], Loss: 0.9980\n",
      "Batch [90/313], Loss: 0.7752\n",
      "Batch [120/313], Loss: 0.8803\n",
      "Batch [150/313], Loss: 0.9612\n",
      "Batch [180/313], Loss: 0.9318\n",
      "Batch [210/313], Loss: 0.8771\n",
      "Batch [240/313], Loss: 1.0312\n",
      "Batch [270/313], Loss: 0.9170\n",
      "Batch [300/313], Loss: 0.9773\n",
      "Epoch [6/20], Loss: 0.8251\n",
      "Validation loss decreased (0.859548 --> 0.825123).  Saving model ...\n",
      "Batch [30/313], Loss: 0.8410\n",
      "Batch [60/313], Loss: 1.0374\n",
      "Batch [90/313], Loss: 0.8241\n",
      "Batch [120/313], Loss: 1.1422\n",
      "Batch [150/313], Loss: 1.2358\n",
      "Batch [180/313], Loss: 0.8795\n",
      "Batch [210/313], Loss: 1.0988\n",
      "Batch [240/313], Loss: 0.8861\n",
      "Batch [270/313], Loss: 1.0090\n",
      "Batch [300/313], Loss: 1.1650\n",
      "Epoch [7/20], Loss: 0.7904\n",
      "Validation loss decreased (0.825123 --> 0.790365).  Saving model ...\n",
      "Batch [30/313], Loss: 1.0018\n",
      "Batch [60/313], Loss: 0.9532\n",
      "Batch [90/313], Loss: 0.9696\n",
      "Batch [120/313], Loss: 0.8683\n",
      "Batch [150/313], Loss: 0.7784\n",
      "Batch [180/313], Loss: 0.8602\n",
      "Batch [210/313], Loss: 0.8503\n",
      "Batch [240/313], Loss: 0.8291\n",
      "Batch [270/313], Loss: 0.9462\n",
      "Batch [300/313], Loss: 0.8967\n",
      "Epoch [8/20], Loss: 0.7582\n",
      "Validation loss decreased (0.790365 --> 0.758214).  Saving model ...\n",
      "Batch [30/313], Loss: 0.9118\n",
      "Batch [60/313], Loss: 0.8373\n",
      "Batch [90/313], Loss: 0.9048\n",
      "Batch [120/313], Loss: 1.3132\n",
      "Batch [150/313], Loss: 0.9117\n",
      "Batch [180/313], Loss: 0.8932\n",
      "Batch [210/313], Loss: 0.9409\n",
      "Batch [240/313], Loss: 0.9460\n",
      "Batch [270/313], Loss: 0.8641\n",
      "Batch [300/313], Loss: 1.2033\n",
      "Epoch [9/20], Loss: 0.7349\n",
      "Validation loss decreased (0.758214 --> 0.734877).  Saving model ...\n",
      "Batch [30/313], Loss: 0.9476\n",
      "Batch [60/313], Loss: 0.8640\n",
      "Batch [90/313], Loss: 0.8971\n",
      "Batch [120/313], Loss: 0.8585\n",
      "Batch [150/313], Loss: 1.0518\n",
      "Batch [180/313], Loss: 0.9441\n",
      "Batch [210/313], Loss: 0.9011\n",
      "Batch [240/313], Loss: 0.7199\n",
      "Batch [270/313], Loss: 0.6870\n",
      "Batch [300/313], Loss: 0.8409\n",
      "Epoch [10/20], Loss: 0.6953\n",
      "Validation loss decreased (0.734877 --> 0.695341).  Saving model ...\n",
      "Batch [30/313], Loss: 0.6510\n",
      "Batch [60/313], Loss: 0.7893\n",
      "Batch [90/313], Loss: 0.9386\n",
      "Batch [120/313], Loss: 0.7564\n",
      "Batch [150/313], Loss: 0.7731\n",
      "Batch [180/313], Loss: 0.7860\n",
      "Batch [210/313], Loss: 0.8943\n",
      "Batch [240/313], Loss: 0.7529\n",
      "Batch [270/313], Loss: 0.9124\n",
      "Batch [300/313], Loss: 0.8199\n",
      "Epoch [11/20], Loss: 0.6711\n",
      "Validation loss decreased (0.695341 --> 0.671117).  Saving model ...\n",
      "Batch [30/313], Loss: 0.8310\n",
      "Batch [60/313], Loss: 0.7215\n",
      "Batch [90/313], Loss: 0.8280\n",
      "Batch [120/313], Loss: 0.8040\n",
      "Batch [150/313], Loss: 0.9160\n",
      "Batch [180/313], Loss: 0.7130\n",
      "Batch [210/313], Loss: 0.9483\n",
      "Batch [240/313], Loss: 0.7290\n",
      "Batch [270/313], Loss: 0.9036\n",
      "Batch [300/313], Loss: 0.8986\n",
      "Epoch [12/20], Loss: 0.6464\n",
      "Validation loss decreased (0.671117 --> 0.646425).  Saving model ...\n",
      "Batch [30/313], Loss: 0.7823\n",
      "Batch [60/313], Loss: 0.6239\n",
      "Batch [90/313], Loss: 0.8085\n",
      "Batch [120/313], Loss: 0.8111\n",
      "Batch [150/313], Loss: 0.7125\n",
      "Batch [180/313], Loss: 0.6649\n",
      "Batch [210/313], Loss: 0.9088\n",
      "Batch [240/313], Loss: 0.8549\n",
      "Batch [270/313], Loss: 0.6787\n",
      "Batch [300/313], Loss: 0.7389\n",
      "Epoch [13/20], Loss: 0.6141\n",
      "Validation loss decreased (0.646425 --> 0.614051).  Saving model ...\n",
      "Batch [30/313], Loss: 0.8236\n",
      "Batch [60/313], Loss: 0.7432\n",
      "Batch [90/313], Loss: 0.6389\n",
      "Batch [120/313], Loss: 0.6998\n",
      "Batch [150/313], Loss: 0.7728\n",
      "Batch [180/313], Loss: 0.5775\n",
      "Batch [210/313], Loss: 0.7282\n",
      "Batch [240/313], Loss: 0.7604\n",
      "Batch [270/313], Loss: 0.8301\n",
      "Batch [300/313], Loss: 0.9709\n",
      "Epoch [14/20], Loss: 0.5828\n",
      "Validation loss decreased (0.614051 --> 0.582755).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5541\n",
      "Batch [60/313], Loss: 0.6363\n",
      "Batch [90/313], Loss: 0.6829\n",
      "Batch [120/313], Loss: 0.7006\n",
      "Batch [150/313], Loss: 0.6780\n",
      "Batch [180/313], Loss: 0.6888\n",
      "Batch [210/313], Loss: 0.8122\n",
      "Batch [240/313], Loss: 0.7414\n",
      "Batch [270/313], Loss: 0.7740\n",
      "Batch [300/313], Loss: 0.8703\n",
      "Epoch [15/20], Loss: 0.5582\n",
      "Validation loss decreased (0.582755 --> 0.558201).  Saving model ...\n",
      "Batch [30/313], Loss: 0.4996\n",
      "Batch [60/313], Loss: 0.5258\n",
      "Batch [90/313], Loss: 0.7299\n",
      "Batch [120/313], Loss: 0.8030\n",
      "Batch [150/313], Loss: 0.7312\n",
      "Batch [180/313], Loss: 0.8614\n",
      "Batch [210/313], Loss: 0.7212\n",
      "Batch [240/313], Loss: 0.6636\n",
      "Batch [270/313], Loss: 0.6511\n",
      "Batch [300/313], Loss: 0.6337\n",
      "Epoch [16/20], Loss: 0.5324\n",
      "Validation loss decreased (0.558201 --> 0.532371).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5609\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "    train_model(net, train_loader, criterion, optimizer, num_epochs=num_epochs)\n",
    "    test_loss, test_accuracy = test_model(net, val_loader, criterion, epoch)\n",
    "    writer.add_scalar(\"Test Loss\", test_loss, epoch)\n",
    "    writer.add_scalar(\"Test Accuracy\", test_accuracy, epoch)\n",
    "    print(f\"Processing Result = Epoch : {epoch}   Loss : {test_loss}   Accuracy : {test_accuracy}\")\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfa545b-514f-4839-81ac-2ee9d4043d64",
   "metadata": {},
   "source": [
    "**Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027bb618-b06d-4510-8d8c-45a86d2b9820",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Result of ResNet = Epoch : {epoch}   Loss : {test_loss}   Accuracy : {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6b62f7-ec5c-4a08-9b72-a90d5c31e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(net, \"resnet_18.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba4f2a-2cb2-47a0-97ab-cb92669dd063",
   "metadata": {},
   "source": [
    "**Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa22644-2f03-4165-ac1e-1b15e4b5ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('10000개 테스트 이미지에서 모델 정확도: %.2f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fd9a55-51f7-4820-86dc-bb2a531134a0",
   "metadata": {},
   "source": [
    "**Visualization of average loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6528ff87-705d-43c3-b799-baffbe11161f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "# 정확도 시각화\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, len(test_accuracies) + 1), test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train vs Test Accuracy per Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107fe74b-41e9-41c7-8f1d-ac320f2b821e",
   "metadata": {},
   "source": [
    "**Top-1 Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127d3a88-236c-4833-afa4-ad772f878041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_top1_accuracy(model, device, data_loader, criterion):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "            # # 각 샘플에 대한 예측 결과와 실제 레이블 출력\n",
    "            # for i in range(data.size(0)):\n",
    "            #     print(f\"Sample {i + 1}: Predicted = {predicted[i].item()}, Actual = {target[i].item()}\")\n",
    "\n",
    "    top1_accuracy = 100 * correct / total\n",
    "    print(f\"Top-1 Accuracy: {top1_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70cb58b-59a7-4921-b66d-337ae39a3c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련 후 검증 데이터셋에 대한 Top-1 정확도 계산 및 출력\n",
    "calculate_top1_accuracy(net, device, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9685776-2742-4899-bc03-69bab0fdf760",
   "metadata": {},
   "source": [
    "**Top-5 Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3833597-d6f0-4061-9acd-98773f9c2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_print_top5_accuracy(model, device, data_loader, criterion):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            # Top-5 예측 결과 가져오기\n",
    "            _, predicted_top5 = torch.topk(outputs, 5, dim=1)\n",
    "            total += target.size(0)\n",
    "            \n",
    "            # 예측된 Top-5 내에 실제 레이블이 있는지 확인\n",
    "            correct += (predicted_top5 == target.view(-1, 1)).sum().item()\n",
    "\n",
    "    top5_accuracy = 100 * correct / total\n",
    "    print(f\"Top-5 Accuracy: {top5_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606ba241-4e9a-49c1-8f74-99b59f6649ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_print_top5_accuracy(net, device, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f700b560-e4d2-4c26-9d1f-ddd264c808a6",
   "metadata": {},
   "source": [
    "**Super Class Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41bb1e9-12aa-4153-a4e8-4bff5517d705",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_to_superclass = {\n",
    "    # aquatic mammals\n",
    "    4: 0, 30: 0, 55: 0, 72: 0, 95: 0,\n",
    "    \n",
    "    # fish\n",
    "    1: 1, 32: 1, 67: 1, 73: 1, 91: 1,\n",
    "    \n",
    "    # flowers\n",
    "    54: 2, 62: 2, 70: 2, 82: 2, 92: 2,\n",
    "    \n",
    "    # food containers\n",
    "    9: 3, 10: 3, 16: 3, 28: 3, 61: 3,\n",
    "    \n",
    "    # fruit and vegetables\n",
    "    0: 4, 51: 4, 53: 4, 57: 4, 83: 4,\n",
    "    \n",
    "    # household electrical devices\n",
    "    22: 5, 39: 5, 40: 5, 86: 5, 87: 5,\n",
    "    \n",
    "    # household furniture\n",
    "    5: 6, 20: 6, 25: 6, 84: 6, 94: 6,\n",
    "    \n",
    "    # insects\n",
    "    6: 7, 7: 7, 14: 7, 18: 7, 24: 7,\n",
    "    \n",
    "    # large carnivores\n",
    "    3: 8, 42: 8, 43: 8, 88: 8, 97: 8,\n",
    "    \n",
    "    # large man-made outdoor things\n",
    "    12: 9, 17: 9, 37: 9, 68: 9, 76: 9,\n",
    "    \n",
    "    # large natural outdoor scenes\n",
    "    23: 10, 33: 10, 49: 10, 60: 10, 71: 10,\n",
    "    \n",
    "    # large omnivores and herbivores\n",
    "    15: 11, 19: 11, 21: 11, 31: 11, 38: 11,\n",
    "    \n",
    "    # medium-sized mammals\n",
    "    34: 12, 63: 12, 64: 12, 66: 12, 75: 12,\n",
    "    \n",
    "    # non-insect invertebrates\n",
    "    26: 13, 45: 13, 77: 13, 79: 13, 99: 13,\n",
    "    \n",
    "    # people\n",
    "    2: 14, 11: 14, 35: 14, 46: 14, 98: 14,\n",
    "    \n",
    "    # reptiles\n",
    "    27: 15, 29: 15, 44: 15, 78: 15, 93: 15,\n",
    "    \n",
    "    # small mammals\n",
    "    36: 16, 50: 16, 65: 16, 74: 16, 80: 16,\n",
    "    \n",
    "    # trees\n",
    "    47: 17, 52: 17, 56: 17, 59: 17, 96: 17,\n",
    "    \n",
    "    # vehicles 1\n",
    "    8: 18, 13: 18, 48: 18, 58: 18, 90: 18,\n",
    "    \n",
    "    # vehicles 2\n",
    "    41: 19, 69: 19, 81: 19, 85: 19, 89: 19\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c642a277-ec41-4458-a8ed-40bd10bba0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def superclass_accuracy(model, device, data_loader, criterion, k=1, super=False):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "\n",
    "            _, predicted = torch.topk(outputs, k, dim=1)\n",
    "\n",
    "            if super:  # 슈퍼클래스 기준으로 계산\n",
    "                # 세부 클래스를 슈퍼클래스로 변환\n",
    "                target_super = torch.tensor([fine_to_superclass[t.item()] for t in target], device=device)\n",
    "                predicted_super = torch.tensor([fine_to_superclass[p.item()] for p in predicted.view(-1)], device=device).view_as(predicted)                \n",
    "                correct += (predicted_super == target_super.view(-1, 1)).sum().item()\n",
    "            else:  # 세부 클래스 기준으로 계산\n",
    "                correct += (predicted == target.view(-1, 1)).sum().item()\n",
    "            \n",
    "            total += target.size(0)\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    if not super:\n",
    "        print(f\"Top-{k} Accuracy: {accuracy:.2f}%\")\n",
    "    else:\n",
    "        print(f\"Super-{k} Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dabb03-118b-46c3-820d-086430425cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "superclass_accuracy(net, device, test_loader, criterion, 1, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0745c0-e4f9-4812-b275-2831d0198f80",
   "metadata": {},
   "source": [
    "**Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6d13d6-047b-4c5f-a63b-6c27da74d523",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./runs/resnet_18/tensorboard --port=8204 --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435a34e-18f6-4e40-a713-1eac0e5f2439",
   "metadata": {},
   "source": [
    "**Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017fadb8-738c-4bf8-928c-c110b6bf7225",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_val_data.classes\n",
    "coarse_classes = [\n",
    "    'aquatic mammals', 'fish', 'flowers', 'food containers', 'fruit and vegetables', 'household electrical devices', \n",
    "    'household furniture', 'insects', 'large carnivores', 'large man-made outdoor things', \n",
    "    'large natural outdoor scenes', 'large omnivores and herbivores', 'medium-sized mammals', \n",
    "    'non-insect invertebrates', 'people', 'reptiles', 'small mammals', 'trees', 'vehicles 1', 'vehicles 2'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026cac75-4f48-4477-8f79-2b815a0b869b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(classes), len(coarse_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b717987-41be-4e2f-b482-7fd3d1f42990",
   "metadata": {},
   "source": [
    "**Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e36424e-dce2-47fa-9a8d-ee9a1d6dfbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = []\n",
    "# y_true = []\n",
    "\n",
    "# # iterate over test data\n",
    "# for x, y in torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size):\n",
    "    \n",
    "#     #print('iter val', i)\n",
    "#     x = x.to(device)\n",
    "#     y = y.to(device)\n",
    "#     z = net(x)\n",
    "#     _, yhat = torch.max(z, 1)\n",
    "#     pred = yhat.data.cpu().numpy()\n",
    "#     y_pred.extend(pred) # Save Prediction\n",
    "\n",
    "#     labels = y.data.cpu().numpy()\n",
    "#     y_true.extend(labels) # Save Truth\n",
    "\n",
    "# # Build confusion matrix\n",
    "# cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "# df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
    "#                      columns = [i for i in classes])\n",
    "# plt.figure(figsize = (128,70))\n",
    "# sns.heatmap(df_cm, annot=True)\n",
    "# plt.title('Confusion Matrix of ResNet (CIFAR100)')\n",
    "# plt.savefig('./runs/resnet_18/Confusion_matrix_ResNet_Cifar100.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d9de9a-35c4-4621-b2ef-218a7cb05aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\" Classification Report of ResNet(CIFAR100)  \\n { classification_report(y_true, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1487dcdb-dd29-4eaa-9e89-e4f2c52e78ad",
   "metadata": {},
   "source": [
    "**Confusion Matrix - Coarse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8404d10-a77a-4544-96d1-70cb306e2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over test data\n",
    "for x, y in torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    z = net(x)\n",
    "    _, yhat = torch.max(z, 1)\n",
    "    \n",
    "    # Save Prediction and convert to superclasses\n",
    "    pred = yhat.data.cpu().numpy()\n",
    "    super_pred = [fine_to_superclass[p] for p in pred]\n",
    "    y_pred.extend(super_pred)\n",
    "    \n",
    "    # Save Truth and convert to superclasses\n",
    "    labels = y.data.cpu().numpy()\n",
    "    super_labels = [fine_to_superclass[l] for l in labels]\n",
    "    y_true.extend(super_labels)\n",
    "\n",
    "# Build confusion matrix for superclasses\n",
    "cf_matrix = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "\n",
    "# Normalize confusion matrix\n",
    "df_cm = pd.DataFrame(cf_matrix, index=coarse_classes,\n",
    "                     columns=coarse_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='.2f', cmap='Blues')\n",
    "plt.title('Confusion Matrix of ResNet (CIFAR-100 Superclass)')\n",
    "plt.savefig('./runs/resnet_18/Confusion_matrix_ResNet_Cifar100_superclass.jpg')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccdb480-4fb5-4644-a842-f1e75d117b21",
   "metadata": {},
   "source": [
    "**Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2e9116-239a-4a43-bc30-fe614635fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Super Classification Report of ResNet(CIFAR100)  \\n { classification_report(y_true, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c4634f-9006-4c4f-b3bb-c3ca0e055e1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
