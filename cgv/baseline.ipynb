{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da55e5a3-cdd0-4b19-a405-6d25853d33f9",
   "metadata": {
    "id": "da55e5a3-cdd0-4b19-a405-6d25853d33f9"
   },
   "source": [
    "**Load Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83038a2e-883a-4da6-97bd-c5371f13c182",
   "metadata": {
    "id": "83038a2e-883a-4da6-97bd-c5371f13c182"
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from utility.utils import EarlyStopping, WarmUpLR, most_recent_folder, most_recent_weights, best_acc_weights, last_epoch\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, Subset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Analysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc13d1-90ff-4301-8dac-07977aaaddcf",
   "metadata": {
    "id": "bbdc13d1-90ff-4301-8dac-07977aaaddcf"
   },
   "source": [
    "**Seed & Device Setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b8539ad-3c66-4c94-abb2-fcc33f97d3ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b8539ad-3c66-4c94-abb2-fcc33f97d3ca",
    "outputId": "ed6e73ba-1056-4f62-fc55-15b56acfe3c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(0)\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b5e51",
   "metadata": {
    "id": "a35b5e51"
   },
   "source": [
    "**Wandb Setting & Set Hyperparameters (수정 구간)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5ce35ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "d5ce35ad",
    "outputId": "26c52ce7-bcef-496e-fd7d-d9ea23426c27"
   },
   "outputs": [],
   "source": [
    "# Scheduler 종류\n",
    "# \"ReduceLROnPlateau\", \"MultiStepLR\"\n",
    "\n",
    "config = {\n",
    "    \"batch_size\" : 128,\n",
    "    \"num_epochs\" : 250,\n",
    "    \"learning_rate\" : 0.1,\n",
    "    \"momentum\" : 0.9,\n",
    "    \"weight_decay\" : 5e-4,\n",
    "    \"nesterov\" : True,\n",
    "    \"gamma\" : 0.2,\n",
    "    \"warm\" : 1,\n",
    "    \"patience\" : 150,\n",
    "    \"milestones\" : [100, 150, 200], # MultiStepLR 전용\n",
    "\n",
    "    \"resume\" : False,   # 학습이 끊겨 이어서 할 때 사용\n",
    "    \"model_name\" : \"ResNet_18+nesterov\",\n",
    "    \"stratified_data\" : True,\n",
    "    \"scheduler\" : \"MultiStepLR\"\n",
    "}\n",
    "# wandb.init(project=\"CIFAR-100_Classification\", name=config[\"model_name\"], config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83da8557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: hyunwook7120 (hyunwook7120-hanyang-university). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\jhw03\\Screen\\cgv\\CIFAR100-Classfication\\cgv\\wandb\\run-20241008_223907-f2wb20vy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hyunwook7120-hanyang-university/CIFAR-100_Classification/runs/f2wb20vy' target=\"_blank\">ResNet_18+nesterov</a></strong> to <a href='https://wandb.ai/hyunwook7120-hanyang-university/CIFAR-100_Classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hyunwook7120-hanyang-university/CIFAR-100_Classification' target=\"_blank\">https://wandb.ai/hyunwook7120-hanyang-university/CIFAR-100_Classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hyunwook7120-hanyang-university/CIFAR-100_Classification/runs/f2wb20vy' target=\"_blank\">https://wandb.ai/hyunwook7120-hanyang-university/CIFAR-100_Classification/runs/f2wb20vy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hyunwook7120-hanyang-university/CIFAR-100_Classification/runs/f2wb20vy?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x21d29bd7730>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"CIFAR-100_Classification\", name=config[\"model_name\"], config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8d1e41f",
   "metadata": {
    "id": "a8d1e41f"
   },
   "outputs": [],
   "source": [
    "fine_to_superclass = {\n",
    "    4: 0, 30: 0, 55: 0, 72: 0, 95: 0,       # aquatic mammals\n",
    "    1: 1, 32: 1, 67: 1, 73: 1, 91: 1,       # fish\n",
    "    54: 2, 62: 2, 70: 2, 82: 2, 92: 2,      # flowers\n",
    "    9: 3, 10: 3, 16: 3, 28: 3, 61: 3,       # food containers\n",
    "    0: 4, 51: 4, 53: 4, 57: 4, 83: 4,       # fruit and vegetables\n",
    "    22: 5, 39: 5, 40: 5, 86: 5, 87: 5,      # household electrical devices\n",
    "    5: 6, 20: 6, 25: 6, 84: 6, 94: 6,       # household furniture\n",
    "    6: 7, 7: 7, 14: 7, 18: 7, 24: 7,        # insects\n",
    "    3: 8, 42: 8, 43: 8, 88: 8, 97: 8,       # large carnivores\n",
    "    12: 9, 17: 9, 37: 9, 68: 9, 76: 9,      # large man-made outdoor things\n",
    "    23: 10, 33: 10, 49: 10, 60: 10, 71: 10, # large natural outdoor scenes\n",
    "    15: 11, 19: 11, 21: 11, 31: 11, 38: 11, # large omnivores and herbivores\n",
    "    34: 12, 63: 12, 64: 12, 66: 12, 75: 12, # medium-sized mammals\n",
    "    26: 13, 45: 13, 77: 13, 79: 13, 99: 13, # non-insect invertebrates\n",
    "    2: 14, 11: 14, 35: 14, 46: 14, 98: 14,  # people\n",
    "    27: 15, 29: 15, 44: 15, 78: 15, 93: 15, # reptiles\n",
    "    36: 16, 50: 16, 65: 16, 74: 16, 80: 16, # small mammals\n",
    "    47: 17, 52: 17, 56: 17, 59: 17, 96: 17, # trees\n",
    "    8: 18, 13: 18, 48: 18, 58: 18, 90: 18,  # vehicles 1\n",
    "    41: 19, 69: 19, 81: 19, 85: 19, 89: 19  # vehicles 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e04e253-49c0-4c37-8260-5e02f2c09452",
   "metadata": {
    "id": "3e04e253-49c0-4c37-8260-5e02f2c09452"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZKl6oKBffJYA",
   "metadata": {
    "id": "ZKl6oKBffJYA"
   },
   "source": [
    "**Data Augementation (수정 구간)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93758c0c-b1d2-4ab9-afb4-bdd60a04dbd3",
   "metadata": {
    "id": "93758c0c-b1d2-4ab9-afb4-bdd60a04dbd3"
   },
   "outputs": [],
   "source": [
    "CIFAR100_TRAIN_MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
    "CIFAR100_TRAIN_STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
    "\n",
    "train_val_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # transforms.RandomRotation(degrees=15),\n",
    "    # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ade0a7-66ce-48f4-96b0-f4719beaf090",
   "metadata": {
    "id": "13ade0a7-66ce-48f4-96b0-f4719beaf090"
   },
   "source": [
    "**Splitting the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09b4a6e5-9118-449a-bfb8-d05f9d339304",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09b4a6e5-9118-449a-bfb8-d05f9d339304",
    "outputId": "f8325035-cd21-4715-b7a6-1e0d8d0dfcd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_val_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_val_transform)\n",
    "val_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=test_transform)\n",
    "test_data = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82c9ab44-2b89-4d17-b90d-f42d1e8fe426",
   "metadata": {
    "id": "82c9ab44-2b89-4d17-b90d-f42d1e8fe426"
   },
   "outputs": [],
   "source": [
    "# Stratified 방식으로 train 데이터를 train/val로 나누기\n",
    "if config[\"stratified_data\"]:\n",
    "  labels = train_val_data.targets\n",
    "  stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "  for train_index, val_index in stratified_split.split(train_val_data.data, labels):\n",
    "      train_data = Subset(train_val_data, train_index)\n",
    "      val_data = Subset(val_data, val_index)\n",
    "else:\n",
    "  # RandomSampler 방식으로 train 데이터를 train/val로 나누기\n",
    "  num_train = len(train_val_data)\n",
    "  indices = list(range(num_train))\n",
    "  split = int(np.floor(0.2 * num_train))  # validation 데이터를 20%로 설정\n",
    "\n",
    "  np.random.shuffle(indices)\n",
    "  train_idx, val_idx = indices[split:], indices[:split]\n",
    "\n",
    "  train_sampler = SubsetRandomSampler(train_idx)\n",
    "  val_sampler = SubsetRandomSampler(val_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce676124-c073-4aec-a000-cb061ac3e10a",
   "metadata": {
    "id": "ce676124-c073-4aec-a000-cb061ac3e10a"
   },
   "source": [
    "**Define Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "824ddf7d-d749-456a-b717-e257489431b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "824ddf7d-d749-456a-b717-e257489431b9",
    "outputId": "8bfd89af-a57a-441c-9a14-d549ff2ef41d"
   },
   "outputs": [],
   "source": [
    "if config[\"stratified_data\"]:\n",
    "  train_loader = DataLoader(train_data, batch_size=config[\"batch_size\"], shuffle=True, num_workers=4)\n",
    "  val_loader = DataLoader(val_data, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4)\n",
    "  test_loader = DataLoader(test_data, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4)\n",
    "else:\n",
    "  train_loader = DataLoader(train_val_data, batch_size=config[\"batch_size\"], sampler=train_sampler, num_workers=4)\n",
    "  val_loader = DataLoader(train_val_data, batch_size=config[\"batch_size\"], shuffle=False, sampler=val_sampler, num_workers=4)\n",
    "  test_loader = DataLoader(test_data, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e300067-8688-4cb1-b34b-a9dcd504c860",
   "metadata": {
    "id": "0e300067-8688-4cb1-b34b-a9dcd504c860"
   },
   "source": [
    "**Model Initialize (수정 부분)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a58ae19-69f4-49a5-b704-d878caa5d7c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a58ae19-69f4-49a5-b704-d878caa5d7c7",
    "outputId": "70f5d330-681b-4ffd-de0b-16232491cdc4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use: cuda:0\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.0, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0.0005, 'nesterov': True, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'initial_lr': 0.1, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313]}]\n"
     ]
    }
   ],
   "source": [
    "from models import resnet, resnext\n",
    "\n",
    "print(\"use:\", device)\n",
    "\n",
    "# 모델 초기화\n",
    "model = resnext.resnext101()\n",
    "net = model\n",
    "# 모델을 GPU로 이동\n",
    "net.to(device)\n",
    "\n",
    "# print(summary(net, (3,224,224)))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=config[\"learning_rate\"], momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"], nesterov=config[\"nesterov\"])\n",
    "\n",
    "if config[\"scheduler\"] == \"MultiStepLR\":\n",
    "    train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, config[\"milestones\"], gamma=config[\"gamma\"]) #learning rate decay\n",
    "elif config[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "    train_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", factor=config[\"gamma\"], patience=5)\n",
    "else:\n",
    "    print(\"No scheduler!!\")\n",
    "iter_per_epoch = len(train_loader)\n",
    "warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * config[\"warm\"])\n",
    "early_stopping = EarlyStopping(patience=config[\"patience\"], verbose=True)\n",
    "\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49f37a-e104-42df-a9a6-6ba473a9881f",
   "metadata": {
    "id": "8b49f37a-e104-42df-a9a6-6ba473a9881f"
   },
   "source": [
    "**Model Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96ba42f4-6da7-4f0c-983b-b8b82da34879",
   "metadata": {
    "id": "96ba42f4-6da7-4f0c-983b-b8b82da34879"
   },
   "outputs": [],
   "source": [
    "# 모델 학습 함수\n",
    "def train_model(net, trainloader, criterion, optimizer, epoch):\n",
    "    net.train()\n",
    "    start = time.time()\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    superclass_correct = 0\n",
    "    superclass_total = 0\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        labels, inputs = labels.to(device), inputs.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 모델 예측\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        top1_correct += torch.sum(preds == labels).item()\n",
    "\n",
    "        _, top5_preds = outputs.topk(5, 1, True, True)\n",
    "        top5_correct += torch.sum(top5_preds.eq(labels.view(-1, 1).expand_as(top5_preds))).item()\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Superclass accuracy\n",
    "        super_preds = torch.tensor([fine_to_superclass[p.item()] for p in preds], dtype=torch.long)\n",
    "        super_labels = torch.tensor([fine_to_superclass[t.item()] for t in labels], dtype=torch.long)\n",
    "        superclass_correct += torch.sum(super_preds == super_labels).item()\n",
    "        superclass_total += super_labels.size(0)\n",
    "\n",
    "        # 역전파 및 최적화\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        n_iter = (epoch - 1) * len(train_loader) + batch_idx + 1\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # 30번째 배치마다 상태 출력\n",
    "        if (batch_idx + 1) % 30 == 0:\n",
    "            print(f\"Batch [{batch_idx+1}/{len(trainloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # LR에 warmup 되는 게 안 보이지만 실제론 이루어지고 있음. batch로 돌 때 이루어짐.\n",
    "        if epoch <= config[\"warm\"]:\n",
    "            warmup_scheduler.step()\n",
    "\n",
    "\n",
    "    # Epoch당 평균 손실 계산 및 출력\n",
    "    epoch_loss = running_loss / total\n",
    "\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    finish = time.time()\n",
    "\n",
    "    # 세부 클래스 및 슈퍼 클래스 정확도 계산\n",
    "    top1_acc = top1_correct / total\n",
    "    top5_acc = top5_correct / total\n",
    "    superclass_acc = superclass_correct / superclass_total\n",
    "    accuracy = [top1_acc, top5_acc, superclass_acc]\n",
    "\n",
    "    print(\"Train set: Epoch: {}, Average loss:{:.4f}, LR: {:.6f} Top-1 Accuracy: {:.4f}, Top-5 Accuracy: {:.4f}, SuperClass Accuracy: {:.4f}, Time consumed:{:.2f}s\".format(\n",
    "        epoch,\n",
    "        epoch_loss,\n",
    "        optimizer.param_groups[0]['lr'],\n",
    "        top1_acc,\n",
    "        top5_acc,\n",
    "        superclass_acc,\n",
    "        finish - start\n",
    "    ))\n",
    "\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "def eval_training(net, testloader, criterion, epoch):\n",
    "    net.eval()\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    superclass_correct = 0\n",
    "    superclass_total = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            top1_correct += torch.sum(preds == labels).item()\n",
    "\n",
    "            _, top5_preds = outputs.topk(5, 1, True, True)\n",
    "            top5_correct += torch.sum(top5_preds.eq(labels.view(-1, 1).expand_as(top5_preds))).item()\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "            test_loss += criterion(outputs, labels)\n",
    "\n",
    "            # Superclass accuracy\n",
    "            super_preds = torch.tensor([fine_to_superclass[p.item()] for p in preds], dtype=torch.long)\n",
    "            super_labels = torch.tensor([fine_to_superclass[t.item()] for t in labels], dtype=torch.long)\n",
    "            superclass_correct += torch.sum(super_preds == super_labels).item()\n",
    "            superclass_total += super_labels.size(0)\n",
    "\n",
    "    finish = time.time()\n",
    "    average_loss = test_loss / total\n",
    "\n",
    "    # 세부 클래스 및 슈퍼 클래스 정확도 계산\n",
    "    top1_acc = top1_correct / total\n",
    "    top5_acc = top5_correct / total\n",
    "    superclass_acc = superclass_correct / superclass_total\n",
    "    accuracy = [top1_acc, top5_acc, superclass_acc]\n",
    "\n",
    "    print(\"Test set: Epoch: {}, Average loss:{:.4f}, Top-1 Accuracy: {:.4f}, Top-5 Accuracy: {:.4f}, SuperClass Accuracy: {:.4f}, Time consumed:{:.2f}s\".format(\n",
    "        epoch,\n",
    "        average_loss,\n",
    "        top1_acc,\n",
    "        top5_acc,\n",
    "        superclass_acc,\n",
    "        finish - start\n",
    "    ))\n",
    "    print()\n",
    "\n",
    "    return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e079b97-a215-4b55-aedd-c1297773426d",
   "metadata": {
    "id": "7e079b97-a215-4b55-aedd-c1297773426d"
   },
   "outputs": [],
   "source": [
    "DATE_FORMAT = '%A_%d_%B_%Y_%Hh_%Mm_%Ss'\n",
    "TIME_NOW = datetime.now().strftime(DATE_FORMAT)\n",
    "\n",
    "if config[\"resume\"]:\n",
    "    recent_folder = most_recent_folder(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\"), DATE_FORMAT)\n",
    "    if not recent_folder:\n",
    "        raise Exception(\"no recent folder were found\")\n",
    "\n",
    "    checkpoint_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder)\n",
    "else:\n",
    "    checkpoint_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", TIME_NOW)\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, '{net}-{epoch}-{type}.pth')\n",
    "\n",
    "best_acc = 0.0\n",
    "epoch = 0\n",
    "weights_path = False\n",
    "\n",
    "if config[\"resume\"]:\n",
    "    best_weights = best_acc_weights(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder))\n",
    "    if best_weights:\n",
    "        weights_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder, best_weights)\n",
    "        print('found best acc weights file:{}'.format(weights_path))\n",
    "        print('load best training file to test acc...')\n",
    "        net.load_state_dict(torch.load(weights_path))\n",
    "        _, best_acc = eval_training(net, test_loader, criterion, epoch)\n",
    "        print(\"best acc is {:0.2f}\".format(best_acc))\n",
    "\n",
    "    recent_weights_file = most_recent_weights(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder))\n",
    "    if not recent_weights_file:\n",
    "        raise Exception(\"no recent weights file were found\")\n",
    "    weights_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder, recent_weights_file)\n",
    "    print(\"loading weights file {} to resume training......\".format(weights_path))\n",
    "    net.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "    resume_epoch = last_epoch(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2e2e73-edc7-4368-910e-e6a71133dd81",
   "metadata": {
    "id": "4a2e2e73-edc7-4368-910e-e6a71133dd81"
   },
   "source": [
    "**Per-Epoch Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c463ddea-8731-4e48-8ba1-23e2961b164a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c463ddea-8731-4e48-8ba1-23e2961b164a",
    "outputId": "c22e6d65-3707-478e-ceb6-b56881f0c945",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [30/313], Loss: 4.8486\n",
      "Batch [60/313], Loss: 4.7074\n",
      "Batch [90/313], Loss: 4.5035\n",
      "Batch [120/313], Loss: 4.3198\n",
      "Batch [150/313], Loss: 4.0522\n",
      "Batch [180/313], Loss: 3.9825\n",
      "Batch [210/313], Loss: 3.9234\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for epoch in tqdm(range(1, config[\"num_epochs\"] + 1)):\n",
    "    if epoch > config[\"warm\"]:\n",
    "        if config[\"scheduler\"] == \"MultiStepLR\":\n",
    "            train_scheduler.step()\n",
    "        elif config[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "            train_scheduler.step(test_loss)\n",
    "\n",
    "    if config[\"resume\"]:\n",
    "        if epoch <= resume_epoch:\n",
    "            continue\n",
    "\n",
    "    epoch_loss, train_accuracy = train_model(net, train_loader, criterion, optimizer, epoch=epoch)\n",
    "    test_loss, test_accuracy = eval_training(net, val_loader, criterion, epoch)\n",
    "\n",
    "    if best_acc < test_accuracy[0]:\n",
    "        if weights_path:  # 모델의 용량 이슈로 새로운 best 모델이 갱신되면 이전 best 모델 삭제\n",
    "            os.remove(weights_path)\n",
    "        weights_path = checkpoint_path.format(net=config[\"model_name\"], epoch=epoch, type=\"best\")\n",
    "        print(\"saving weights file to {}\".format(weights_path))\n",
    "        torch.save(net.state_dict(), weights_path)\n",
    "        best_acc = test_accuracy[0]\n",
    "\n",
    "    early_stopping(test_loss, net)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "        \"train_loss\": epoch_loss,\n",
    "        \"train top-1 accuracy\": train_accuracy[0],\n",
    "        \"train top-5 accuracy\": train_accuracy[1],\n",
    "        \"train super-class accuracy\": train_accuracy[2],\n",
    "        \"val_loss\": test_loss,\n",
    "        \"valid top-1 accuracy\": test_accuracy[0],\n",
    "        \"valid top-5 accuracy\": test_accuracy[1],\n",
    "        \"valid super-class accuracy\": test_accuracy[2]\n",
    "    })\n",
    "\n",
    "end_time = time.time()\n",
    "wandb.log({\"train_time\": end_time - start_time})\n",
    "\n",
    "# Result\n",
    "print(f\" Result of ResNet = Epoch : {epoch}   Loss : {test_loss:.4f}   Top-1 Accuracy : {test_accuracy[0]*100:.4f}%  Top-5 Accuracy : {test_accuracy[1]*100:.4f}%   Super Accuracy : {test_accuracy[2]*100:.4f}%   Total_Accuracy : {sum(test_accuracy)*100:.4f}    Time : {end_time - start_time:.4f}\")\n",
    "이걸로 바꿔줘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7dd25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 위의 코드 interrupt 걸어서 중단했을 경우 실행\n",
    "# end_time = time.time()\n",
    "# wandb.log({\"train_time\": end_time - start_time})\n",
    "\n",
    "# # Result\n",
    "# print(f\" Result of ResNet = Epoch : {epoch}   Loss : {test_loss:.4f}   Top-1 Accuracy : {test_accuracy[0]*100:.4f}%  Top-5 Accuracy : {test_accuracy[1]*100:.4f}%   Super Accuracy : {test_accuracy[2]*100:.4f}%   Total_Accuracy : {sum(test_accuracy)*100:.4f}    Time : {end_time - start_time:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p5YcRgjxvB92",
   "metadata": {
    "id": "p5YcRgjxvB92"
   },
   "source": [
    "**Best Model Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "VM1-xrA_3Ug6",
   "metadata": {
    "id": "VM1-xrA_3Ug6"
   },
   "outputs": [],
   "source": [
    "def all_accuracy(net, test_loader, device):\n",
    "    net2.eval()\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    superclass_correct = 0\n",
    "    superclass_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (images, labels) in enumerate(test_loader):\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net2(images)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            top1_correct += torch.sum(preds == labels).item()\n",
    "\n",
    "            _, top5_preds = outputs.topk(5, 1, True, True)\n",
    "            top5_correct += torch.sum(top5_preds.eq(labels.view(-1, 1).expand_as(top5_preds))).item()\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Superclass accuracy\n",
    "            super_preds = torch.tensor([fine_to_superclass[p.item()] for p in preds], dtype=torch.long)\n",
    "            super_labels = torch.tensor([fine_to_superclass[t.item()] for t in labels], dtype=torch.long)\n",
    "            superclass_correct += torch.sum(super_preds == super_labels).item()\n",
    "            superclass_total += super_labels.size(0)\n",
    "\n",
    "    # 세부 클래스 및 슈퍼 클래스 정확도 계산\n",
    "    top1_acc = top1_correct / total\n",
    "    top5_acc = top5_correct / total\n",
    "    superclass_acc = superclass_correct / superclass_total\n",
    "    accuracy = [top1_acc, top5_acc, superclass_acc]\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rVneR6Xh_XfY",
   "metadata": {
    "id": "rVneR6Xh_XfY"
   },
   "outputs": [],
   "source": [
    "net2 = model\n",
    "net2.to(device)\n",
    "net2.eval()\n",
    "\n",
    "# 이 코드로 best 모델이 load 되지 않을 경우 아래 코드에서 경로 직접 지정\n",
    "recent_folder = most_recent_folder(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\"), DATE_FORMAT)\n",
    "if not recent_folder:\n",
    "    raise Exception(\"no recent folder were found\")\n",
    "\n",
    "best_weights = best_acc_weights(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder))\n",
    "if best_weights:\n",
    "    weights_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder, best_weights)\n",
    "    print('found best acc weights file:{}'.format(weights_path))\n",
    "    print('load best training file to test acc...')\n",
    "    net2.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "acc = all_accuracy(net2, test_loader, device)\n",
    "print(\"Top-1 accuracy: {:.4f}\".format(acc[0]))\n",
    "print(\"Top-5 accuracy: {:.4f}\".format(acc[1]))\n",
    "print(\"Super-Class accuracy: {:.4f}\".format(acc[2]))\n",
    "\n",
    "wandb.log({\n",
    "    \"Test Top-1 accuracy\": acc[0],\n",
    "    \"Test Top-5 accuracy\": acc[1],\n",
    "    \"Test Super-Class accuracy\": acc[2],\n",
    "    \"Total Score\": sum(acc)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3exQ4RCI34-1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3exQ4RCI34-1",
    "outputId": "6c819a3d-91ce-48d0-e7f1-495bed210b91"
   },
   "outputs": [],
   "source": [
    "# # 위 코드에서 에러가 발생해 경로 지정이 제대로 되지 않을 경우 사용\n",
    "\n",
    "# net2 = model\n",
    "# net2.to(device)\n",
    "# net2.load_state_dict(torch.load(\"runs/wide_resnet/savepoints/Tuesday_01_October_2024_13h_15m_52s/wide_resnet-229-best.pth\"))\n",
    "\n",
    "# acc = all_accuracy(net2, test_loader, device)\n",
    "# print(\"Top-1 accuracy: {:.4f}\".format(acc[0]))\n",
    "# print(\"Top-5 accuracy: {:.4f}\".format(acc[1]))\n",
    "# print(\"Super-Class accuracy: {:.4f}\".format(acc[2]))\n",
    "# wandb.log({\n",
    "#     \"Test Top-1 accuracy\": acc[0],\n",
    "#     \"Test Top-5 accuracy\": acc[1],\n",
    "#     \"Test Super-Class accuracy\": acc[2],\n",
    "#     \"Total Score\": sum(acc)\n",
    "#     })\n",
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc7f408",
   "metadata": {
    "id": "cdc7f408"
   },
   "source": [
    "**Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8404d10-a77a-4544-96d1-70cb306e2a05",
   "metadata": {
    "id": "d8404d10-a77a-4544-96d1-70cb306e2a05"
   },
   "outputs": [],
   "source": [
    "import wandb.sklearn\n",
    "\n",
    "classes = train_val_data.classes\n",
    "coarse_classes = [\n",
    "    'aquatic mammals', 'fish', 'flowers', 'food containers', 'fruit and vegetables', 'household electrical devices',\n",
    "    'household furniture', 'insects', 'large carnivores', 'large man-made outdoor things',\n",
    "    'large natural outdoor scenes', 'large omnivores and herbivores', 'medium-sized mammals',\n",
    "    'non-insect invertebrates', 'people', 'reptiles', 'small mammals', 'trees', 'vehicles 1', 'vehicles 2'\n",
    "]\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "y_probs = []\n",
    "\n",
    "for x, y in torch.utils.data.DataLoader(dataset=test_data, batch_size=config[\"batch_size\"]):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    z = net2(x)\n",
    "    _, yhat = torch.max(z, 1)\n",
    "    probs = z.softmax(dim=1).cpu().detach().numpy()\n",
    "\n",
    "    pred = yhat.data.cpu().numpy()\n",
    "    \n",
    "    super_pred = [coarse_classes[fine_to_superclass[p]] for p in pred]\n",
    "    y_pred.extend(super_pred)\n",
    "\n",
    "    labels = y.data.cpu().numpy()\n",
    "    super_labels = [coarse_classes[fine_to_superclass[l]] for l in labels]\n",
    "    y_true.extend(super_labels)\n",
    "\n",
    "    y_probs.extend(probs)\n",
    "    \n",
    "cf_matrix = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "\n",
    "df_cm = pd.DataFrame(cf_matrix, index=coarse_classes,\n",
    "                     columns=coarse_classes)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='.2f', cmap='Blues')\n",
    "plt.title(f'Confusion Matrix of {config[\"model_name\"]} (CIFAR-100 Superclass)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Real')\n",
    "\n",
    "conf_matrix_img = wandb.Image(plt.gcf())\n",
    "plt.close()\n",
    "\n",
    "class_report = classification_report(y_true, y_pred, labels=coarse_classes, zero_division=0)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.text(0.01, 0.05, str(class_report), {'fontsize': 12}, fontproperties='monospace')  # monospaced font\n",
    "plt.axis('off')  # 축 제거\n",
    "plt.tight_layout()\n",
    "\n",
    "class_report_img = wandb.Image(plt.gcf())\n",
    "plt.close()\n",
    "\n",
    "wandb.log({\n",
    "    \"Confusion Matrix\": conf_matrix_img,\n",
    "    \"Classification Report\": class_report_img\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c9076a5-82f4-4ec9-9903-5f20c093aa6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Super-Class accuracy</td><td>▁</td></tr><tr><td>Test Top-1 accuracy</td><td>▁</td></tr><tr><td>Test Top-5 accuracy</td><td>▁</td></tr><tr><td>Total Score</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>learning_rate</td><td>██████████▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train super-class accuracy</td><td>▁▃▅▅▅▅▅▆▆▆▆▇████████████████████████████</td></tr><tr><td>train top-1 accuracy</td><td>▁▃▃▃▃▄▄▄▄▄██▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>train top-5 accuracy</td><td>▁▆▇▇▇▇▇▇▇███████████████████████████████</td></tr><tr><td>train_loss</td><td>█▇▅▄▃▃▃▃▃▃▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_time</td><td>▁</td></tr><tr><td>val_loss</td><td>█▄▃▃▃▃▂▃▃▃▁▁▁▁▁▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid super-class accuracy</td><td>▁▃▄▄▅▅▅▅▅▅▅▅▅█▇▇▇▇▇▇▇███████████████████</td></tr><tr><td>valid top-1 accuracy</td><td>▁▃▃▃▄▄▃▃▄▄▃▂▇▇▇▆▆▆▆▆▆▆██████████████████</td></tr><tr><td>valid top-5 accuracy</td><td>▁▂▂▄▃▃▄▄▄▄▃▄█▇▇▇▇▆▆▆████████████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Super-Class accuracy</td><td>0.8466</td></tr><tr><td>Test Top-1 accuracy</td><td>0.7576</td></tr><tr><td>Test Top-5 accuracy</td><td>0.9286</td></tr><tr><td>Total Score</td><td>2.5328</td></tr><tr><td>epoch</td><td>250</td></tr><tr><td>learning_rate</td><td>0.00016</td></tr><tr><td>train super-class accuracy</td><td>0.99992</td></tr><tr><td>train top-1 accuracy</td><td>0.9998</td></tr><tr><td>train top-5 accuracy</td><td>1</td></tr><tr><td>train_loss</td><td>0.00942</td></tr><tr><td>train_time</td><td>13554.57328</td></tr><tr><td>val_loss</td><td>0.00789</td></tr><tr><td>valid super-class accuracy</td><td>0.8381</td></tr><tr><td>valid top-1 accuracy</td><td>0.7493</td></tr><tr><td>valid top-5 accuracy</td><td>0.9281</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resnet_18_base</strong> at: <a href='https://wandb.ai/hyunwook7120-hanyang-university/CIFAR-100_Classification/runs/3c4eoo5o' target=\"_blank\">https://wandb.ai/hyunwook7120-hanyang-university/CIFAR-100_Classification/runs/3c4eoo5o</a><br/> View project at: <a href='https://wandb.ai/hyunwook7120-hanyang-university/CIFAR-100_Classification' target=\"_blank\">https://wandb.ai/hyunwook7120-hanyang-university/CIFAR-100_Classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 2 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241006_132105-3c4eoo5o\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d3a3b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
