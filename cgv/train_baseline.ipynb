{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab70dee5-0563-44bf-aac5-fd2065985481",
   "metadata": {},
   "source": [
    "# **Data - 나영**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e457d367-10af-4473-8f3d-77198447a7bb",
   "metadata": {},
   "source": [
    "### **Load Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "562d9cb0-9691-4f07-a788-38b1f17bb4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import torchvision.models as models\n",
    "from utility.early_stopping import EarlyStopping\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f8e0bb-e8d3-45cf-a26f-2405ff3fce13",
   "metadata": {},
   "source": [
    "**Seed Setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d84c3f64-0389-4b7c-83c8-a32ee52e3738",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d348e605-f472-4e8b-9e59-d7f9971c7c55",
   "metadata": {},
   "source": [
    "**Device Setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "035d096c-7962-4214-a220-c5b27b3d04d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2a04d-7b38-4826-b55c-f31ad07811d9",
   "metadata": {},
   "source": [
    "**Set Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4225472f-8390-4f0e-b3ea-85598022cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936f9c8-6a53-4d17-8b05-2f7bbde99746",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb77a99-0ecc-4dee-8373-8f73d0814215",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomCrop(32, padding=4), \n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2bee68-5466-4bdc-bbaa-2ae55ca6463e",
   "metadata": {},
   "source": [
    "### **Load Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3ac53-39cd-458e-802e-6b7da0170f63",
   "metadata": {},
   "source": [
    "**Splitting th training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2cf780b-d6ba-4123-a0c1-be7a7a762575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_val_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_val_transform)\n",
    "test_data = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "257be9c9-628b-4ef4-8f1e-a9466ee8d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_transform)\n",
    "# test_data = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f00da17f-0962-4907-8aae-505f5a64cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 데이터를 train/val로 나누기\n",
    "num_train = len(train_val_data)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(0.2 * num_train))  # validation 데이터를 20%로 설정\n",
    "\n",
    "np.random.shuffle(indices)\n",
    "train_idx, val_idx = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "val_sampler = SubsetRandomSampler(val_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b3c41-7fe8-4959-81b8-50354ef33c9c",
   "metadata": {},
   "source": [
    "**Define DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6133b787-b855-4064-b019-88a62b64d56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_val_data, batch_size=batch_size, sampler=train_sampler, num_workers=2)\n",
    "val_loader = DataLoader(train_val_data, batch_size=batch_size, sampler=val_sampler, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b18f60-5d55-4b53-9506-788bcb425362",
   "metadata": {},
   "source": [
    "# **Model - 하연**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb4694-6995-4316-8009-49c47d4d5fdd",
   "metadata": {},
   "source": [
    "models 폴더에 만들고 import 하는 식으로 해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba9548e-68bc-4b50-8f43-75c084ae7f3d",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16c71685-4257-4da5-8f8b-6d8f8945f4d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use: cuda:0\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "         MaxPool2d-3           [-1, 64, 56, 56]               0\n",
      "            Conv2d-4           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-5           [-1, 64, 56, 56]             128\n",
      "            Conv2d-6           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
      "        BasicBlock-8           [-1, 64, 56, 56]               0\n",
      "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
      "           Conv2d-11           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-12           [-1, 64, 56, 56]             128\n",
      "       BasicBlock-13           [-1, 64, 56, 56]               0\n",
      "           Conv2d-14          [-1, 128, 28, 28]          73,728\n",
      "      BatchNorm2d-15          [-1, 128, 28, 28]             256\n",
      "           Conv2d-16          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-17          [-1, 128, 28, 28]             256\n",
      "           Conv2d-18          [-1, 128, 28, 28]           8,192\n",
      "      BatchNorm2d-19          [-1, 128, 28, 28]             256\n",
      "       BasicBlock-20          [-1, 128, 28, 28]               0\n",
      "           Conv2d-21          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-22          [-1, 128, 28, 28]             256\n",
      "           Conv2d-23          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-24          [-1, 128, 28, 28]             256\n",
      "       BasicBlock-25          [-1, 128, 28, 28]               0\n",
      "           Conv2d-26          [-1, 256, 14, 14]         294,912\n",
      "      BatchNorm2d-27          [-1, 256, 14, 14]             512\n",
      "           Conv2d-28          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-29          [-1, 256, 14, 14]             512\n",
      "           Conv2d-30          [-1, 256, 14, 14]          32,768\n",
      "      BatchNorm2d-31          [-1, 256, 14, 14]             512\n",
      "       BasicBlock-32          [-1, 256, 14, 14]               0\n",
      "           Conv2d-33          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-34          [-1, 256, 14, 14]             512\n",
      "           Conv2d-35          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
      "       BasicBlock-37          [-1, 256, 14, 14]               0\n",
      "           Conv2d-38            [-1, 512, 7, 7]       1,179,648\n",
      "      BatchNorm2d-39            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-40            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-41            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-42            [-1, 512, 7, 7]         131,072\n",
      "      BatchNorm2d-43            [-1, 512, 7, 7]           1,024\n",
      "       BasicBlock-44            [-1, 512, 7, 7]               0\n",
      "           Conv2d-45            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-46            [-1, 512, 7, 7]           1,024\n",
      "           Conv2d-47            [-1, 512, 7, 7]       2,359,296\n",
      "      BatchNorm2d-48            [-1, 512, 7, 7]           1,024\n",
      "       BasicBlock-49            [-1, 512, 7, 7]               0\n",
      "          Dropout-50                  [-1, 512]               0\n",
      "           Linear-51                  [-1, 100]          51,300\n",
      "================================================================\n",
      "Total params: 11,227,812\n",
      "Trainable params: 11,227,812\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 45.18\n",
      "Params size (MB): 42.83\n",
      "Estimated Total Size (MB): 88.58\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# # models 폴더의 경로 추가\n",
    "# sys.path.append('./models')\n",
    "\n",
    "print(\"use:\", device)\n",
    "\n",
    "# 모델 import 하기\n",
    "from models.resnetRS import ResNetRS18\n",
    "\n",
    "# 모델 초기화\n",
    "net = ResNetRS18()\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "net.to(device)\n",
    "\n",
    "# 모델 구조 출력\n",
    "print(summary(net, (3, 224, 224)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae781b4f-9e8b-4ea3-8b1e-9b67e857a088",
   "metadata": {},
   "source": [
    "### **Loss and Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a1b668f-0288-43c3-a08a-83a11a0d85d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61]}]\n"
     ]
    }
   ],
   "source": [
    "# 손실함수 초기화\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저 초기화\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# 옵티마이저의 state_dict 출력\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f09003b-468e-4e89-9285-b6ec501c20cf",
   "metadata": {},
   "source": [
    "# **Train - 하연**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a80805-e7a2-4bdc-b854-9d183c306bf5",
   "metadata": {},
   "source": [
    "### **Model Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4337e604-8477-4a57-80fc-a2d96afcb153",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./runs/resnet_18/tensorboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13f6bb1b-efc7-4146-acbb-b2b28de16580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = \"./runs/resnet_18/checkpoints\"\n",
    "# early_stopping = EarlyStopping(save_path)\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "17b20456-0576-49db-b3d1-97ef4cb49d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 함수\n",
    "def train_model(model, trainloader, criterion, optimizer, num_epochs=25):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "            labels = labels.type(torch.LongTensor).to(device)  # CPU에서 long type tensor로 변환\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 모델 예측\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # 역전파 및 최적화\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # 30번째 배치마다 상태 출력\n",
    "            if (batch_idx + 1) % 30 == 0:\n",
    "                print(f\"Batch [{batch_idx+1}/{len(trainloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Epoch당 평균 손실 계산 및 출력\n",
    "        epoch_loss = running_loss / len(trainloader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping 체크\n",
    "        early_stopping(epoch_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825de7e7-59bc-49aa-bd7a-c27a61c10fcb",
   "metadata": {},
   "source": [
    "**Model Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15180770-96df-487a-8c70-5743cc4dd448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 및 테스트 함수 (superclass 예측 포함)\n",
    "def test_model(model, testloader, criterion, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # 모델 예측\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "            \n",
    "            # 예측 결과 저장 및 정확도 계산\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (pred == labels).sum().item()\n",
    "\n",
    "            # TensorBoard에 테스트 손실 및 정확도 기록\n",
    "            writer.add_scalar(\"Test Loss\", test_loss / len(testloader.dataset), epoch)\n",
    "            writer.add_scalar(\"Test Accuracy\", correct / len(testloader.dataset), epoch)\n",
    "\n",
    "    # 평균 손실 및 정확도 계산\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    accuracy = correct / len(testloader.dataset)\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0669a96-2d88-4b8b-bc33-82701eabf923",
   "metadata": {},
   "source": [
    "### **Per-Epoch Activity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c330ce51-1f1d-4ad8-a127-a82f34c3899e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [30/313], Loss: 5.3023\n",
      "Batch [60/313], Loss: 5.1915\n",
      "Batch [90/313], Loss: 4.7582\n",
      "Batch [120/313], Loss: 4.4300\n",
      "Batch [150/313], Loss: 4.2861\n",
      "Batch [180/313], Loss: 4.2456\n",
      "Batch [210/313], Loss: 4.3841\n",
      "Batch [240/313], Loss: 4.3330\n",
      "Batch [270/313], Loss: 4.1105\n",
      "Batch [300/313], Loss: 4.3609\n",
      "Epoch [1/20], Loss: 3.7049\n",
      "Validation loss decreased (inf --> 3.704945).  Saving model ...\n",
      "Batch [30/313], Loss: 4.0137\n",
      "Batch [60/313], Loss: 4.0519\n",
      "Batch [90/313], Loss: 3.9849\n",
      "Batch [120/313], Loss: 4.0932\n",
      "Batch [150/313], Loss: 3.9244\n",
      "Batch [180/313], Loss: 3.7676\n",
      "Batch [210/313], Loss: 3.8219\n",
      "Batch [240/313], Loss: 3.8011\n",
      "Batch [270/313], Loss: 3.8658\n",
      "Batch [300/313], Loss: 3.9379\n",
      "Epoch [2/20], Loss: 3.2026\n",
      "Validation loss decreased (3.704945 --> 3.202595).  Saving model ...\n",
      "Batch [30/313], Loss: 3.7803\n",
      "Batch [60/313], Loss: 3.7133\n",
      "Batch [90/313], Loss: 4.0542\n",
      "Batch [120/313], Loss: 3.7856\n",
      "Batch [150/313], Loss: 3.7804\n",
      "Batch [180/313], Loss: 3.7771\n",
      "Batch [210/313], Loss: 3.8058\n",
      "Batch [240/313], Loss: 3.7100\n",
      "Batch [270/313], Loss: 3.6624\n",
      "Batch [300/313], Loss: 3.4154\n",
      "Epoch [3/20], Loss: 3.0141\n",
      "Validation loss decreased (3.202595 --> 3.014080).  Saving model ...\n",
      "Batch [30/313], Loss: 3.6350\n",
      "Batch [60/313], Loss: 3.4335\n",
      "Batch [90/313], Loss: 3.5907\n",
      "Batch [120/313], Loss: 3.7584\n",
      "Batch [150/313], Loss: 3.5568\n",
      "Batch [180/313], Loss: 3.8296\n",
      "Batch [210/313], Loss: 3.6058\n",
      "Batch [240/313], Loss: 3.4799\n",
      "Batch [270/313], Loss: 3.5903\n",
      "Batch [300/313], Loss: 3.4864\n",
      "Epoch [4/20], Loss: 2.8734\n",
      "Validation loss decreased (3.014080 --> 2.873408).  Saving model ...\n",
      "Batch [30/313], Loss: 3.5529\n",
      "Batch [60/313], Loss: 3.6068\n",
      "Batch [90/313], Loss: 3.4002\n",
      "Batch [120/313], Loss: 3.2593\n",
      "Batch [150/313], Loss: 3.3988\n",
      "Batch [180/313], Loss: 3.3429\n",
      "Batch [210/313], Loss: 3.3773\n",
      "Batch [240/313], Loss: 3.8082\n",
      "Batch [270/313], Loss: 3.3357\n",
      "Batch [300/313], Loss: 3.1974\n",
      "Epoch [5/20], Loss: 2.7582\n",
      "Validation loss decreased (2.873408 --> 2.758207).  Saving model ...\n",
      "Batch [30/313], Loss: 3.5844\n",
      "Batch [60/313], Loss: 3.3571\n",
      "Batch [90/313], Loss: 3.2722\n",
      "Batch [120/313], Loss: 3.4010\n",
      "Batch [150/313], Loss: 3.2606\n",
      "Batch [180/313], Loss: 3.2600\n",
      "Batch [210/313], Loss: 3.1644\n",
      "Batch [240/313], Loss: 3.4438\n",
      "Batch [270/313], Loss: 3.3136\n",
      "Batch [300/313], Loss: 3.4651\n",
      "Epoch [6/20], Loss: 2.6575\n",
      "Validation loss decreased (2.758207 --> 2.657479).  Saving model ...\n",
      "Batch [30/313], Loss: 3.0896\n",
      "Batch [60/313], Loss: 3.1817\n",
      "Batch [90/313], Loss: 3.3723\n",
      "Batch [120/313], Loss: 3.1216\n",
      "Batch [150/313], Loss: 3.2850\n",
      "Batch [180/313], Loss: 3.2877\n",
      "Batch [210/313], Loss: 3.4656\n",
      "Batch [240/313], Loss: 3.3186\n",
      "Batch [270/313], Loss: 3.2983\n",
      "Batch [300/313], Loss: 3.1684\n",
      "Epoch [7/20], Loss: 2.5662\n",
      "Validation loss decreased (2.657479 --> 2.566179).  Saving model ...\n",
      "Batch [30/313], Loss: 3.2640\n",
      "Batch [60/313], Loss: 3.2884\n",
      "Batch [90/313], Loss: 2.9843\n",
      "Batch [120/313], Loss: 3.0014\n",
      "Batch [150/313], Loss: 3.1562\n",
      "Batch [180/313], Loss: 3.2567\n",
      "Batch [210/313], Loss: 3.5270\n",
      "Batch [240/313], Loss: 3.1366\n",
      "Batch [270/313], Loss: 3.2974\n",
      "Batch [300/313], Loss: 3.2805\n",
      "Epoch [8/20], Loss: 2.4927\n",
      "Validation loss decreased (2.566179 --> 2.492658).  Saving model ...\n",
      "Batch [30/313], Loss: 3.0780\n",
      "Batch [60/313], Loss: 3.3015\n",
      "Batch [90/313], Loss: 3.3023\n",
      "Batch [120/313], Loss: 2.9467\n",
      "Batch [150/313], Loss: 3.1277\n",
      "Batch [180/313], Loss: 3.1549\n",
      "Batch [210/313], Loss: 3.2124\n",
      "Batch [240/313], Loss: 3.2134\n",
      "Batch [270/313], Loss: 2.8992\n",
      "Batch [300/313], Loss: 3.0450\n",
      "Epoch [9/20], Loss: 2.4137\n",
      "Validation loss decreased (2.492658 --> 2.413700).  Saving model ...\n",
      "Batch [30/313], Loss: 3.0189\n",
      "Batch [60/313], Loss: 2.9984\n",
      "Batch [90/313], Loss: 3.0935\n",
      "Batch [120/313], Loss: 2.7062\n",
      "Batch [150/313], Loss: 2.8656\n",
      "Batch [180/313], Loss: 2.8163\n",
      "Batch [210/313], Loss: 2.6582\n",
      "Batch [240/313], Loss: 2.8407\n",
      "Batch [270/313], Loss: 3.1145\n",
      "Batch [300/313], Loss: 3.0131\n",
      "Epoch [10/20], Loss: 2.3413\n",
      "Validation loss decreased (2.413700 --> 2.341350).  Saving model ...\n",
      "Batch [30/313], Loss: 2.9991\n",
      "Batch [60/313], Loss: 2.8394\n",
      "Batch [90/313], Loss: 3.0334\n",
      "Batch [120/313], Loss: 2.6897\n",
      "Batch [150/313], Loss: 3.2232\n",
      "Batch [180/313], Loss: 2.8827\n",
      "Batch [210/313], Loss: 2.6782\n",
      "Batch [240/313], Loss: 2.9008\n",
      "Batch [270/313], Loss: 2.9303\n",
      "Batch [300/313], Loss: 2.8679\n",
      "Epoch [11/20], Loss: 2.2727\n",
      "Validation loss decreased (2.341350 --> 2.272669).  Saving model ...\n",
      "Batch [30/313], Loss: 2.8399\n",
      "Batch [60/313], Loss: 2.7757\n",
      "Batch [90/313], Loss: 2.7953\n",
      "Batch [120/313], Loss: 2.7729\n",
      "Batch [150/313], Loss: 2.5513\n",
      "Batch [180/313], Loss: 2.4495\n",
      "Batch [210/313], Loss: 2.6716\n",
      "Batch [240/313], Loss: 2.6775\n",
      "Batch [270/313], Loss: 2.8162\n",
      "Batch [300/313], Loss: 2.7285\n",
      "Epoch [12/20], Loss: 2.2208\n",
      "Validation loss decreased (2.272669 --> 2.220837).  Saving model ...\n",
      "Batch [30/313], Loss: 2.8964\n",
      "Batch [60/313], Loss: 2.7027\n",
      "Batch [90/313], Loss: 2.7998\n",
      "Batch [120/313], Loss: 2.7599\n",
      "Batch [150/313], Loss: 2.6396\n",
      "Batch [180/313], Loss: 2.6132\n",
      "Batch [210/313], Loss: 2.8063\n",
      "Batch [240/313], Loss: 2.5961\n",
      "Batch [270/313], Loss: 2.7503\n",
      "Batch [300/313], Loss: 2.8462\n",
      "Epoch [13/20], Loss: 2.1592\n",
      "Validation loss decreased (2.220837 --> 2.159164).  Saving model ...\n",
      "Batch [30/313], Loss: 2.5945\n",
      "Batch [60/313], Loss: 2.5615\n",
      "Batch [90/313], Loss: 2.4096\n",
      "Batch [120/313], Loss: 2.7223\n",
      "Batch [150/313], Loss: 2.8081\n",
      "Batch [180/313], Loss: 2.6401\n",
      "Batch [210/313], Loss: 2.6704\n",
      "Batch [240/313], Loss: 2.7054\n",
      "Batch [270/313], Loss: 2.9695\n",
      "Batch [300/313], Loss: 2.5217\n",
      "Epoch [14/20], Loss: 2.1098\n",
      "Validation loss decreased (2.159164 --> 2.109822).  Saving model ...\n",
      "Batch [30/313], Loss: 2.3590\n",
      "Batch [60/313], Loss: 2.6736\n",
      "Batch [90/313], Loss: 2.3911\n",
      "Batch [120/313], Loss: 2.6912\n",
      "Batch [150/313], Loss: 2.4575\n",
      "Batch [180/313], Loss: 2.6499\n",
      "Batch [210/313], Loss: 2.3657\n",
      "Batch [240/313], Loss: 2.5305\n",
      "Batch [270/313], Loss: 2.5508\n",
      "Batch [300/313], Loss: 2.3123\n",
      "Epoch [15/20], Loss: 2.0584\n",
      "Validation loss decreased (2.109822 --> 2.058375).  Saving model ...\n",
      "Batch [30/313], Loss: 2.5532\n",
      "Batch [60/313], Loss: 2.5177\n",
      "Batch [90/313], Loss: 2.3531\n",
      "Batch [120/313], Loss: 2.7241\n",
      "Batch [150/313], Loss: 2.6147\n",
      "Batch [180/313], Loss: 2.3028\n",
      "Batch [210/313], Loss: 2.3594\n",
      "Batch [240/313], Loss: 2.7690\n",
      "Batch [270/313], Loss: 2.6397\n",
      "Batch [300/313], Loss: 2.5180\n",
      "Epoch [16/20], Loss: 2.0171\n",
      "Validation loss decreased (2.058375 --> 2.017064).  Saving model ...\n",
      "Batch [30/313], Loss: 2.4914\n",
      "Batch [60/313], Loss: 2.8703\n",
      "Batch [90/313], Loss: 2.6514\n",
      "Batch [120/313], Loss: 2.2340\n",
      "Batch [150/313], Loss: 2.4421\n",
      "Batch [180/313], Loss: 2.5427\n",
      "Batch [210/313], Loss: 2.3988\n",
      "Batch [240/313], Loss: 2.3711\n",
      "Batch [270/313], Loss: 2.2735\n",
      "Batch [300/313], Loss: 2.8284\n",
      "Epoch [17/20], Loss: 1.9723\n",
      "Validation loss decreased (2.017064 --> 1.972294).  Saving model ...\n",
      "Batch [30/313], Loss: 2.5040\n",
      "Batch [60/313], Loss: 1.9718\n",
      "Batch [90/313], Loss: 2.2944\n",
      "Batch [120/313], Loss: 2.4271\n",
      "Batch [150/313], Loss: 2.4309\n",
      "Batch [180/313], Loss: 2.6923\n",
      "Batch [210/313], Loss: 2.2566\n",
      "Batch [240/313], Loss: 2.6617\n",
      "Batch [270/313], Loss: 2.4488\n",
      "Batch [300/313], Loss: 2.3030\n",
      "Epoch [18/20], Loss: 1.9201\n",
      "Validation loss decreased (1.972294 --> 1.920121).  Saving model ...\n",
      "Batch [30/313], Loss: 2.1018\n",
      "Batch [60/313], Loss: 2.3214\n",
      "Batch [90/313], Loss: 2.2490\n",
      "Batch [120/313], Loss: 2.2251\n",
      "Batch [150/313], Loss: 2.4518\n",
      "Batch [180/313], Loss: 2.1468\n",
      "Batch [210/313], Loss: 2.3086\n",
      "Batch [240/313], Loss: 2.3744\n",
      "Batch [270/313], Loss: 2.2880\n",
      "Batch [300/313], Loss: 2.3818\n",
      "Epoch [19/20], Loss: 1.8863\n",
      "Validation loss decreased (1.920121 --> 1.886314).  Saving model ...\n",
      "Batch [30/313], Loss: 2.4436\n",
      "Batch [60/313], Loss: 2.4048\n",
      "Batch [90/313], Loss: 2.4488\n",
      "Batch [120/313], Loss: 2.4285\n",
      "Batch [150/313], Loss: 2.2218\n",
      "Batch [180/313], Loss: 2.2724\n",
      "Batch [210/313], Loss: 2.5191\n",
      "Batch [240/313], Loss: 2.5484\n",
      "Batch [270/313], Loss: 2.3464\n",
      "Batch [300/313], Loss: 2.1047\n",
      "Epoch [20/20], Loss: 1.8449\n",
      "Validation loss decreased (1.886314 --> 1.844894).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██▏                                        | 1/20 [02:13<42:16, 133.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]   Loss: 2.4096   Accuracy: 38.69%\n",
      "Batch [30/313], Loss: 2.1739\n",
      "Batch [60/313], Loss: 2.1746\n",
      "Batch [90/313], Loss: 1.9595\n",
      "Batch [120/313], Loss: 2.2605\n",
      "Batch [150/313], Loss: 2.0449\n",
      "Batch [180/313], Loss: 2.1386\n",
      "Batch [210/313], Loss: 2.4236\n",
      "Batch [240/313], Loss: 2.1114\n",
      "Batch [270/313], Loss: 2.5650\n",
      "Batch [300/313], Loss: 2.2065\n",
      "Epoch [1/20], Loss: 1.8136\n",
      "Validation loss decreased (1.844894 --> 1.813579).  Saving model ...\n",
      "Batch [30/313], Loss: 2.1287\n",
      "Batch [60/313], Loss: 2.2129\n",
      "Batch [90/313], Loss: 2.1800\n",
      "Batch [120/313], Loss: 2.0237\n",
      "Batch [150/313], Loss: 2.5200\n",
      "Batch [180/313], Loss: 2.2684\n",
      "Batch [210/313], Loss: 2.0880\n",
      "Batch [240/313], Loss: 2.1274\n",
      "Batch [270/313], Loss: 2.1275\n",
      "Batch [300/313], Loss: 2.0116\n",
      "Epoch [2/20], Loss: 1.7739\n",
      "Validation loss decreased (1.813579 --> 1.773912).  Saving model ...\n",
      "Batch [30/313], Loss: 2.1107\n",
      "Batch [60/313], Loss: 2.3633\n",
      "Batch [90/313], Loss: 1.9269\n",
      "Batch [120/313], Loss: 1.9945\n",
      "Batch [150/313], Loss: 2.0662\n",
      "Batch [180/313], Loss: 2.1056\n",
      "Batch [210/313], Loss: 2.2136\n",
      "Batch [240/313], Loss: 2.0478\n",
      "Batch [270/313], Loss: 2.4518\n",
      "Batch [300/313], Loss: 2.2064\n",
      "Epoch [3/20], Loss: 1.7396\n",
      "Validation loss decreased (1.773912 --> 1.739560).  Saving model ...\n",
      "Batch [30/313], Loss: 1.9524\n",
      "Batch [60/313], Loss: 2.1784\n",
      "Batch [90/313], Loss: 1.9825\n",
      "Batch [120/313], Loss: 2.0248\n",
      "Batch [150/313], Loss: 2.4926\n",
      "Batch [180/313], Loss: 2.1641\n",
      "Batch [210/313], Loss: 2.1645\n",
      "Batch [240/313], Loss: 2.3036\n",
      "Batch [270/313], Loss: 2.0364\n",
      "Batch [300/313], Loss: 2.2095\n",
      "Epoch [4/20], Loss: 1.7061\n",
      "Validation loss decreased (1.739560 --> 1.706119).  Saving model ...\n",
      "Batch [30/313], Loss: 1.9152\n",
      "Batch [60/313], Loss: 2.0403\n",
      "Batch [90/313], Loss: 2.1307\n",
      "Batch [120/313], Loss: 2.0000\n",
      "Batch [150/313], Loss: 2.1485\n",
      "Batch [180/313], Loss: 2.2785\n",
      "Batch [210/313], Loss: 2.1517\n",
      "Batch [240/313], Loss: 1.9237\n",
      "Batch [270/313], Loss: 2.0116\n",
      "Batch [300/313], Loss: 1.8201\n",
      "Epoch [5/20], Loss: 1.6713\n",
      "Validation loss decreased (1.706119 --> 1.671258).  Saving model ...\n",
      "Batch [30/313], Loss: 2.0000\n",
      "Batch [60/313], Loss: 1.7857\n",
      "Batch [90/313], Loss: 2.1264\n",
      "Batch [120/313], Loss: 2.1906\n",
      "Batch [150/313], Loss: 2.3175\n",
      "Batch [180/313], Loss: 1.8385\n",
      "Batch [210/313], Loss: 2.0128\n",
      "Batch [240/313], Loss: 2.1542\n",
      "Batch [270/313], Loss: 2.1066\n",
      "Batch [300/313], Loss: 1.9939\n",
      "Epoch [6/20], Loss: 1.6322\n",
      "Validation loss decreased (1.671258 --> 1.632224).  Saving model ...\n",
      "Batch [30/313], Loss: 1.8945\n",
      "Batch [60/313], Loss: 2.2683\n",
      "Batch [90/313], Loss: 1.8819\n",
      "Batch [120/313], Loss: 1.9499\n",
      "Batch [150/313], Loss: 1.9708\n",
      "Batch [180/313], Loss: 2.6418\n",
      "Batch [210/313], Loss: 2.0677\n",
      "Batch [240/313], Loss: 2.3266\n",
      "Batch [270/313], Loss: 1.8555\n",
      "Batch [300/313], Loss: 1.8683\n",
      "Epoch [7/20], Loss: 1.6157\n",
      "Validation loss decreased (1.632224 --> 1.615736).  Saving model ...\n",
      "Batch [30/313], Loss: 1.8842\n",
      "Batch [60/313], Loss: 1.7679\n",
      "Batch [90/313], Loss: 1.8620\n",
      "Batch [120/313], Loss: 2.0814\n",
      "Batch [150/313], Loss: 2.0873\n",
      "Batch [180/313], Loss: 1.9705\n",
      "Batch [210/313], Loss: 1.9173\n",
      "Batch [240/313], Loss: 1.8780\n",
      "Batch [270/313], Loss: 2.2804\n",
      "Batch [300/313], Loss: 1.9794\n",
      "Epoch [8/20], Loss: 1.5789\n",
      "Validation loss decreased (1.615736 --> 1.578873).  Saving model ...\n",
      "Batch [30/313], Loss: 2.0602\n",
      "Batch [60/313], Loss: 1.8105\n",
      "Batch [90/313], Loss: 1.6762\n",
      "Batch [120/313], Loss: 1.7500\n",
      "Batch [150/313], Loss: 1.9415\n",
      "Batch [180/313], Loss: 2.1264\n",
      "Batch [210/313], Loss: 1.9121\n",
      "Batch [240/313], Loss: 2.1843\n",
      "Batch [270/313], Loss: 1.8931\n",
      "Batch [300/313], Loss: 2.0398\n",
      "Epoch [9/20], Loss: 1.5384\n",
      "Validation loss decreased (1.578873 --> 1.538363).  Saving model ...\n",
      "Batch [30/313], Loss: 1.8332\n",
      "Batch [60/313], Loss: 1.9250\n",
      "Batch [90/313], Loss: 1.8921\n",
      "Batch [120/313], Loss: 1.7415\n",
      "Batch [150/313], Loss: 1.9756\n",
      "Batch [180/313], Loss: 2.0317\n",
      "Batch [210/313], Loss: 2.1614\n",
      "Batch [240/313], Loss: 2.0136\n",
      "Batch [270/313], Loss: 1.9595\n",
      "Batch [300/313], Loss: 1.9424\n",
      "Epoch [10/20], Loss: 1.5150\n",
      "Validation loss decreased (1.538363 --> 1.515034).  Saving model ...\n",
      "Batch [30/313], Loss: 1.6215\n",
      "Batch [60/313], Loss: 2.1131\n",
      "Batch [90/313], Loss: 2.1332\n",
      "Batch [120/313], Loss: 1.8153\n",
      "Batch [150/313], Loss: 1.8358\n",
      "Batch [180/313], Loss: 1.8347\n",
      "Batch [210/313], Loss: 1.8192\n",
      "Batch [240/313], Loss: 1.8134\n",
      "Batch [270/313], Loss: 2.0438\n",
      "Batch [300/313], Loss: 1.8294\n",
      "Epoch [11/20], Loss: 1.4847\n",
      "Validation loss decreased (1.515034 --> 1.484738).  Saving model ...\n",
      "Batch [30/313], Loss: 1.6803\n",
      "Batch [60/313], Loss: 1.5912\n",
      "Batch [90/313], Loss: 1.5115\n",
      "Batch [120/313], Loss: 1.9861\n",
      "Batch [150/313], Loss: 2.0142\n",
      "Batch [180/313], Loss: 1.8214\n",
      "Batch [210/313], Loss: 1.7860\n",
      "Batch [240/313], Loss: 1.6493\n",
      "Batch [270/313], Loss: 1.5868\n",
      "Batch [300/313], Loss: 1.8451\n",
      "Epoch [12/20], Loss: 1.4602\n",
      "Validation loss decreased (1.484738 --> 1.460179).  Saving model ...\n",
      "Batch [30/313], Loss: 1.8368\n",
      "Batch [60/313], Loss: 1.7696\n",
      "Batch [90/313], Loss: 1.9809\n",
      "Batch [120/313], Loss: 1.7550\n",
      "Batch [150/313], Loss: 1.7923\n",
      "Batch [180/313], Loss: 1.7031\n",
      "Batch [210/313], Loss: 1.8511\n",
      "Batch [240/313], Loss: 1.7793\n",
      "Batch [270/313], Loss: 1.9679\n",
      "Batch [300/313], Loss: 1.7890\n",
      "Epoch [13/20], Loss: 1.4232\n",
      "Validation loss decreased (1.460179 --> 1.423162).  Saving model ...\n",
      "Batch [30/313], Loss: 1.6888\n",
      "Batch [60/313], Loss: 1.5219\n",
      "Batch [90/313], Loss: 1.4845\n",
      "Batch [120/313], Loss: 1.5820\n",
      "Batch [150/313], Loss: 1.8992\n",
      "Batch [180/313], Loss: 1.6659\n",
      "Batch [210/313], Loss: 1.7863\n",
      "Batch [240/313], Loss: 1.5151\n",
      "Batch [270/313], Loss: 1.9112\n",
      "Batch [300/313], Loss: 1.7492\n",
      "Epoch [14/20], Loss: 1.3951\n",
      "Validation loss decreased (1.423162 --> 1.395113).  Saving model ...\n",
      "Batch [30/313], Loss: 1.5898\n",
      "Batch [60/313], Loss: 1.7767\n",
      "Batch [90/313], Loss: 1.8613\n",
      "Batch [120/313], Loss: 1.6745\n",
      "Batch [150/313], Loss: 1.9920\n",
      "Batch [180/313], Loss: 1.6002\n",
      "Batch [210/313], Loss: 1.8524\n",
      "Batch [240/313], Loss: 1.8532\n",
      "Batch [270/313], Loss: 1.7030\n",
      "Batch [300/313], Loss: 1.9100\n",
      "Epoch [15/20], Loss: 1.3582\n",
      "Validation loss decreased (1.395113 --> 1.358244).  Saving model ...\n",
      "Batch [30/313], Loss: 1.6893\n",
      "Batch [60/313], Loss: 1.5352\n",
      "Batch [90/313], Loss: 1.6453\n",
      "Batch [120/313], Loss: 1.6321\n",
      "Batch [150/313], Loss: 1.6932\n",
      "Batch [180/313], Loss: 1.7537\n",
      "Batch [210/313], Loss: 1.8888\n",
      "Batch [240/313], Loss: 1.6224\n",
      "Batch [270/313], Loss: 1.6913\n",
      "Batch [300/313], Loss: 1.6586\n",
      "Epoch [16/20], Loss: 1.3370\n",
      "Validation loss decreased (1.358244 --> 1.337022).  Saving model ...\n",
      "Batch [30/313], Loss: 1.7460\n",
      "Batch [60/313], Loss: 1.6638\n",
      "Batch [90/313], Loss: 1.5800\n",
      "Batch [120/313], Loss: 1.5598\n",
      "Batch [150/313], Loss: 1.5405\n",
      "Batch [180/313], Loss: 1.5795\n",
      "Batch [210/313], Loss: 1.5118\n",
      "Batch [240/313], Loss: 1.8182\n",
      "Batch [270/313], Loss: 1.6849\n",
      "Batch [300/313], Loss: 1.9307\n",
      "Epoch [17/20], Loss: 1.3057\n",
      "Validation loss decreased (1.337022 --> 1.305705).  Saving model ...\n",
      "Batch [30/313], Loss: 1.5233\n",
      "Batch [60/313], Loss: 1.4927\n",
      "Batch [90/313], Loss: 1.3305\n",
      "Batch [120/313], Loss: 1.5048\n",
      "Batch [150/313], Loss: 1.6275\n",
      "Batch [180/313], Loss: 1.5731\n",
      "Batch [210/313], Loss: 1.6151\n",
      "Batch [240/313], Loss: 1.5027\n",
      "Batch [270/313], Loss: 1.4085\n",
      "Batch [300/313], Loss: 1.6808\n",
      "Epoch [18/20], Loss: 1.2778\n",
      "Validation loss decreased (1.305705 --> 1.277847).  Saving model ...\n",
      "Batch [30/313], Loss: 1.5425\n",
      "Batch [60/313], Loss: 1.6500\n",
      "Batch [90/313], Loss: 1.6069\n",
      "Batch [120/313], Loss: 1.8279\n",
      "Batch [150/313], Loss: 1.6034\n",
      "Batch [180/313], Loss: 1.4720\n",
      "Batch [210/313], Loss: 1.5134\n",
      "Batch [240/313], Loss: 1.2961\n",
      "Batch [270/313], Loss: 1.7919\n",
      "Batch [300/313], Loss: 1.7485\n",
      "Epoch [19/20], Loss: 1.2589\n",
      "Validation loss decreased (1.277847 --> 1.258888).  Saving model ...\n",
      "Batch [30/313], Loss: 1.4989\n",
      "Batch [60/313], Loss: 1.2569\n",
      "Batch [90/313], Loss: 1.3197\n",
      "Batch [120/313], Loss: 1.2444\n",
      "Batch [150/313], Loss: 1.5499\n",
      "Batch [180/313], Loss: 1.5975\n",
      "Batch [210/313], Loss: 1.3636\n",
      "Batch [240/313], Loss: 1.2621\n",
      "Batch [270/313], Loss: 1.3495\n",
      "Batch [300/313], Loss: 1.4075\n",
      "Epoch [20/20], Loss: 1.2283\n",
      "Validation loss decreased (1.258888 --> 1.228343).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████▎                                      | 2/20 [04:25<39:51, 132.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20]   Loss: 2.2295   Accuracy: 44.74%\n",
      "Batch [30/313], Loss: 1.3519\n",
      "Batch [60/313], Loss: 1.2732\n",
      "Batch [90/313], Loss: 1.5231\n",
      "Batch [120/313], Loss: 1.5140\n",
      "Batch [150/313], Loss: 1.6289\n",
      "Batch [180/313], Loss: 1.3187\n",
      "Batch [210/313], Loss: 1.5221\n",
      "Batch [240/313], Loss: 1.6114\n",
      "Batch [270/313], Loss: 1.5023\n",
      "Batch [300/313], Loss: 1.5804\n",
      "Epoch [1/20], Loss: 1.2062\n",
      "Validation loss decreased (1.228343 --> 1.206229).  Saving model ...\n",
      "Batch [30/313], Loss: 1.4971\n",
      "Batch [60/313], Loss: 1.3450\n",
      "Batch [90/313], Loss: 1.3157\n",
      "Batch [120/313], Loss: 1.5741\n",
      "Batch [150/313], Loss: 1.4132\n",
      "Batch [180/313], Loss: 1.5135\n",
      "Batch [210/313], Loss: 1.2469\n",
      "Batch [240/313], Loss: 1.3625\n",
      "Batch [270/313], Loss: 1.5136\n",
      "Batch [300/313], Loss: 1.6751\n",
      "Epoch [2/20], Loss: 1.1685\n",
      "Validation loss decreased (1.206229 --> 1.168496).  Saving model ...\n",
      "Batch [30/313], Loss: 1.4670\n",
      "Batch [60/313], Loss: 1.3587\n",
      "Batch [90/313], Loss: 1.4445\n",
      "Batch [120/313], Loss: 1.5624\n",
      "Batch [150/313], Loss: 1.7404\n",
      "Batch [180/313], Loss: 1.5065\n",
      "Batch [210/313], Loss: 1.4723\n",
      "Batch [240/313], Loss: 1.5321\n",
      "Batch [270/313], Loss: 1.5729\n",
      "Batch [300/313], Loss: 1.4520\n",
      "Epoch [3/20], Loss: 1.1418\n",
      "Validation loss decreased (1.168496 --> 1.141822).  Saving model ...\n",
      "Batch [30/313], Loss: 1.3505\n",
      "Batch [60/313], Loss: 1.4913\n",
      "Batch [90/313], Loss: 1.4588\n",
      "Batch [120/313], Loss: 1.6030\n",
      "Batch [150/313], Loss: 1.6247\n",
      "Batch [180/313], Loss: 1.3915\n",
      "Batch [210/313], Loss: 1.5480\n",
      "Batch [240/313], Loss: 1.4892\n",
      "Batch [270/313], Loss: 1.2555\n",
      "Batch [300/313], Loss: 1.5077\n",
      "Epoch [4/20], Loss: 1.1154\n",
      "Validation loss decreased (1.141822 --> 1.115391).  Saving model ...\n",
      "Batch [30/313], Loss: 1.6011\n",
      "Batch [60/313], Loss: 1.4320\n",
      "Batch [90/313], Loss: 1.5230\n",
      "Batch [120/313], Loss: 1.4935\n",
      "Batch [150/313], Loss: 1.1748\n",
      "Batch [180/313], Loss: 1.3547\n",
      "Batch [210/313], Loss: 1.2891\n",
      "Batch [240/313], Loss: 1.4567\n",
      "Batch [270/313], Loss: 1.3180\n",
      "Batch [300/313], Loss: 1.3046\n",
      "Epoch [5/20], Loss: 1.0980\n",
      "Validation loss decreased (1.115391 --> 1.098037).  Saving model ...\n",
      "Batch [30/313], Loss: 1.2356\n",
      "Batch [60/313], Loss: 1.3029\n",
      "Batch [90/313], Loss: 1.2927\n",
      "Batch [120/313], Loss: 1.5754\n",
      "Batch [150/313], Loss: 1.2547\n",
      "Batch [180/313], Loss: 1.2497\n",
      "Batch [210/313], Loss: 1.5881\n",
      "Batch [240/313], Loss: 1.4767\n",
      "Batch [270/313], Loss: 1.4381\n",
      "Batch [300/313], Loss: 1.6421\n",
      "Epoch [6/20], Loss: 1.0766\n",
      "Validation loss decreased (1.098037 --> 1.076628).  Saving model ...\n",
      "Batch [30/313], Loss: 1.2220\n",
      "Batch [60/313], Loss: 1.1971\n",
      "Batch [90/313], Loss: 1.1190\n",
      "Batch [120/313], Loss: 1.4585\n",
      "Batch [150/313], Loss: 1.3977\n",
      "Batch [180/313], Loss: 1.4571\n",
      "Batch [210/313], Loss: 1.3270\n",
      "Batch [240/313], Loss: 1.4807\n",
      "Batch [270/313], Loss: 1.4214\n",
      "Batch [300/313], Loss: 1.4345\n",
      "Epoch [7/20], Loss: 1.0388\n",
      "Validation loss decreased (1.076628 --> 1.038789).  Saving model ...\n",
      "Batch [30/313], Loss: 1.1360\n",
      "Batch [60/313], Loss: 1.1760\n",
      "Batch [90/313], Loss: 1.0389\n",
      "Batch [120/313], Loss: 1.0862\n",
      "Batch [150/313], Loss: 1.3879\n",
      "Batch [180/313], Loss: 1.3389\n",
      "Batch [210/313], Loss: 1.4288\n",
      "Batch [240/313], Loss: 1.1384\n",
      "Batch [270/313], Loss: 1.3926\n",
      "Batch [300/313], Loss: 1.5101\n",
      "Epoch [8/20], Loss: 1.0245\n",
      "Validation loss decreased (1.038789 --> 1.024498).  Saving model ...\n",
      "Batch [30/313], Loss: 1.2823\n",
      "Batch [60/313], Loss: 1.2513\n",
      "Batch [90/313], Loss: 1.3964\n",
      "Batch [120/313], Loss: 1.2956\n",
      "Batch [150/313], Loss: 1.2092\n",
      "Batch [180/313], Loss: 1.0850\n",
      "Batch [210/313], Loss: 1.2031\n",
      "Batch [240/313], Loss: 1.0857\n",
      "Batch [270/313], Loss: 1.5563\n",
      "Batch [300/313], Loss: 1.2487\n",
      "Epoch [9/20], Loss: 0.9959\n",
      "Validation loss decreased (1.024498 --> 0.995882).  Saving model ...\n",
      "Batch [30/313], Loss: 1.1353\n",
      "Batch [60/313], Loss: 1.0631\n",
      "Batch [90/313], Loss: 1.0873\n",
      "Batch [120/313], Loss: 1.4166\n",
      "Batch [150/313], Loss: 1.2033\n",
      "Batch [180/313], Loss: 1.1759\n",
      "Batch [210/313], Loss: 0.9682\n",
      "Batch [240/313], Loss: 1.3543\n",
      "Batch [270/313], Loss: 0.9749\n",
      "Batch [300/313], Loss: 1.0079\n",
      "Epoch [10/20], Loss: 0.9677\n",
      "Validation loss decreased (0.995882 --> 0.967651).  Saving model ...\n",
      "Batch [30/313], Loss: 1.2199\n",
      "Batch [60/313], Loss: 0.9313\n",
      "Batch [90/313], Loss: 0.9925\n",
      "Batch [120/313], Loss: 1.0932\n",
      "Batch [150/313], Loss: 1.0861\n",
      "Batch [180/313], Loss: 1.2379\n",
      "Batch [210/313], Loss: 1.0546\n",
      "Batch [240/313], Loss: 1.5519\n",
      "Batch [270/313], Loss: 1.0597\n",
      "Batch [300/313], Loss: 1.2847\n",
      "Epoch [11/20], Loss: 0.9535\n",
      "Validation loss decreased (0.967651 --> 0.953503).  Saving model ...\n",
      "Batch [30/313], Loss: 1.2409\n",
      "Batch [60/313], Loss: 1.1370\n",
      "Batch [90/313], Loss: 1.2879\n",
      "Batch [120/313], Loss: 1.2185\n",
      "Batch [150/313], Loss: 1.1706\n",
      "Batch [180/313], Loss: 1.2070\n",
      "Batch [210/313], Loss: 1.1650\n",
      "Batch [240/313], Loss: 1.1885\n",
      "Batch [270/313], Loss: 1.0854\n",
      "Batch [300/313], Loss: 1.4110\n",
      "Epoch [12/20], Loss: 0.9195\n",
      "Validation loss decreased (0.953503 --> 0.919536).  Saving model ...\n",
      "Batch [30/313], Loss: 1.2501\n",
      "Batch [60/313], Loss: 1.0920\n",
      "Batch [90/313], Loss: 0.8875\n",
      "Batch [120/313], Loss: 1.1305\n",
      "Batch [150/313], Loss: 1.1473\n",
      "Batch [180/313], Loss: 1.0630\n",
      "Batch [210/313], Loss: 1.1724\n",
      "Batch [240/313], Loss: 0.9878\n",
      "Batch [270/313], Loss: 1.0841\n",
      "Batch [300/313], Loss: 1.1070\n",
      "Epoch [13/20], Loss: 0.8940\n",
      "Validation loss decreased (0.919536 --> 0.893960).  Saving model ...\n",
      "Batch [30/313], Loss: 1.0853\n",
      "Batch [60/313], Loss: 0.9648\n",
      "Batch [90/313], Loss: 1.0988\n",
      "Batch [120/313], Loss: 1.0197\n",
      "Batch [150/313], Loss: 1.2813\n",
      "Batch [180/313], Loss: 1.1062\n",
      "Batch [210/313], Loss: 1.0738\n",
      "Batch [240/313], Loss: 0.8893\n",
      "Batch [270/313], Loss: 1.1653\n",
      "Batch [300/313], Loss: 1.3967\n",
      "Epoch [14/20], Loss: 0.8851\n",
      "Validation loss decreased (0.893960 --> 0.885130).  Saving model ...\n",
      "Batch [30/313], Loss: 1.2630\n",
      "Batch [60/313], Loss: 1.2719\n",
      "Batch [90/313], Loss: 0.9620\n",
      "Batch [120/313], Loss: 0.8133\n",
      "Batch [150/313], Loss: 1.0434\n",
      "Batch [180/313], Loss: 0.9743\n",
      "Batch [210/313], Loss: 1.3378\n",
      "Batch [240/313], Loss: 1.0005\n",
      "Batch [270/313], Loss: 1.3169\n",
      "Batch [300/313], Loss: 1.1241\n",
      "Epoch [15/20], Loss: 0.8624\n",
      "Validation loss decreased (0.885130 --> 0.862422).  Saving model ...\n",
      "Batch [30/313], Loss: 1.1004\n",
      "Batch [60/313], Loss: 1.0881\n",
      "Batch [90/313], Loss: 0.9795\n",
      "Batch [120/313], Loss: 1.0723\n",
      "Batch [150/313], Loss: 1.1145\n",
      "Batch [180/313], Loss: 1.1910\n",
      "Batch [210/313], Loss: 1.1374\n",
      "Batch [240/313], Loss: 1.0249\n",
      "Batch [270/313], Loss: 1.0336\n",
      "Batch [300/313], Loss: 1.0071\n",
      "Epoch [16/20], Loss: 0.8332\n",
      "Validation loss decreased (0.862422 --> 0.833192).  Saving model ...\n",
      "Batch [30/313], Loss: 1.0419\n",
      "Batch [60/313], Loss: 1.1584\n",
      "Batch [90/313], Loss: 0.9454\n",
      "Batch [120/313], Loss: 0.8568\n",
      "Batch [150/313], Loss: 1.1308\n",
      "Batch [180/313], Loss: 1.0821\n",
      "Batch [210/313], Loss: 0.9598\n",
      "Batch [240/313], Loss: 1.2163\n",
      "Batch [270/313], Loss: 0.9268\n",
      "Batch [300/313], Loss: 1.2435\n",
      "Epoch [17/20], Loss: 0.8188\n",
      "Validation loss decreased (0.833192 --> 0.818816).  Saving model ...\n",
      "Batch [30/313], Loss: 1.0274\n",
      "Batch [60/313], Loss: 0.8656\n",
      "Batch [90/313], Loss: 0.8909\n",
      "Batch [120/313], Loss: 1.2672\n",
      "Batch [150/313], Loss: 0.6912\n",
      "Batch [180/313], Loss: 0.9181\n",
      "Batch [210/313], Loss: 1.0284\n",
      "Batch [240/313], Loss: 0.9902\n",
      "Batch [270/313], Loss: 1.0773\n",
      "Batch [300/313], Loss: 0.9855\n",
      "Epoch [18/20], Loss: 0.7977\n",
      "Validation loss decreased (0.818816 --> 0.797714).  Saving model ...\n",
      "Batch [30/313], Loss: 0.9127\n",
      "Batch [60/313], Loss: 0.8573\n",
      "Batch [90/313], Loss: 1.0432\n",
      "Batch [120/313], Loss: 1.2232\n",
      "Batch [150/313], Loss: 1.1134\n",
      "Batch [180/313], Loss: 0.9162\n",
      "Batch [210/313], Loss: 0.9442\n",
      "Batch [240/313], Loss: 1.1940\n",
      "Batch [270/313], Loss: 1.1769\n",
      "Batch [300/313], Loss: 1.1711\n",
      "Epoch [19/20], Loss: 0.7768\n",
      "Validation loss decreased (0.797714 --> 0.776756).  Saving model ...\n",
      "Batch [30/313], Loss: 0.8878\n",
      "Batch [60/313], Loss: 0.8811\n",
      "Batch [90/313], Loss: 0.7579\n",
      "Batch [120/313], Loss: 0.9395\n",
      "Batch [150/313], Loss: 0.9654\n",
      "Batch [180/313], Loss: 0.8759\n",
      "Batch [210/313], Loss: 0.8396\n",
      "Batch [240/313], Loss: 0.9524\n",
      "Batch [270/313], Loss: 1.1609\n",
      "Batch [300/313], Loss: 0.8624\n",
      "Epoch [20/20], Loss: 0.7487\n",
      "Validation loss decreased (0.776756 --> 0.748711).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|██████▍                                    | 3/20 [06:38<37:36, 132.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20]   Loss: 2.4292   Accuracy: 45.09%\n",
      "Batch [30/313], Loss: 0.8907\n",
      "Batch [60/313], Loss: 0.8085\n",
      "Batch [90/313], Loss: 1.0777\n",
      "Batch [120/313], Loss: 1.0519\n",
      "Batch [150/313], Loss: 0.9564\n",
      "Batch [180/313], Loss: 1.0441\n",
      "Batch [210/313], Loss: 0.9786\n",
      "Batch [240/313], Loss: 0.8696\n",
      "Batch [270/313], Loss: 1.0113\n",
      "Batch [300/313], Loss: 0.9897\n",
      "Epoch [1/20], Loss: 0.7339\n",
      "Validation loss decreased (0.748711 --> 0.733936).  Saving model ...\n",
      "Batch [30/313], Loss: 0.9393\n",
      "Batch [60/313], Loss: 0.8441\n",
      "Batch [90/313], Loss: 0.9259\n",
      "Batch [120/313], Loss: 0.6647\n",
      "Batch [150/313], Loss: 1.0327\n",
      "Batch [180/313], Loss: 1.0460\n",
      "Batch [210/313], Loss: 1.1740\n",
      "Batch [240/313], Loss: 0.7867\n",
      "Batch [270/313], Loss: 0.8857\n",
      "Batch [300/313], Loss: 0.8685\n",
      "Epoch [2/20], Loss: 0.7084\n",
      "Validation loss decreased (0.733936 --> 0.708357).  Saving model ...\n",
      "Batch [30/313], Loss: 0.8308\n",
      "Batch [60/313], Loss: 0.8094\n",
      "Batch [90/313], Loss: 0.9613\n",
      "Batch [120/313], Loss: 0.7402\n",
      "Batch [150/313], Loss: 1.1394\n",
      "Batch [180/313], Loss: 0.8758\n",
      "Batch [210/313], Loss: 1.0069\n",
      "Batch [240/313], Loss: 0.9878\n",
      "Batch [270/313], Loss: 0.9240\n",
      "Batch [300/313], Loss: 0.8021\n",
      "Epoch [3/20], Loss: 0.6915\n",
      "Validation loss decreased (0.708357 --> 0.691517).  Saving model ...\n",
      "Batch [30/313], Loss: 0.7988\n",
      "Batch [60/313], Loss: 0.8106\n",
      "Batch [90/313], Loss: 0.8133\n",
      "Batch [120/313], Loss: 0.8603\n",
      "Batch [150/313], Loss: 0.7331\n",
      "Batch [180/313], Loss: 0.8039\n",
      "Batch [210/313], Loss: 0.7942\n",
      "Batch [240/313], Loss: 0.7940\n",
      "Batch [270/313], Loss: 0.8657\n",
      "Batch [300/313], Loss: 0.8640\n",
      "Epoch [4/20], Loss: 0.6766\n",
      "Validation loss decreased (0.691517 --> 0.676554).  Saving model ...\n",
      "Batch [30/313], Loss: 0.6230\n",
      "Batch [60/313], Loss: 0.8052\n",
      "Batch [90/313], Loss: 0.7534\n",
      "Batch [120/313], Loss: 0.6549\n",
      "Batch [150/313], Loss: 0.7539\n",
      "Batch [180/313], Loss: 0.8687\n",
      "Batch [210/313], Loss: 0.8706\n",
      "Batch [240/313], Loss: 0.8528\n",
      "Batch [270/313], Loss: 0.9686\n",
      "Batch [300/313], Loss: 0.8645\n",
      "Epoch [5/20], Loss: 0.6493\n",
      "Validation loss decreased (0.676554 --> 0.649318).  Saving model ...\n",
      "Batch [30/313], Loss: 0.8495\n",
      "Batch [60/313], Loss: 0.7741\n",
      "Batch [90/313], Loss: 0.7654\n",
      "Batch [120/313], Loss: 0.7630\n",
      "Batch [150/313], Loss: 0.6646\n",
      "Batch [180/313], Loss: 0.8047\n",
      "Batch [210/313], Loss: 0.8722\n",
      "Batch [240/313], Loss: 0.8325\n",
      "Batch [270/313], Loss: 0.6773\n",
      "Batch [300/313], Loss: 0.8448\n",
      "Epoch [6/20], Loss: 0.6279\n",
      "Validation loss decreased (0.649318 --> 0.627876).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5888\n",
      "Batch [60/313], Loss: 0.7518\n",
      "Batch [90/313], Loss: 0.7053\n",
      "Batch [120/313], Loss: 0.8721\n",
      "Batch [150/313], Loss: 0.7078\n",
      "Batch [180/313], Loss: 0.8168\n",
      "Batch [210/313], Loss: 0.6024\n",
      "Batch [240/313], Loss: 0.6822\n",
      "Batch [270/313], Loss: 0.7352\n",
      "Batch [300/313], Loss: 0.7119\n",
      "Epoch [7/20], Loss: 0.6207\n",
      "Validation loss decreased (0.627876 --> 0.620660).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5568\n",
      "Batch [60/313], Loss: 0.5029\n",
      "Batch [90/313], Loss: 0.8655\n",
      "Batch [120/313], Loss: 0.7742\n",
      "Batch [150/313], Loss: 0.6707\n",
      "Batch [180/313], Loss: 0.8672\n",
      "Batch [210/313], Loss: 0.7501\n",
      "Batch [240/313], Loss: 0.7365\n",
      "Batch [270/313], Loss: 0.8493\n",
      "Batch [300/313], Loss: 0.7770\n",
      "Epoch [8/20], Loss: 0.5937\n",
      "Validation loss decreased (0.620660 --> 0.593679).  Saving model ...\n",
      "Batch [30/313], Loss: 0.6290\n",
      "Batch [60/313], Loss: 0.6282\n",
      "Batch [90/313], Loss: 0.6991\n",
      "Batch [120/313], Loss: 0.6288\n",
      "Batch [150/313], Loss: 0.9430\n",
      "Batch [180/313], Loss: 0.9110\n",
      "Batch [210/313], Loss: 0.5997\n",
      "Batch [240/313], Loss: 0.9161\n",
      "Batch [270/313], Loss: 0.8089\n",
      "Batch [300/313], Loss: 0.9903\n",
      "Epoch [9/20], Loss: 0.5724\n",
      "Validation loss decreased (0.593679 --> 0.572380).  Saving model ...\n",
      "Batch [30/313], Loss: 0.7072\n",
      "Batch [60/313], Loss: 0.5864\n",
      "Batch [90/313], Loss: 0.6612\n",
      "Batch [120/313], Loss: 0.6919\n",
      "Batch [150/313], Loss: 0.7807\n",
      "Batch [180/313], Loss: 0.6496\n",
      "Batch [210/313], Loss: 0.6743\n",
      "Batch [240/313], Loss: 1.0527\n",
      "Batch [270/313], Loss: 0.6764\n",
      "Batch [300/313], Loss: 0.7952\n",
      "Epoch [10/20], Loss: 0.5699\n",
      "Validation loss decreased (0.572380 --> 0.569870).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5617\n",
      "Batch [60/313], Loss: 0.7671\n",
      "Batch [90/313], Loss: 0.5612\n",
      "Batch [120/313], Loss: 0.7657\n",
      "Batch [150/313], Loss: 0.7432\n",
      "Batch [180/313], Loss: 0.7170\n",
      "Batch [210/313], Loss: 0.7279\n",
      "Batch [240/313], Loss: 0.5834\n",
      "Batch [270/313], Loss: 0.5631\n",
      "Batch [300/313], Loss: 0.6486\n",
      "Epoch [11/20], Loss: 0.5399\n",
      "Validation loss decreased (0.569870 --> 0.539862).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5601\n",
      "Batch [60/313], Loss: 0.6820\n",
      "Batch [90/313], Loss: 0.5661\n",
      "Batch [120/313], Loss: 0.8119\n",
      "Batch [150/313], Loss: 0.7266\n",
      "Batch [180/313], Loss: 0.7722\n",
      "Batch [210/313], Loss: 0.5209\n",
      "Batch [240/313], Loss: 0.7395\n",
      "Batch [270/313], Loss: 0.6454\n",
      "Batch [300/313], Loss: 0.5303\n",
      "Epoch [12/20], Loss: 0.5307\n",
      "Validation loss decreased (0.539862 --> 0.530743).  Saving model ...\n",
      "Batch [30/313], Loss: 0.4474\n",
      "Batch [60/313], Loss: 0.6296\n",
      "Batch [90/313], Loss: 0.6528\n",
      "Batch [120/313], Loss: 0.7783\n",
      "Batch [150/313], Loss: 0.5486\n",
      "Batch [180/313], Loss: 0.5340\n",
      "Batch [210/313], Loss: 0.7074\n",
      "Batch [240/313], Loss: 0.7874\n",
      "Batch [270/313], Loss: 0.8017\n",
      "Batch [300/313], Loss: 0.5636\n",
      "Epoch [13/20], Loss: 0.5144\n",
      "Validation loss decreased (0.530743 --> 0.514376).  Saving model ...\n",
      "Batch [30/313], Loss: 0.4996\n",
      "Batch [60/313], Loss: 0.6545\n",
      "Batch [90/313], Loss: 0.6909\n",
      "Batch [120/313], Loss: 0.6514\n",
      "Batch [150/313], Loss: 0.6446\n",
      "Batch [180/313], Loss: 0.5866\n",
      "Batch [210/313], Loss: 0.6478\n",
      "Batch [240/313], Loss: 0.5997\n",
      "Batch [270/313], Loss: 0.6053\n",
      "Batch [300/313], Loss: 0.7236\n",
      "Epoch [14/20], Loss: 0.5017\n",
      "Validation loss decreased (0.514376 --> 0.501674).  Saving model ...\n",
      "Batch [30/313], Loss: 0.4333\n",
      "Batch [60/313], Loss: 0.5350\n",
      "Batch [90/313], Loss: 0.6731\n",
      "Batch [120/313], Loss: 0.6561\n",
      "Batch [150/313], Loss: 0.6450\n",
      "Batch [180/313], Loss: 0.4816\n",
      "Batch [210/313], Loss: 0.5799\n",
      "Batch [240/313], Loss: 0.7453\n",
      "Batch [270/313], Loss: 0.5470\n",
      "Batch [300/313], Loss: 0.6011\n",
      "Epoch [15/20], Loss: 0.4762\n",
      "Validation loss decreased (0.501674 --> 0.476180).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5157\n",
      "Batch [60/313], Loss: 0.5359\n",
      "Batch [90/313], Loss: 0.4324\n",
      "Batch [120/313], Loss: 0.6138\n",
      "Batch [150/313], Loss: 0.6329\n",
      "Batch [180/313], Loss: 0.6998\n",
      "Batch [210/313], Loss: 0.6191\n",
      "Batch [240/313], Loss: 0.5338\n",
      "Batch [270/313], Loss: 0.6383\n",
      "Batch [300/313], Loss: 0.5901\n",
      "Epoch [16/20], Loss: 0.4737\n",
      "Validation loss decreased (0.476180 --> 0.473735).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5409\n",
      "Batch [60/313], Loss: 0.5576\n",
      "Batch [90/313], Loss: 0.4032\n",
      "Batch [120/313], Loss: 0.4477\n",
      "Batch [150/313], Loss: 0.5375\n",
      "Batch [180/313], Loss: 0.5719\n",
      "Batch [210/313], Loss: 0.6950\n",
      "Batch [240/313], Loss: 0.6748\n",
      "Batch [270/313], Loss: 0.7405\n",
      "Batch [300/313], Loss: 0.6160\n",
      "Epoch [17/20], Loss: 0.4566\n",
      "Validation loss decreased (0.473735 --> 0.456596).  Saving model ...\n",
      "Batch [30/313], Loss: 0.4596\n",
      "Batch [60/313], Loss: 0.4288\n",
      "Batch [90/313], Loss: 0.4482\n",
      "Batch [120/313], Loss: 0.5226\n",
      "Batch [150/313], Loss: 0.5721\n",
      "Batch [180/313], Loss: 0.8021\n",
      "Batch [210/313], Loss: 0.6902\n",
      "Batch [240/313], Loss: 0.7067\n",
      "Batch [270/313], Loss: 0.6708\n",
      "Batch [300/313], Loss: 0.6248\n",
      "Epoch [18/20], Loss: 0.4462\n",
      "Validation loss decreased (0.456596 --> 0.446216).  Saving model ...\n",
      "Batch [30/313], Loss: 0.4432\n",
      "Batch [60/313], Loss: 0.5038\n",
      "Batch [90/313], Loss: 0.6071\n",
      "Batch [120/313], Loss: 0.5288\n",
      "Batch [150/313], Loss: 0.4788\n",
      "Batch [180/313], Loss: 0.4989\n",
      "Batch [210/313], Loss: 0.6300\n",
      "Batch [240/313], Loss: 0.4346\n",
      "Batch [270/313], Loss: 0.4563\n",
      "Batch [300/313], Loss: 0.5733\n",
      "Epoch [19/20], Loss: 0.4309\n",
      "Validation loss decreased (0.446216 --> 0.430899).  Saving model ...\n",
      "Batch [30/313], Loss: 0.6449\n",
      "Batch [60/313], Loss: 0.2902\n",
      "Batch [90/313], Loss: 0.4113\n",
      "Batch [120/313], Loss: 0.5526\n",
      "Batch [150/313], Loss: 0.5270\n",
      "Batch [180/313], Loss: 0.4977\n",
      "Batch [210/313], Loss: 0.4966\n",
      "Batch [240/313], Loss: 0.6565\n",
      "Batch [270/313], Loss: 0.4622\n",
      "Batch [300/313], Loss: 0.5616\n",
      "Epoch [20/20], Loss: 0.4164\n",
      "Validation loss decreased (0.430899 --> 0.416428).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▌                                  | 4/20 [08:52<35:30, 133.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20]   Loss: 2.7058   Accuracy: 45.19%\n",
      "Batch [30/313], Loss: 0.3761\n",
      "Batch [60/313], Loss: 0.4670\n",
      "Batch [90/313], Loss: 0.4061\n",
      "Batch [120/313], Loss: 0.6016\n",
      "Batch [150/313], Loss: 0.6680\n",
      "Batch [180/313], Loss: 0.6241\n",
      "Batch [210/313], Loss: 0.5216\n",
      "Batch [240/313], Loss: 0.6144\n",
      "Batch [270/313], Loss: 0.4618\n",
      "Batch [300/313], Loss: 0.4761\n",
      "Epoch [1/20], Loss: 0.4070\n",
      "Validation loss decreased (0.416428 --> 0.407026).  Saving model ...\n",
      "Batch [30/313], Loss: 0.4127\n",
      "Batch [60/313], Loss: 0.4773\n",
      "Batch [90/313], Loss: 0.4568\n",
      "Batch [120/313], Loss: 0.7312\n",
      "Batch [150/313], Loss: 0.5654\n",
      "Batch [180/313], Loss: 0.4435\n",
      "Batch [210/313], Loss: 0.4968\n",
      "Batch [240/313], Loss: 0.4967\n",
      "Batch [270/313], Loss: 0.5701\n",
      "Batch [300/313], Loss: 0.4787\n",
      "Epoch [2/20], Loss: 0.3968\n",
      "Validation loss decreased (0.407026 --> 0.396781).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5334\n",
      "Batch [60/313], Loss: 0.5181\n",
      "Batch [90/313], Loss: 0.5376\n",
      "Batch [120/313], Loss: 0.4445\n",
      "Batch [150/313], Loss: 0.4975\n",
      "Batch [180/313], Loss: 0.4537\n",
      "Batch [210/313], Loss: 0.5593\n",
      "Batch [240/313], Loss: 0.4656\n",
      "Batch [270/313], Loss: 0.4163\n",
      "Batch [300/313], Loss: 0.6151\n",
      "Epoch [3/20], Loss: 0.3785\n",
      "Validation loss decreased (0.396781 --> 0.378518).  Saving model ...\n",
      "Batch [30/313], Loss: 0.3571\n",
      "Batch [60/313], Loss: 0.3160\n",
      "Batch [90/313], Loss: 0.4876\n",
      "Batch [120/313], Loss: 0.4606\n",
      "Batch [150/313], Loss: 0.5610\n",
      "Batch [180/313], Loss: 0.3881\n",
      "Batch [210/313], Loss: 0.4864\n",
      "Batch [240/313], Loss: 0.3426\n",
      "Batch [270/313], Loss: 0.4387\n",
      "Batch [300/313], Loss: 0.4737\n",
      "Epoch [4/20], Loss: 0.3689\n",
      "Validation loss decreased (0.378518 --> 0.368870).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# Per-Epoch Activity 코드\n",
    "for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "    # 모델 학습\n",
    "    train_model(net, train_loader, criterion, optimizer, num_epochs=num_epochs)\n",
    "    \n",
    "    # 테스트 평가\n",
    "    test_loss, test_accuracy = test_model(net, test_loader, criterion, epoch)\n",
    "    \n",
    "    # TensorBoard에 테스트 결과 기록\n",
    "    writer.add_scalar(\"Test Loss\", test_loss, epoch)\n",
    "    writer.add_scalar(\"Test Accuracy\", test_accuracy, epoch)\n",
    "\n",
    "    # 현재 epoch 결과 출력\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}]   Loss: {test_loss:.4f}   Accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "# TensorBoard writer 닫기\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6271219f-96f2-4967-939d-a24d6def8eb0",
   "metadata": {},
   "source": [
    "### **Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f94ca2-f1e7-4452-b501-9b36e7025a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Result of ResNet = Epoch : {epoch}   Loss : {test_loss}   Accuracy : {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb5d42f-9728-4e40-94eb-997ad195b5b2",
   "metadata": {},
   "source": [
    "# Test - 나영(Accuracy) 현욱(Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c59d485-f2bb-4dc0-85ac-55a5f7ef1289",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('10000개 테스트 이미지에서 모델 정확도: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b41eb1-d372-4e64-b1d9-d2552ed44d99",
   "metadata": {},
   "source": [
    "**Visualization of average loss(수정 필요)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45250e0a-4741-4228-a62a-28b9733fd1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # 훈련과정에서 손실을 기록\n",
    "    train_loss = 0\n",
    "    total_samples = 0\n",
    "    net.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        total_samples += inputs.size(0)\n",
    "    \n",
    "    train_losses.append(train_loss / total_samples)\n",
    "\n",
    "    # 평가 과정에서 손실과 정확도를 기록\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    test_losses.append(test_loss / total)\n",
    "    test_accuracies.append(correct / total)\n",
    "\n",
    "# 손실과 정확도 그래프 그리기\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(test_accuracies, label='Test Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea3df5-03bd-4f43-8b2b-d251da3ea733",
   "metadata": {},
   "source": [
    "**Top-1 Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32fbe81-49f4-431b-b1fd-e8bcb50d0c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_top1_accuracy(model, device, data_loader, criterion):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "            # # 각 샘플에 대한 예측 결과와 실제 레이블 출력\n",
    "            # for i in range(data.size(0)):\n",
    "            #     print(f\"Sample {i + 1}: Predicted = {predicted[i].item()}, Actual = {target[i].item()}\")\n",
    "\n",
    "    top1_accuracy = 100 * correct / total\n",
    "    print(f\"Top-1 Accuracy: {top1_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb41004-213b-4b75-8c10-2741a33871d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련 후 검증 데이터셋에 대한 Top-1 정확도 계산 및 출력\n",
    "calculate_top1_accuracy(net, device, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0993a9-bc4f-40e1-909c-c63d6c7dd36e",
   "metadata": {},
   "source": [
    "**Top-5 Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ec995a-2515-488b-b266-3d566d9490b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_and_print_top5_accuracy(model, device, data_loader, criterion):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():  # 그래디언트 계산 비활성화\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            outputs = model(data)\n",
    "            # Top-5 예측 결과 가져오기\n",
    "            _, predicted_top5 = torch.topk(outputs, 5, dim=1)\n",
    "            total += target.size(0)\n",
    "            \n",
    "            # 예측된 Top-5 내에 실제 레이블이 있는지 확인\n",
    "            correct += (predicted_top5 == target.view(-1, 1)).sum().item()\n",
    "\n",
    "    top5_accuracy = 100 * correct / total\n",
    "    print(f\"Top-5 Accuracy: {top5_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2761aaae-f57c-44e7-9b55-fc9959abdb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_and_print_top5_accuracy(net, device, val_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6503fa03-bac6-4758-a623-8cec95a40985",
   "metadata": {},
   "source": [
    "# Data Analysis - 현욱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1321873-e871-4ee1-883e-637fe9aa843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=./runs/resnet_18/tensorboard --port=8202 --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cce3873-fb7d-447b-9e9e-2c2c68a8320f",
   "metadata": {},
   "source": [
    "### **Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459fcbfd-53ee-4489-a204-c366745ff4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_data.classes\n",
    "coarse_classes = [\n",
    "    'aquatic mammals', 'fish', 'flowers', 'food containers', 'fruit and vegetables', 'household electrical devices', \n",
    "    'household furniture', 'insects', 'large carnivores', 'large man-made outdoor things', \n",
    "    'large natural outdoor scenes', 'large omnivores and herbivores', 'medium-sized mammals', \n",
    "    'non-insect invertebrates', 'people', 'reptiles', 'small mammals', 'trees', 'vehicles 1', 'vehicles 2'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12998ea-5e92-47e9-8f07-5506b467b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(classes), len(coarse_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65824d7-6382-46c5-a654-ce9c579aaee2",
   "metadata": {},
   "source": [
    "##### **Fine_to_coarse_mapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22925779-77ff-457d-bac1-1daa7f1746ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-100 세부 클래스(fine classes)와 상위 클래스(coarse classes) 매핑\n",
    "fine_to_coarse_mapping = {\n",
    "    # aquatic mammals\n",
    "    'beaver': 'aquatic mammals',\n",
    "    'dolphin': 'aquatic mammals',\n",
    "    'otter': 'aquatic mammals',\n",
    "    'seal': 'aquatic mammals',\n",
    "    'whale': 'aquatic mammals',\n",
    "    \n",
    "    # fish\n",
    "    'aquarium fish': 'fish',\n",
    "    'flatfish': 'fish',\n",
    "    'ray': 'fish',\n",
    "    'shark': 'fish',\n",
    "    'trout': 'fish',\n",
    "    \n",
    "    # flowers\n",
    "    'orchids': 'flowers',\n",
    "    'poppies': 'flowers',\n",
    "    'roses': 'flowers',\n",
    "    'sunflowers': 'flowers',\n",
    "    'tulips': 'flowers',\n",
    "    \n",
    "    # food containers\n",
    "    'bottles': 'food containers',\n",
    "    'bowls': 'food containers',\n",
    "    'cans': 'food containers',\n",
    "    'cups': 'food containers',\n",
    "    'plates': 'food containers',\n",
    "    \n",
    "    # fruit and vegetables\n",
    "    'apples': 'fruit and vegetables',\n",
    "    'mushrooms': 'fruit and vegetables',\n",
    "    'oranges': 'fruit and vegetables',\n",
    "    'pears': 'fruit and vegetables',\n",
    "    'sweet peppers': 'fruit and vegetables',\n",
    "    \n",
    "    # household electrical devices\n",
    "    'clock': 'household electrical devices',\n",
    "    'computer keyboard': 'household electrical devices',\n",
    "    'lamp': 'household electrical devices',\n",
    "    'telephone': 'household electrical devices',\n",
    "    'television': 'household electrical devices',\n",
    "    \n",
    "    # household furniture\n",
    "    'bed': 'household furniture',\n",
    "    'chair': 'household furniture',\n",
    "    'couch': 'household furniture',\n",
    "    'table': 'household furniture',\n",
    "    'wardrobe': 'household furniture',\n",
    "    \n",
    "    # insects\n",
    "    'bee': 'insects',\n",
    "    'beetle': 'insects',\n",
    "    'butterfly': 'insects',\n",
    "    'caterpillar': 'insects',\n",
    "    'cockroach': 'insects',\n",
    "    \n",
    "    # large carnivores\n",
    "    'bear': 'large carnivores',\n",
    "    'leopard': 'large carnivores',\n",
    "    'lion': 'large carnivores',\n",
    "    'tiger': 'large carnivores',\n",
    "    'wolf': 'large carnivores',\n",
    "    \n",
    "    # large man-made outdoor things\n",
    "    'bridge': 'large man-made outdoor things',\n",
    "    'castle': 'large man-made outdoor things',\n",
    "    'house': 'large man-made outdoor things',\n",
    "    'road': 'large man-made outdoor things',\n",
    "    'skyscraper': 'large man-made outdoor things',\n",
    "    \n",
    "    # large natural outdoor scenes\n",
    "    'cloud': 'large natural outdoor scenes',\n",
    "    'forest': 'large natural outdoor scenes',\n",
    "    'mountain': 'large natural outdoor scenes',\n",
    "    'plain': 'large natural outdoor scenes',\n",
    "    'sea': 'large natural outdoor scenes',\n",
    "    \n",
    "    # large omnivores and herbivores\n",
    "    'camel': 'large omnivores and herbivores',\n",
    "    'cattle': 'large omnivores and herbivores',\n",
    "    'chimpanzee': 'large omnivores and herbivores',\n",
    "    'elephant': 'large omnivores and herbivores',\n",
    "    'kangaroo': 'large omnivores and herbivores',\n",
    "    \n",
    "    # medium-sized mammals\n",
    "    'fox': 'medium-sized mammals',\n",
    "    'porcupine': 'medium-sized mammals',\n",
    "    'possum': 'medium-sized mammals',\n",
    "    'raccoon': 'medium-sized mammals',\n",
    "    'skunk': 'medium-sized mammals',\n",
    "    \n",
    "    # non-insect invertebrates\n",
    "    'crab': 'non-insect invertebrates',\n",
    "    'lobster': 'non-insect invertebrates',\n",
    "    'snail': 'non-insect invertebrates',\n",
    "    'spider': 'non-insect invertebrates',\n",
    "    'worm': 'non-insect invertebrates',\n",
    "    \n",
    "    # people\n",
    "    'baby': 'people',\n",
    "    'boy': 'people',\n",
    "    'girl': 'people',\n",
    "    'man': 'people',\n",
    "    'woman': 'people',\n",
    "    \n",
    "    # reptiles\n",
    "    'crocodile': 'reptiles',\n",
    "    'dinosaur': 'reptiles',\n",
    "    'lizard': 'reptiles',\n",
    "    'snake': 'reptiles',\n",
    "    'turtle': 'reptiles',\n",
    "    \n",
    "    # small mammals\n",
    "    'hamster': 'small mammals',\n",
    "    'mouse': 'small mammals',\n",
    "    'rabbit': 'small mammals',\n",
    "    'shrew': 'small mammals',\n",
    "    'squirrel': 'small mammals',\n",
    "    \n",
    "    # trees\n",
    "    'maple': 'trees',\n",
    "    'oak': 'trees',\n",
    "    'palm': 'trees',\n",
    "    'pine': 'trees',\n",
    "    'willow': 'trees',\n",
    "    \n",
    "    # vehicles 1\n",
    "    'bicycle': 'vehicles 1',\n",
    "    'bus': 'vehicles 1',\n",
    "    'motorcycle': 'vehicles 1',\n",
    "    'pickup truck': 'vehicles 1',\n",
    "    'train': 'vehicles 1',\n",
    "    \n",
    "    # vehicles 2\n",
    "    'lawn-mower': 'vehicles 2',\n",
    "    'rocket': 'vehicles 2',\n",
    "    'streetcar': 'vehicles 2',\n",
    "    'tank': 'vehicles 2',\n",
    "    'tractor': 'vehicles 2'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defd13fc-9e00-4f14-8ce4-c568bf583a0b",
   "metadata": {},
   "source": [
    "### **Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08932ed-cd47-4196-ae9c-187bd20f02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over test data\n",
    "for x, y in torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size):\n",
    "    \n",
    "    #print('iter val', i)\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    z = net(x)\n",
    "    _, yhat = torch.max(z, 1)\n",
    "    pred = yhat.data.cpu().numpy()\n",
    "    y_pred.extend(pred) # Save Prediction\n",
    "\n",
    "    labels = y.data.cpu().numpy()\n",
    "    y_true.extend(labels) # Save Truth\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "plt.figure(figsize = (128,70))\n",
    "sns.heatmap(df_cm, annot=True)\n",
    "plt.title('Confusion Matrix of ResNet (CIFAR100)')\n",
    "plt.savefig('./runs/resnet_18/Confusion_matrix_ResNet_Cifar100.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bdc8f9-0c1e-4787-aaea-0f00e2fa18b3",
   "metadata": {},
   "source": [
    "### **Confusion Matrix - Coarse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773340a7-fea7-41e1-87cf-b57e6bb8a934",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5be2bacf-9f08-4158-a1f5-388c2b93bd1b",
   "metadata": {},
   "source": [
    "### **Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32cb235-7bd0-429f-9a25-5bcf162d3dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\" Classification Report of ResNet(CIFAR100)  \\n { classification_report(y_true, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ff632-9bd4-4ce6-9a3e-f4908b6c02db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Classification Report as txt file \n",
    "with open(\"./runs/resnet_18/cr_ResNet18.txt\", \"w\") as text_file:\n",
    "    print(classification_report(y_true, y_pred, digits=4), file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06191708-b289-4ada-97c9-5d4d42eaf1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Classification Report txt file \n",
    "with open(\"./runs/resnet_18/cr_ResNet18.txt\", \"r\") as f:\n",
    "  cr = f.read()\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdae401-61f1-47fc-a281-fcc45a3be062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
