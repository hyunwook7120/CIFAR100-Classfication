{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab70dee5-0563-44bf-aac5-fd2065985481",
   "metadata": {},
   "source": [
    "# **Data - 나영**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e457d367-10af-4473-8f3d-77198447a7bb",
   "metadata": {},
   "source": [
    "### **Load Modules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "562d9cb0-9691-4f07-a788-38b1f17bb4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import torchvision.models as models\n",
    "from utility.early_stopping import EarlyStopping\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# sklearn\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f8e0bb-e8d3-45cf-a26f-2405ff3fce13",
   "metadata": {},
   "source": [
    "**Seed Setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d84c3f64-0389-4b7c-83c8-a32ee52e3738",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d348e605-f472-4e8b-9e59-d7f9971c7c55",
   "metadata": {},
   "source": [
    "**Device Setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "035d096c-7962-4214-a220-c5b27b3d04d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2a04d-7b38-4826-b55c-f31ad07811d9",
   "metadata": {},
   "source": [
    "**Set Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4225472f-8390-4f0e-b3ea-85598022cfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 20\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a936f9c8-6a53-4d17-8b05-2f7bbde99746",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eb77a99-0ecc-4dee-8373-8f73d0814215",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(), \n",
    "    transforms.ToTensor(),\n",
    "    # 무작위로 -15도에서 +15도 사이 회전\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    # 밝기, 대비, 채도, 색조 변형 (임의 값)\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomCrop(32, padding=4), \n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5071, 0.4867, 0.4408], std=[0.2675, 0.2565, 0.2761])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2bee68-5466-4bdc-bbaa-2ae55ca6463e",
   "metadata": {},
   "source": [
    "### **Load Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a3ac53-39cd-458e-802e-6b7da0170f63",
   "metadata": {},
   "source": [
    "**Splitting th training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2cf780b-d6ba-4123-a0c1-be7a7a762575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_val_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_val_transform)\n",
    "test_data = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f00da17f-0962-4907-8aae-505f5a64cfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified 방식으로 train 데이터셋을 train/val로 나누기 위해 라벨 정보 추출\n",
    "labels = train_val_data.targets  # CIFAR-100 라벨 목록\n",
    "\n",
    "# StratifiedShuffleSplit을 사용하여 train/validation을 80:20으로 나눕니다.\n",
    "stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "# train_index, val_index를 생성\n",
    "for train_index, val_index in stratified_split.split(train_val_data.data, labels):\n",
    "    train_data = Subset(train_val_data, train_index)\n",
    "    val_data = Subset(train_val_data, val_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59e202-d265-46b4-bd1a-206ad9d3f8b0",
   "metadata": {},
   "source": [
    "Check train datas distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dd0b7d2-1911-43e0-b4f6-c55c4861a7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset class distribution:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train dataset class distribution:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation dataset class distribution:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# 클래스 분포를 시각화하는 함수\n",
    "def plot_class_distribution(dataset, labels, title):\n",
    "    if isinstance(dataset, Subset):\n",
    "        indices = dataset.indices\n",
    "        subset_labels = np.array(labels)[indices]\n",
    "    else:\n",
    "        subset_labels = labels\n",
    "\n",
    "    class_counts = Counter(subset_labels)\n",
    "    classes, counts = zip(*sorted(class_counts.items()))  # 클래스 번호 순서대로 정렬\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(classes, counts, color='skyblue')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# 원본 데이터셋의 클래스 분포 시각화\n",
    "print(\"Original dataset class distribution:\")\n",
    "plot_class_distribution(train_val_data, labels, \"Original Dataset Class Distribution\")\n",
    "\n",
    "# Train 데이터셋의 클래스 분포 시각화\n",
    "print(\"\\nTrain dataset class distribution:\")\n",
    "plot_class_distribution(train_data, labels, \"Train Dataset Class Distribution\")\n",
    "\n",
    "# Validation 데이터셋의 클래스 분포 시각화\n",
    "print(\"\\nValidation dataset class distribution:\")\n",
    "plot_class_distribution(val_data, labels, \"Validation Dataset Class Distribution\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b3c41-7fe8-4959-81b8-50354ef33c9c",
   "metadata": {},
   "source": [
    "**Define DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6133b787-b855-4064-b019-88a62b64d56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b18f60-5d55-4b53-9506-788bcb425362",
   "metadata": {},
   "source": [
    "# **Model - 하연**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb4694-6995-4316-8009-49c47d4d5fdd",
   "metadata": {},
   "source": [
    "models 폴더에 만들고 import 하는 식으로 해주세요"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba9548e-68bc-4b50-8f43-75c084ae7f3d",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16c71685-4257-4da5-8f8b-6d8f8945f4d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use: cuda:0\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 112, 112]             864\n",
      "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
      "   SwishActivation-3         [-1, 32, 112, 112]               0\n",
      "            Conv2d-4         [-1, 32, 112, 112]           1,024\n",
      "       BatchNorm2d-5         [-1, 32, 112, 112]              64\n",
      "   SwishActivation-6         [-1, 32, 112, 112]               0\n",
      "            Conv2d-7         [-1, 32, 112, 112]             288\n",
      "       BatchNorm2d-8         [-1, 32, 112, 112]              64\n",
      "   SwishActivation-9         [-1, 32, 112, 112]               0\n",
      "AdaptiveAvgPool2d-10             [-1, 32, 1, 1]               0\n",
      "           Linear-11                    [-1, 8]             264\n",
      "             ReLU-12                    [-1, 8]               0\n",
      "           Linear-13                   [-1, 32]             288\n",
      "          Sigmoid-14                   [-1, 32]               0\n",
      "          SEBlock-15         [-1, 32, 112, 112]               0\n",
      "           Conv2d-16         [-1, 16, 112, 112]             512\n",
      "      BatchNorm2d-17         [-1, 16, 112, 112]              32\n",
      "      MBConvBlock-18         [-1, 16, 112, 112]               0\n",
      "           Conv2d-19         [-1, 96, 112, 112]           1,536\n",
      "      BatchNorm2d-20         [-1, 96, 112, 112]             192\n",
      "  SwishActivation-21         [-1, 96, 112, 112]               0\n",
      "           Conv2d-22           [-1, 96, 56, 56]             864\n",
      "      BatchNorm2d-23           [-1, 96, 56, 56]             192\n",
      "  SwishActivation-24           [-1, 96, 56, 56]               0\n",
      "AdaptiveAvgPool2d-25             [-1, 96, 1, 1]               0\n",
      "           Linear-26                   [-1, 24]           2,328\n",
      "             ReLU-27                   [-1, 24]               0\n",
      "           Linear-28                   [-1, 96]           2,400\n",
      "          Sigmoid-29                   [-1, 96]               0\n",
      "          SEBlock-30           [-1, 96, 56, 56]               0\n",
      "           Conv2d-31           [-1, 24, 56, 56]           2,304\n",
      "      BatchNorm2d-32           [-1, 24, 56, 56]              48\n",
      "      MBConvBlock-33           [-1, 24, 56, 56]               0\n",
      "           Conv2d-34          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-35          [-1, 144, 56, 56]             288\n",
      "  SwishActivation-36          [-1, 144, 56, 56]               0\n",
      "           Conv2d-37          [-1, 144, 56, 56]           1,296\n",
      "      BatchNorm2d-38          [-1, 144, 56, 56]             288\n",
      "  SwishActivation-39          [-1, 144, 56, 56]               0\n",
      "AdaptiveAvgPool2d-40            [-1, 144, 1, 1]               0\n",
      "           Linear-41                   [-1, 36]           5,220\n",
      "             ReLU-42                   [-1, 36]               0\n",
      "           Linear-43                  [-1, 144]           5,328\n",
      "          Sigmoid-44                  [-1, 144]               0\n",
      "          SEBlock-45          [-1, 144, 56, 56]               0\n",
      "           Conv2d-46           [-1, 24, 56, 56]           3,456\n",
      "      BatchNorm2d-47           [-1, 24, 56, 56]              48\n",
      "      MBConvBlock-48           [-1, 24, 56, 56]               0\n",
      "           Conv2d-49          [-1, 144, 56, 56]           3,456\n",
      "      BatchNorm2d-50          [-1, 144, 56, 56]             288\n",
      "  SwishActivation-51          [-1, 144, 56, 56]               0\n",
      "           Conv2d-52          [-1, 144, 28, 28]           3,600\n",
      "      BatchNorm2d-53          [-1, 144, 28, 28]             288\n",
      "  SwishActivation-54          [-1, 144, 28, 28]               0\n",
      "AdaptiveAvgPool2d-55            [-1, 144, 1, 1]               0\n",
      "           Linear-56                   [-1, 36]           5,220\n",
      "             ReLU-57                   [-1, 36]               0\n",
      "           Linear-58                  [-1, 144]           5,328\n",
      "          Sigmoid-59                  [-1, 144]               0\n",
      "          SEBlock-60          [-1, 144, 28, 28]               0\n",
      "           Conv2d-61           [-1, 40, 28, 28]           5,760\n",
      "      BatchNorm2d-62           [-1, 40, 28, 28]              80\n",
      "      MBConvBlock-63           [-1, 40, 28, 28]               0\n",
      "           Conv2d-64          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-65          [-1, 240, 28, 28]             480\n",
      "  SwishActivation-66          [-1, 240, 28, 28]               0\n",
      "           Conv2d-67          [-1, 240, 28, 28]           6,000\n",
      "      BatchNorm2d-68          [-1, 240, 28, 28]             480\n",
      "  SwishActivation-69          [-1, 240, 28, 28]               0\n",
      "AdaptiveAvgPool2d-70            [-1, 240, 1, 1]               0\n",
      "           Linear-71                   [-1, 60]          14,460\n",
      "             ReLU-72                   [-1, 60]               0\n",
      "           Linear-73                  [-1, 240]          14,640\n",
      "          Sigmoid-74                  [-1, 240]               0\n",
      "          SEBlock-75          [-1, 240, 28, 28]               0\n",
      "           Conv2d-76           [-1, 40, 28, 28]           9,600\n",
      "      BatchNorm2d-77           [-1, 40, 28, 28]              80\n",
      "      MBConvBlock-78           [-1, 40, 28, 28]               0\n",
      "           Conv2d-79          [-1, 240, 28, 28]           9,600\n",
      "      BatchNorm2d-80          [-1, 240, 28, 28]             480\n",
      "  SwishActivation-81          [-1, 240, 28, 28]               0\n",
      "           Conv2d-82          [-1, 240, 14, 14]           2,160\n",
      "      BatchNorm2d-83          [-1, 240, 14, 14]             480\n",
      "  SwishActivation-84          [-1, 240, 14, 14]               0\n",
      "AdaptiveAvgPool2d-85            [-1, 240, 1, 1]               0\n",
      "           Linear-86                   [-1, 60]          14,460\n",
      "             ReLU-87                   [-1, 60]               0\n",
      "           Linear-88                  [-1, 240]          14,640\n",
      "          Sigmoid-89                  [-1, 240]               0\n",
      "          SEBlock-90          [-1, 240, 14, 14]               0\n",
      "           Conv2d-91           [-1, 80, 14, 14]          19,200\n",
      "      BatchNorm2d-92           [-1, 80, 14, 14]             160\n",
      "      MBConvBlock-93           [-1, 80, 14, 14]               0\n",
      "           Conv2d-94          [-1, 480, 14, 14]          38,400\n",
      "      BatchNorm2d-95          [-1, 480, 14, 14]             960\n",
      "  SwishActivation-96          [-1, 480, 14, 14]               0\n",
      "           Conv2d-97          [-1, 480, 14, 14]           4,320\n",
      "      BatchNorm2d-98          [-1, 480, 14, 14]             960\n",
      "  SwishActivation-99          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-100            [-1, 480, 1, 1]               0\n",
      "          Linear-101                  [-1, 120]          57,720\n",
      "            ReLU-102                  [-1, 120]               0\n",
      "          Linear-103                  [-1, 480]          58,080\n",
      "         Sigmoid-104                  [-1, 480]               0\n",
      "         SEBlock-105          [-1, 480, 14, 14]               0\n",
      "          Conv2d-106           [-1, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-107           [-1, 80, 14, 14]             160\n",
      "     MBConvBlock-108           [-1, 80, 14, 14]               0\n",
      "          Conv2d-109          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-110          [-1, 480, 14, 14]             960\n",
      " SwishActivation-111          [-1, 480, 14, 14]               0\n",
      "          Conv2d-112          [-1, 480, 14, 14]           4,320\n",
      "     BatchNorm2d-113          [-1, 480, 14, 14]             960\n",
      " SwishActivation-114          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-115            [-1, 480, 1, 1]               0\n",
      "          Linear-116                  [-1, 120]          57,720\n",
      "            ReLU-117                  [-1, 120]               0\n",
      "          Linear-118                  [-1, 480]          58,080\n",
      "         Sigmoid-119                  [-1, 480]               0\n",
      "         SEBlock-120          [-1, 480, 14, 14]               0\n",
      "          Conv2d-121           [-1, 80, 14, 14]          38,400\n",
      "     BatchNorm2d-122           [-1, 80, 14, 14]             160\n",
      "     MBConvBlock-123           [-1, 80, 14, 14]               0\n",
      "          Conv2d-124          [-1, 480, 14, 14]          38,400\n",
      "     BatchNorm2d-125          [-1, 480, 14, 14]             960\n",
      " SwishActivation-126          [-1, 480, 14, 14]               0\n",
      "          Conv2d-127          [-1, 480, 14, 14]          12,000\n",
      "     BatchNorm2d-128          [-1, 480, 14, 14]             960\n",
      " SwishActivation-129          [-1, 480, 14, 14]               0\n",
      "AdaptiveAvgPool2d-130            [-1, 480, 1, 1]               0\n",
      "          Linear-131                  [-1, 120]          57,720\n",
      "            ReLU-132                  [-1, 120]               0\n",
      "          Linear-133                  [-1, 480]          58,080\n",
      "         Sigmoid-134                  [-1, 480]               0\n",
      "         SEBlock-135          [-1, 480, 14, 14]               0\n",
      "          Conv2d-136          [-1, 112, 14, 14]          53,760\n",
      "     BatchNorm2d-137          [-1, 112, 14, 14]             224\n",
      "     MBConvBlock-138          [-1, 112, 14, 14]               0\n",
      "          Conv2d-139          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-140          [-1, 672, 14, 14]           1,344\n",
      " SwishActivation-141          [-1, 672, 14, 14]               0\n",
      "          Conv2d-142          [-1, 672, 14, 14]          16,800\n",
      "     BatchNorm2d-143          [-1, 672, 14, 14]           1,344\n",
      " SwishActivation-144          [-1, 672, 14, 14]               0\n",
      "AdaptiveAvgPool2d-145            [-1, 672, 1, 1]               0\n",
      "          Linear-146                  [-1, 168]         113,064\n",
      "            ReLU-147                  [-1, 168]               0\n",
      "          Linear-148                  [-1, 672]         113,568\n",
      "         Sigmoid-149                  [-1, 672]               0\n",
      "         SEBlock-150          [-1, 672, 14, 14]               0\n",
      "          Conv2d-151          [-1, 112, 14, 14]          75,264\n",
      "     BatchNorm2d-152          [-1, 112, 14, 14]             224\n",
      "     MBConvBlock-153          [-1, 112, 14, 14]               0\n",
      "          Conv2d-154          [-1, 672, 14, 14]          75,264\n",
      "     BatchNorm2d-155          [-1, 672, 14, 14]           1,344\n",
      " SwishActivation-156          [-1, 672, 14, 14]               0\n",
      "          Conv2d-157            [-1, 672, 7, 7]          16,800\n",
      "     BatchNorm2d-158            [-1, 672, 7, 7]           1,344\n",
      " SwishActivation-159            [-1, 672, 7, 7]               0\n",
      "AdaptiveAvgPool2d-160            [-1, 672, 1, 1]               0\n",
      "          Linear-161                  [-1, 168]         113,064\n",
      "            ReLU-162                  [-1, 168]               0\n",
      "          Linear-163                  [-1, 672]         113,568\n",
      "         Sigmoid-164                  [-1, 672]               0\n",
      "         SEBlock-165            [-1, 672, 7, 7]               0\n",
      "          Conv2d-166            [-1, 192, 7, 7]         129,024\n",
      "     BatchNorm2d-167            [-1, 192, 7, 7]             384\n",
      "     MBConvBlock-168            [-1, 192, 7, 7]               0\n",
      "          Conv2d-169           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-170           [-1, 1152, 7, 7]           2,304\n",
      " SwishActivation-171           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-172           [-1, 1152, 7, 7]          28,800\n",
      "     BatchNorm2d-173           [-1, 1152, 7, 7]           2,304\n",
      " SwishActivation-174           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-175           [-1, 1152, 1, 1]               0\n",
      "          Linear-176                  [-1, 288]         332,064\n",
      "            ReLU-177                  [-1, 288]               0\n",
      "          Linear-178                 [-1, 1152]         332,928\n",
      "         Sigmoid-179                 [-1, 1152]               0\n",
      "         SEBlock-180           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-181            [-1, 192, 7, 7]         221,184\n",
      "     BatchNorm2d-182            [-1, 192, 7, 7]             384\n",
      "     MBConvBlock-183            [-1, 192, 7, 7]               0\n",
      "          Conv2d-184           [-1, 1152, 7, 7]         221,184\n",
      "     BatchNorm2d-185           [-1, 1152, 7, 7]           2,304\n",
      " SwishActivation-186           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-187           [-1, 1152, 7, 7]          10,368\n",
      "     BatchNorm2d-188           [-1, 1152, 7, 7]           2,304\n",
      " SwishActivation-189           [-1, 1152, 7, 7]               0\n",
      "AdaptiveAvgPool2d-190           [-1, 1152, 1, 1]               0\n",
      "          Linear-191                  [-1, 288]         332,064\n",
      "            ReLU-192                  [-1, 288]               0\n",
      "          Linear-193                 [-1, 1152]         332,928\n",
      "         Sigmoid-194                 [-1, 1152]               0\n",
      "         SEBlock-195           [-1, 1152, 7, 7]               0\n",
      "          Conv2d-196            [-1, 320, 7, 7]         368,640\n",
      "     BatchNorm2d-197            [-1, 320, 7, 7]             640\n",
      "     MBConvBlock-198            [-1, 320, 7, 7]               0\n",
      "          Conv2d-199           [-1, 1280, 7, 7]         409,600\n",
      "     BatchNorm2d-200           [-1, 1280, 7, 7]           2,560\n",
      " SwishActivation-201           [-1, 1280, 7, 7]               0\n",
      "AdaptiveAvgPool2d-202           [-1, 1280, 1, 1]               0\n",
      "         Flatten-203                 [-1, 1280]               0\n",
      "          Linear-204                  [-1, 100]         128,100\n",
      "================================================================\n",
      "Total params: 4,592,860\n",
      "Trainable params: 4,592,860\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 167.18\n",
      "Params size (MB): 17.52\n",
      "Estimated Total Size (MB): 185.28\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"use:\", device)\n",
    "\n",
    "# 모델 import 하기\n",
    "# from models.resnetRS import ResNetRS50\n",
    "# from models import resnetRS\n",
    "from models import efficientNet\n",
    "from models.efficientNet import EfficientNet\n",
    "\n",
    "# 모델 초기화\n",
    "net = efficientNet.EfficientNet()\n",
    "\n",
    "# 모델을 GPU로 이동\n",
    "net.to(device)\n",
    "\n",
    "# 모델 구조 출력\n",
    "print(summary(net, (3, 224, 224)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae781b4f-9e8b-4ea3-8b1e-9b67e857a088",
   "metadata": {},
   "source": [
    "### **Loss and Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a1b668f-0288-43c3-a08a-83a11a0d85d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176]}]\n"
     ]
    }
   ],
   "source": [
    "# 손실함수 초기화\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저 초기화\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "# 옵티마이저의 state_dict 출력\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f09003b-468e-4e89-9285-b6ec501c20cf",
   "metadata": {},
   "source": [
    "# **Train - 하연**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a80805-e7a2-4bdc-b854-9d183c306bf5",
   "metadata": {},
   "source": [
    "### **Model Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4337e604-8477-4a57-80fc-a2d96afcb153",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter(\"./runs/resnet_18/tensorboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13f6bb1b-efc7-4146-acbb-b2b28de16580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = \"./runs/resnet_18/checkpoints\"\n",
    "# early_stopping = EarlyStopping(save_path)\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17b20456-0576-49db-b3d1-97ef4cb49d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 함수\n",
    "def train_model(model, trainloader, criterion, optimizer, num_epochs=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "            labels = labels.type(torch.LongTensor).to(device)  # CPU에서 long type tensor로 변환\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 모델 예측\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # 역전파 및 최적화\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # 30번째 배치마다 상태 출력\n",
    "            if (batch_idx + 1) % 30 == 0:\n",
    "                print(f\"Batch [{batch_idx+1}/{len(trainloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Epoch당 평균 손실 계산 및 출력\n",
    "        epoch_loss = running_loss / len(trainloader.dataset)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping 체크\n",
    "        early_stopping(epoch_loss, model)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825de7e7-59bc-49aa-bd7a-c27a61c10fcb",
   "metadata": {},
   "source": [
    "**Model Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15180770-96df-487a-8c70-5743cc4dd448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 및 테스트 함수 (superclass 예측 포함)\n",
    "def test_model(model, testloader, criterion, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in testloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # 모델 예측\n",
    "            outputs = model(inputs)\n",
    "            test_loss += criterion(outputs, labels).item() * inputs.size(0)\n",
    "            \n",
    "            # 예측 결과 저장 및 정확도 계산\n",
    "            pred = outputs.argmax(dim=1)\n",
    "            correct += (pred == labels).sum().item()\n",
    "\n",
    "            # TensorBoard에 테스트 손실 및 정확도 기록\n",
    "            writer.add_scalar(\"Test Loss\", test_loss / len(testloader.dataset), epoch)\n",
    "            writer.add_scalar(\"Test Accuracy\", correct / len(testloader.dataset), epoch)\n",
    "\n",
    "    # 평균 손실 및 정확도 계산\n",
    "    test_loss /= len(testloader.dataset)\n",
    "    accuracy = correct / len(testloader.dataset)\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0669a96-2d88-4b8b-bc33-82701eabf923",
   "metadata": {},
   "source": [
    "### **Per-Epoch Activity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c330ce51-1f1d-4ad8-a127-a82f34c3899e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                    | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch [30/313], Loss: 4.6466\n",
      "Batch [60/313], Loss: 4.6349\n",
      "Batch [90/313], Loss: 4.6293\n",
      "Batch [120/313], Loss: 4.5360\n",
      "Batch [150/313], Loss: 4.5498\n",
      "Batch [180/313], Loss: 4.5761\n",
      "Batch [210/313], Loss: 4.5321\n",
      "Batch [240/313], Loss: 4.5141\n",
      "Batch [270/313], Loss: 4.5259\n",
      "Batch [300/313], Loss: 4.4235\n",
      "Epoch [1/20], Loss: 4.5716\n",
      "Validation loss decreased (inf --> 4.571617).  Saving model ...\n",
      "Batch [30/313], Loss: 4.3439\n",
      "Batch [60/313], Loss: 4.4229\n",
      "Batch [90/313], Loss: 4.4189\n",
      "Batch [120/313], Loss: 4.3297\n",
      "Batch [150/313], Loss: 4.3747\n",
      "Batch [180/313], Loss: 4.1262\n",
      "Batch [210/313], Loss: 4.2377\n",
      "Batch [240/313], Loss: 4.1177\n",
      "Batch [270/313], Loss: 4.1863\n",
      "Batch [300/313], Loss: 4.2737\n",
      "Epoch [2/20], Loss: 4.2671\n",
      "Validation loss decreased (4.571617 --> 4.267093).  Saving model ...\n",
      "Batch [30/313], Loss: 4.2060\n",
      "Batch [60/313], Loss: 3.9721\n",
      "Batch [90/313], Loss: 4.0382\n",
      "Batch [120/313], Loss: 4.0890\n",
      "Batch [150/313], Loss: 4.0109\n",
      "Batch [180/313], Loss: 4.0116\n",
      "Batch [210/313], Loss: 4.1934\n",
      "Batch [240/313], Loss: 3.8658\n",
      "Batch [270/313], Loss: 3.9826\n",
      "Batch [300/313], Loss: 4.0407\n",
      "Epoch [3/20], Loss: 4.0399\n",
      "Validation loss decreased (4.267093 --> 4.039857).  Saving model ...\n",
      "Batch [30/313], Loss: 3.9601\n",
      "Batch [60/313], Loss: 3.8720\n",
      "Batch [90/313], Loss: 3.7616\n",
      "Batch [120/313], Loss: 3.9445\n",
      "Batch [150/313], Loss: 3.8014\n",
      "Batch [180/313], Loss: 3.9902\n",
      "Batch [210/313], Loss: 3.9861\n",
      "Batch [240/313], Loss: 3.8325\n",
      "Batch [270/313], Loss: 3.8326\n",
      "Batch [300/313], Loss: 3.7639\n",
      "Epoch [4/20], Loss: 3.8862\n",
      "Validation loss decreased (4.039857 --> 3.886186).  Saving model ...\n",
      "Batch [30/313], Loss: 4.0426\n",
      "Batch [60/313], Loss: 4.0051\n",
      "Batch [90/313], Loss: 3.6485\n",
      "Batch [120/313], Loss: 3.6864\n",
      "Batch [150/313], Loss: 3.8054\n",
      "Batch [180/313], Loss: 3.7552\n",
      "Batch [210/313], Loss: 3.8500\n",
      "Batch [240/313], Loss: 3.8327\n",
      "Batch [270/313], Loss: 3.7953\n",
      "Batch [300/313], Loss: 3.8969\n",
      "Epoch [5/20], Loss: 3.7776\n",
      "Validation loss decreased (3.886186 --> 3.777581).  Saving model ...\n",
      "Batch [30/313], Loss: 3.6105\n",
      "Batch [60/313], Loss: 3.8182\n",
      "Batch [90/313], Loss: 3.8003\n",
      "Batch [120/313], Loss: 3.6812\n",
      "Batch [150/313], Loss: 3.8409\n",
      "Batch [180/313], Loss: 3.8639\n",
      "Batch [210/313], Loss: 3.5010\n",
      "Batch [240/313], Loss: 3.6580\n",
      "Batch [270/313], Loss: 3.6227\n",
      "Batch [300/313], Loss: 3.6783\n",
      "Epoch [6/20], Loss: 3.6977\n",
      "Validation loss decreased (3.777581 --> 3.697717).  Saving model ...\n",
      "Batch [30/313], Loss: 3.7268\n",
      "Batch [60/313], Loss: 3.6162\n",
      "Batch [90/313], Loss: 3.5632\n",
      "Batch [120/313], Loss: 3.6789\n",
      "Batch [150/313], Loss: 3.5089\n",
      "Batch [180/313], Loss: 3.4163\n",
      "Batch [210/313], Loss: 3.6390\n",
      "Batch [240/313], Loss: 3.6522\n",
      "Batch [270/313], Loss: 3.7216\n",
      "Batch [300/313], Loss: 3.7838\n",
      "Epoch [7/20], Loss: 3.6180\n",
      "Validation loss decreased (3.697717 --> 3.617969).  Saving model ...\n",
      "Batch [30/313], Loss: 3.8156\n",
      "Batch [60/313], Loss: 3.5166\n",
      "Batch [90/313], Loss: 3.7160\n",
      "Batch [120/313], Loss: 3.6249\n",
      "Batch [150/313], Loss: 3.5559\n",
      "Batch [180/313], Loss: 3.4647\n",
      "Batch [210/313], Loss: 3.3847\n",
      "Batch [240/313], Loss: 3.5410\n",
      "Batch [270/313], Loss: 3.3666\n",
      "Batch [300/313], Loss: 3.6123\n",
      "Epoch [8/20], Loss: 3.5547\n",
      "Validation loss decreased (3.617969 --> 3.554733).  Saving model ...\n",
      "Batch [30/313], Loss: 3.5144\n",
      "Batch [60/313], Loss: 3.5024\n",
      "Batch [90/313], Loss: 3.5466\n",
      "Batch [120/313], Loss: 3.3817\n",
      "Batch [150/313], Loss: 3.4810\n",
      "Batch [180/313], Loss: 3.4209\n",
      "Batch [210/313], Loss: 3.4069\n",
      "Batch [240/313], Loss: 3.3556\n",
      "Batch [270/313], Loss: 3.7013\n",
      "Batch [300/313], Loss: 3.4258\n",
      "Epoch [9/20], Loss: 3.4836\n",
      "Validation loss decreased (3.554733 --> 3.483569).  Saving model ...\n",
      "Batch [30/313], Loss: 3.3689\n",
      "Batch [60/313], Loss: 3.6689\n",
      "Batch [90/313], Loss: 3.4336\n",
      "Batch [120/313], Loss: 3.5111\n",
      "Batch [150/313], Loss: 3.2161\n",
      "Batch [180/313], Loss: 3.4649\n",
      "Batch [210/313], Loss: 3.4375\n",
      "Batch [240/313], Loss: 3.2601\n",
      "Batch [270/313], Loss: 3.1272\n",
      "Batch [300/313], Loss: 3.3737\n",
      "Epoch [10/20], Loss: 3.4331\n",
      "Validation loss decreased (3.483569 --> 3.433139).  Saving model ...\n",
      "Batch [30/313], Loss: 3.2994\n",
      "Batch [60/313], Loss: 3.3967\n",
      "Batch [90/313], Loss: 3.5583\n",
      "Batch [120/313], Loss: 3.5605\n",
      "Batch [150/313], Loss: 3.4157\n",
      "Batch [180/313], Loss: 3.2832\n",
      "Batch [210/313], Loss: 3.3495\n",
      "Batch [240/313], Loss: 3.3694\n",
      "Batch [270/313], Loss: 3.1127\n",
      "Batch [300/313], Loss: 3.3749\n",
      "Epoch [11/20], Loss: 3.3797\n",
      "Validation loss decreased (3.433139 --> 3.379702).  Saving model ...\n",
      "Batch [30/313], Loss: 3.3927\n",
      "Batch [60/313], Loss: 3.0905\n",
      "Batch [90/313], Loss: 3.7842\n",
      "Batch [120/313], Loss: 3.4665\n",
      "Batch [150/313], Loss: 3.1978\n",
      "Batch [180/313], Loss: 3.2455\n",
      "Batch [210/313], Loss: 3.3771\n",
      "Batch [240/313], Loss: 3.1985\n",
      "Batch [270/313], Loss: 3.2932\n",
      "Batch [300/313], Loss: 3.3492\n",
      "Epoch [12/20], Loss: 3.3254\n",
      "Validation loss decreased (3.379702 --> 3.325380).  Saving model ...\n",
      "Batch [30/313], Loss: 3.4307\n",
      "Batch [60/313], Loss: 3.1975\n",
      "Batch [90/313], Loss: 3.1079\n",
      "Batch [120/313], Loss: 3.4611\n",
      "Batch [150/313], Loss: 3.1757\n",
      "Batch [180/313], Loss: 3.1038\n",
      "Batch [210/313], Loss: 3.5459\n",
      "Batch [240/313], Loss: 3.2633\n",
      "Batch [270/313], Loss: 3.2273\n",
      "Batch [300/313], Loss: 3.3627\n",
      "Epoch [13/20], Loss: 3.2850\n",
      "Validation loss decreased (3.325380 --> 3.285010).  Saving model ...\n",
      "Batch [30/313], Loss: 3.3900\n",
      "Batch [60/313], Loss: 3.1240\n",
      "Batch [90/313], Loss: 3.1012\n",
      "Batch [120/313], Loss: 3.1686\n",
      "Batch [150/313], Loss: 3.2999\n",
      "Batch [180/313], Loss: 3.2638\n",
      "Batch [210/313], Loss: 3.2307\n",
      "Batch [240/313], Loss: 3.1236\n",
      "Batch [270/313], Loss: 3.3019\n",
      "Batch [300/313], Loss: 2.9255\n",
      "Epoch [14/20], Loss: 3.2378\n",
      "Validation loss decreased (3.285010 --> 3.237824).  Saving model ...\n",
      "Batch [30/313], Loss: 3.2019\n",
      "Batch [60/313], Loss: 3.1794\n",
      "Batch [90/313], Loss: 3.1859\n",
      "Batch [120/313], Loss: 3.1588\n",
      "Batch [150/313], Loss: 3.3532\n",
      "Batch [180/313], Loss: 3.1445\n",
      "Batch [210/313], Loss: 3.4647\n",
      "Batch [240/313], Loss: 3.3453\n",
      "Batch [270/313], Loss: 3.1788\n",
      "Batch [300/313], Loss: 3.2342\n",
      "Epoch [15/20], Loss: 3.1980\n",
      "Validation loss decreased (3.237824 --> 3.197984).  Saving model ...\n",
      "Batch [30/313], Loss: 3.3384\n",
      "Batch [60/313], Loss: 2.9489\n",
      "Batch [90/313], Loss: 2.8969\n",
      "Batch [120/313], Loss: 3.2016\n",
      "Batch [150/313], Loss: 2.8613\n",
      "Batch [180/313], Loss: 3.2976\n",
      "Batch [210/313], Loss: 3.0567\n",
      "Batch [240/313], Loss: 3.1789\n",
      "Batch [270/313], Loss: 3.1385\n",
      "Batch [300/313], Loss: 3.2465\n",
      "Epoch [16/20], Loss: 3.1589\n",
      "Validation loss decreased (3.197984 --> 3.158893).  Saving model ...\n",
      "Batch [30/313], Loss: 3.1626\n",
      "Batch [60/313], Loss: 3.2012\n",
      "Batch [90/313], Loss: 2.8154\n",
      "Batch [120/313], Loss: 3.1380\n",
      "Batch [150/313], Loss: 3.0714\n",
      "Batch [180/313], Loss: 3.2266\n",
      "Batch [210/313], Loss: 3.1832\n",
      "Batch [240/313], Loss: 3.0649\n",
      "Batch [270/313], Loss: 2.9813\n",
      "Batch [300/313], Loss: 3.2005\n",
      "Epoch [17/20], Loss: 3.1247\n",
      "Validation loss decreased (3.158893 --> 3.124695).  Saving model ...\n",
      "Batch [30/313], Loss: 3.2648\n",
      "Batch [60/313], Loss: 3.1650\n",
      "Batch [90/313], Loss: 2.9589\n",
      "Batch [120/313], Loss: 3.2401\n",
      "Batch [150/313], Loss: 3.1485\n",
      "Batch [180/313], Loss: 3.1448\n",
      "Batch [210/313], Loss: 3.0311\n",
      "Batch [240/313], Loss: 2.9028\n",
      "Batch [270/313], Loss: 3.1682\n",
      "Batch [300/313], Loss: 2.9700\n",
      "Epoch [18/20], Loss: 3.0930\n",
      "Validation loss decreased (3.124695 --> 3.092961).  Saving model ...\n",
      "Batch [30/313], Loss: 3.1544\n",
      "Batch [60/313], Loss: 2.8305\n",
      "Batch [90/313], Loss: 3.0376\n",
      "Batch [120/313], Loss: 2.8707\n",
      "Batch [150/313], Loss: 2.9880\n",
      "Batch [180/313], Loss: 2.9702\n",
      "Batch [210/313], Loss: 3.0367\n",
      "Batch [240/313], Loss: 3.0766\n",
      "Batch [270/313], Loss: 3.0923\n",
      "Batch [300/313], Loss: 3.0393\n",
      "Epoch [19/20], Loss: 3.0514\n",
      "Validation loss decreased (3.092961 --> 3.051447).  Saving model ...\n",
      "Batch [30/313], Loss: 2.8565\n",
      "Batch [60/313], Loss: 3.1006\n",
      "Batch [90/313], Loss: 2.8755\n",
      "Batch [120/313], Loss: 3.1512\n",
      "Batch [150/313], Loss: 3.1971\n",
      "Batch [180/313], Loss: 2.9048\n",
      "Batch [210/313], Loss: 3.1562\n",
      "Batch [240/313], Loss: 2.9371\n",
      "Batch [270/313], Loss: 2.8796\n",
      "Batch [300/313], Loss: 3.0366\n",
      "Epoch [20/20], Loss: 3.0148\n",
      "Validation loss decreased (3.051447 --> 3.014839).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██                                      | 1/20 [23:27<7:25:40, 1407.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]   Loss: 2.9229   Accuracy: 27.37%\n",
      "Batch [30/313], Loss: 3.0685\n",
      "Batch [60/313], Loss: 2.8553\n",
      "Batch [90/313], Loss: 3.1370\n",
      "Batch [120/313], Loss: 2.8980\n",
      "Batch [150/313], Loss: 2.8095\n",
      "Batch [180/313], Loss: 3.1143\n",
      "Batch [210/313], Loss: 2.9950\n",
      "Batch [240/313], Loss: 3.2338\n",
      "Batch [270/313], Loss: 3.0470\n",
      "Batch [300/313], Loss: 3.0304\n",
      "Epoch [1/20], Loss: 2.9837\n",
      "Validation loss decreased (3.014839 --> 2.983719).  Saving model ...\n",
      "Batch [30/313], Loss: 2.6869\n",
      "Batch [60/313], Loss: 3.1254\n",
      "Batch [90/313], Loss: 2.9286\n",
      "Batch [120/313], Loss: 2.8528\n",
      "Batch [150/313], Loss: 3.1816\n",
      "Batch [180/313], Loss: 3.1451\n",
      "Batch [210/313], Loss: 3.0382\n",
      "Batch [240/313], Loss: 3.0035\n",
      "Batch [270/313], Loss: 2.9164\n",
      "Batch [300/313], Loss: 2.9252\n",
      "Epoch [2/20], Loss: 2.9558\n",
      "Validation loss decreased (2.983719 --> 2.955828).  Saving model ...\n",
      "Batch [30/313], Loss: 2.8573\n",
      "Batch [60/313], Loss: 2.9200\n",
      "Batch [90/313], Loss: 3.0024\n",
      "Batch [120/313], Loss: 3.3556\n",
      "Batch [150/313], Loss: 3.1431\n",
      "Batch [180/313], Loss: 2.9680\n",
      "Batch [210/313], Loss: 2.8728\n",
      "Batch [240/313], Loss: 2.7633\n",
      "Batch [270/313], Loss: 2.7256\n",
      "Batch [300/313], Loss: 2.7900\n",
      "Epoch [3/20], Loss: 2.9152\n",
      "Validation loss decreased (2.955828 --> 2.915216).  Saving model ...\n",
      "Batch [30/313], Loss: 2.8550\n",
      "Batch [60/313], Loss: 2.6340\n",
      "Batch [90/313], Loss: 2.8986\n",
      "Batch [120/313], Loss: 2.9730\n",
      "Batch [150/313], Loss: 2.9203\n",
      "Batch [180/313], Loss: 2.5866\n",
      "Batch [210/313], Loss: 2.9205\n",
      "Batch [240/313], Loss: 2.7031\n",
      "Batch [270/313], Loss: 2.9247\n",
      "Batch [300/313], Loss: 2.9350\n",
      "Epoch [4/20], Loss: 2.8906\n",
      "Validation loss decreased (2.915216 --> 2.890597).  Saving model ...\n",
      "Batch [30/313], Loss: 2.8005\n",
      "Batch [60/313], Loss: 2.5960\n",
      "Batch [90/313], Loss: 2.9724\n",
      "Batch [120/313], Loss: 2.6856\n",
      "Batch [150/313], Loss: 2.8434\n",
      "Batch [180/313], Loss: 3.1108\n",
      "Batch [210/313], Loss: 2.9275\n",
      "Batch [240/313], Loss: 2.8246\n",
      "Batch [270/313], Loss: 2.7825\n",
      "Batch [300/313], Loss: 2.6459\n",
      "Epoch [5/20], Loss: 2.8650\n",
      "Validation loss decreased (2.890597 --> 2.865042).  Saving model ...\n",
      "Batch [30/313], Loss: 3.0433\n",
      "Batch [60/313], Loss: 2.8152\n",
      "Batch [90/313], Loss: 2.7008\n",
      "Batch [120/313], Loss: 2.6902\n",
      "Batch [150/313], Loss: 2.7535\n",
      "Batch [180/313], Loss: 3.0758\n",
      "Batch [210/313], Loss: 2.6723\n",
      "Batch [240/313], Loss: 2.8031\n",
      "Batch [270/313], Loss: 2.7871\n",
      "Batch [300/313], Loss: 2.9286\n",
      "Epoch [6/20], Loss: 2.8321\n",
      "Validation loss decreased (2.865042 --> 2.832059).  Saving model ...\n",
      "Batch [30/313], Loss: 2.5959\n",
      "Batch [60/313], Loss: 2.9197\n",
      "Batch [90/313], Loss: 2.8115\n",
      "Batch [120/313], Loss: 2.7189\n",
      "Batch [150/313], Loss: 2.4944\n",
      "Batch [180/313], Loss: 2.6640\n",
      "Batch [210/313], Loss: 2.6087\n",
      "Batch [240/313], Loss: 2.8437\n",
      "Batch [270/313], Loss: 2.6497\n",
      "Batch [300/313], Loss: 2.8620\n",
      "Epoch [7/20], Loss: 2.8012\n",
      "Validation loss decreased (2.832059 --> 2.801173).  Saving model ...\n",
      "Batch [30/313], Loss: 2.9452\n",
      "Batch [60/313], Loss: 2.8689\n",
      "Batch [90/313], Loss: 3.0986\n",
      "Batch [120/313], Loss: 2.6337\n",
      "Batch [150/313], Loss: 2.8150\n",
      "Batch [180/313], Loss: 2.6274\n",
      "Batch [210/313], Loss: 2.9017\n",
      "Batch [240/313], Loss: 2.7089\n",
      "Batch [270/313], Loss: 2.9552\n",
      "Batch [300/313], Loss: 2.7058\n",
      "Epoch [8/20], Loss: 2.7700\n",
      "Validation loss decreased (2.801173 --> 2.770004).  Saving model ...\n",
      "Batch [30/313], Loss: 2.6626\n",
      "Batch [60/313], Loss: 2.7223\n",
      "Batch [90/313], Loss: 2.9503\n",
      "Batch [120/313], Loss: 2.9693\n",
      "Batch [150/313], Loss: 2.5898\n",
      "Batch [180/313], Loss: 2.6543\n",
      "Batch [210/313], Loss: 2.6862\n",
      "Batch [240/313], Loss: 2.7190\n",
      "Batch [270/313], Loss: 2.6691\n",
      "Batch [300/313], Loss: 2.6813\n",
      "Epoch [9/20], Loss: 2.7356\n",
      "Validation loss decreased (2.770004 --> 2.735565).  Saving model ...\n",
      "Batch [30/313], Loss: 2.3243\n",
      "Batch [60/313], Loss: 2.8694\n",
      "Batch [90/313], Loss: 2.6464\n",
      "Batch [120/313], Loss: 2.7182\n",
      "Batch [150/313], Loss: 2.7458\n",
      "Batch [180/313], Loss: 2.5489\n",
      "Batch [210/313], Loss: 2.5359\n",
      "Batch [240/313], Loss: 2.5178\n",
      "Batch [270/313], Loss: 2.6589\n",
      "Batch [300/313], Loss: 2.9066\n",
      "Epoch [10/20], Loss: 2.7148\n",
      "Validation loss decreased (2.735565 --> 2.714827).  Saving model ...\n",
      "Batch [30/313], Loss: 2.7241\n",
      "Batch [60/313], Loss: 2.6256\n",
      "Batch [90/313], Loss: 2.6608\n",
      "Batch [120/313], Loss: 2.5093\n",
      "Batch [150/313], Loss: 2.6552\n",
      "Batch [180/313], Loss: 2.4324\n",
      "Batch [210/313], Loss: 2.8316\n",
      "Batch [240/313], Loss: 3.0351\n",
      "Batch [270/313], Loss: 2.8481\n",
      "Batch [300/313], Loss: 2.6823\n",
      "Epoch [11/20], Loss: 2.6817\n",
      "Validation loss decreased (2.714827 --> 2.681662).  Saving model ...\n",
      "Batch [30/313], Loss: 2.5437\n",
      "Batch [60/313], Loss: 2.4317\n",
      "Batch [90/313], Loss: 2.7837\n",
      "Batch [120/313], Loss: 2.6363\n",
      "Batch [150/313], Loss: 2.6786\n",
      "Batch [180/313], Loss: 2.8266\n",
      "Batch [210/313], Loss: 2.7736\n",
      "Batch [240/313], Loss: 2.9198\n",
      "Batch [270/313], Loss: 2.8003\n",
      "Batch [300/313], Loss: 2.6512\n",
      "Epoch [12/20], Loss: 2.6572\n",
      "Validation loss decreased (2.681662 --> 2.657178).  Saving model ...\n",
      "Batch [30/313], Loss: 2.8516\n",
      "Batch [60/313], Loss: 2.6831\n",
      "Batch [90/313], Loss: 2.5539\n",
      "Batch [120/313], Loss: 2.6938\n",
      "Batch [150/313], Loss: 2.4703\n",
      "Batch [180/313], Loss: 2.6749\n",
      "Batch [210/313], Loss: 2.7130\n",
      "Batch [240/313], Loss: 2.6359\n",
      "Batch [270/313], Loss: 2.8355\n",
      "Batch [300/313], Loss: 2.7132\n",
      "Epoch [13/20], Loss: 2.6313\n",
      "Validation loss decreased (2.657178 --> 2.631327).  Saving model ...\n",
      "Batch [30/313], Loss: 2.4838\n",
      "Batch [60/313], Loss: 2.6609\n",
      "Batch [90/313], Loss: 2.5495\n",
      "Batch [120/313], Loss: 2.6716\n",
      "Batch [150/313], Loss: 2.5218\n",
      "Batch [180/313], Loss: 2.4259\n",
      "Batch [210/313], Loss: 2.6638\n",
      "Batch [240/313], Loss: 2.5692\n",
      "Batch [270/313], Loss: 2.4435\n",
      "Batch [300/313], Loss: 2.7673\n",
      "Epoch [14/20], Loss: 2.6021\n",
      "Validation loss decreased (2.631327 --> 2.602063).  Saving model ...\n",
      "Batch [30/313], Loss: 2.7062\n",
      "Batch [60/313], Loss: 2.7005\n",
      "Batch [90/313], Loss: 2.5229\n",
      "Batch [120/313], Loss: 2.3903\n",
      "Batch [150/313], Loss: 2.3394\n",
      "Batch [180/313], Loss: 2.4629\n",
      "Batch [210/313], Loss: 2.5981\n",
      "Batch [240/313], Loss: 2.6703\n",
      "Batch [270/313], Loss: 2.5006\n",
      "Batch [300/313], Loss: 2.5896\n",
      "Epoch [15/20], Loss: 2.5839\n",
      "Validation loss decreased (2.602063 --> 2.583927).  Saving model ...\n",
      "Batch [30/313], Loss: 2.3868\n",
      "Batch [60/313], Loss: 2.6617\n",
      "Batch [90/313], Loss: 2.4875\n",
      "Batch [120/313], Loss: 2.5883\n",
      "Batch [150/313], Loss: 2.7344\n",
      "Batch [180/313], Loss: 2.4397\n",
      "Batch [210/313], Loss: 2.6705\n",
      "Batch [240/313], Loss: 2.9487\n",
      "Batch [270/313], Loss: 2.4794\n",
      "Batch [300/313], Loss: 2.7510\n",
      "Epoch [16/20], Loss: 2.5539\n",
      "Validation loss decreased (2.583927 --> 2.553868).  Saving model ...\n",
      "Batch [30/313], Loss: 2.6337\n",
      "Batch [60/313], Loss: 2.4751\n",
      "Batch [90/313], Loss: 2.5481\n",
      "Batch [120/313], Loss: 2.4516\n",
      "Batch [150/313], Loss: 2.5934\n",
      "Batch [180/313], Loss: 2.6955\n",
      "Batch [210/313], Loss: 2.2689\n",
      "Batch [240/313], Loss: 2.5027\n",
      "Batch [270/313], Loss: 2.3607\n",
      "Batch [300/313], Loss: 2.6611\n",
      "Epoch [17/20], Loss: 2.5232\n",
      "Validation loss decreased (2.553868 --> 2.523166).  Saving model ...\n",
      "Batch [30/313], Loss: 2.3878\n",
      "Batch [60/313], Loss: 2.6979\n",
      "Batch [90/313], Loss: 2.4768\n",
      "Batch [120/313], Loss: 2.5983\n",
      "Batch [150/313], Loss: 2.7161\n",
      "Batch [180/313], Loss: 2.4819\n",
      "Batch [210/313], Loss: 2.1793\n",
      "Batch [240/313], Loss: 2.4664\n",
      "Batch [270/313], Loss: 2.7232\n",
      "Batch [300/313], Loss: 2.4389\n",
      "Epoch [18/20], Loss: 2.5012\n",
      "Validation loss decreased (2.523166 --> 2.501217).  Saving model ...\n",
      "Batch [30/313], Loss: 2.4593\n",
      "Batch [60/313], Loss: 2.4470\n",
      "Batch [90/313], Loss: 2.4774\n",
      "Batch [120/313], Loss: 2.2188\n",
      "Batch [150/313], Loss: 2.3679\n",
      "Batch [180/313], Loss: 2.7743\n",
      "Batch [210/313], Loss: 2.3359\n",
      "Batch [240/313], Loss: 2.3419\n",
      "Batch [270/313], Loss: 2.5230\n",
      "Batch [300/313], Loss: 2.6349\n",
      "Epoch [19/20], Loss: 2.4756\n",
      "Validation loss decreased (2.501217 --> 2.475552).  Saving model ...\n",
      "Batch [30/313], Loss: 2.5175\n",
      "Batch [60/313], Loss: 2.4286\n",
      "Batch [90/313], Loss: 2.4041\n",
      "Batch [120/313], Loss: 2.5233\n",
      "Batch [150/313], Loss: 2.4698\n",
      "Batch [180/313], Loss: 2.3141\n",
      "Batch [210/313], Loss: 2.4863\n",
      "Batch [240/313], Loss: 2.6632\n",
      "Batch [270/313], Loss: 2.2928\n",
      "Batch [300/313], Loss: 2.5531\n",
      "Epoch [20/20], Loss: 2.4540\n",
      "Validation loss decreased (2.475552 --> 2.453974).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████                                    | 2/20 [50:08<7:36:20, 1521.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20]   Loss: 2.5771   Accuracy: 35.17%\n",
      "Batch [30/313], Loss: 2.4538\n",
      "Batch [60/313], Loss: 2.6232\n",
      "Batch [90/313], Loss: 2.4808\n",
      "Batch [120/313], Loss: 2.5905\n",
      "Batch [150/313], Loss: 2.3634\n",
      "Batch [180/313], Loss: 2.3710\n",
      "Batch [210/313], Loss: 2.4057\n",
      "Batch [240/313], Loss: 2.2141\n",
      "Batch [270/313], Loss: 2.4481\n",
      "Batch [300/313], Loss: 2.1581\n",
      "Epoch [1/20], Loss: 2.4259\n",
      "Validation loss decreased (2.453974 --> 2.425924).  Saving model ...\n",
      "Batch [30/313], Loss: 2.2154\n",
      "Batch [60/313], Loss: 2.2683\n",
      "Batch [90/313], Loss: 2.3529\n",
      "Batch [120/313], Loss: 2.7588\n",
      "Batch [150/313], Loss: 2.2913\n",
      "Batch [180/313], Loss: 2.3225\n",
      "Batch [210/313], Loss: 2.3930\n",
      "Batch [240/313], Loss: 2.4130\n",
      "Batch [270/313], Loss: 2.5254\n",
      "Batch [300/313], Loss: 2.3257\n",
      "Epoch [2/20], Loss: 2.3997\n",
      "Validation loss decreased (2.425924 --> 2.399746).  Saving model ...\n",
      "Batch [30/313], Loss: 2.1458\n",
      "Batch [60/313], Loss: 2.3404\n",
      "Batch [90/313], Loss: 2.5106\n",
      "Batch [120/313], Loss: 2.4598\n",
      "Batch [150/313], Loss: 2.3861\n",
      "Batch [180/313], Loss: 2.4484\n",
      "Batch [210/313], Loss: 2.0813\n",
      "Batch [240/313], Loss: 2.3828\n",
      "Batch [270/313], Loss: 2.5980\n",
      "Batch [300/313], Loss: 2.2792\n",
      "Epoch [3/20], Loss: 2.3756\n",
      "Validation loss decreased (2.399746 --> 2.375573).  Saving model ...\n",
      "Batch [30/313], Loss: 2.4180\n",
      "Batch [60/313], Loss: 2.5121\n",
      "Batch [90/313], Loss: 2.0068\n",
      "Batch [120/313], Loss: 2.2737\n",
      "Batch [150/313], Loss: 2.2918\n",
      "Batch [180/313], Loss: 2.2427\n",
      "Batch [210/313], Loss: 2.4732\n",
      "Batch [240/313], Loss: 2.2839\n",
      "Batch [270/313], Loss: 2.6072\n",
      "Batch [300/313], Loss: 2.0914\n",
      "Epoch [4/20], Loss: 2.3548\n",
      "Validation loss decreased (2.375573 --> 2.354816).  Saving model ...\n",
      "Batch [30/313], Loss: 2.3206\n",
      "Batch [60/313], Loss: 2.4599\n",
      "Batch [90/313], Loss: 2.1304\n",
      "Batch [120/313], Loss: 2.1640\n",
      "Batch [150/313], Loss: 2.0844\n",
      "Batch [180/313], Loss: 2.2444\n",
      "Batch [210/313], Loss: 2.0657\n",
      "Batch [240/313], Loss: 2.4608\n",
      "Batch [270/313], Loss: 2.3287\n",
      "Batch [300/313], Loss: 2.3357\n",
      "Epoch [5/20], Loss: 2.3285\n",
      "Validation loss decreased (2.354816 --> 2.328501).  Saving model ...\n",
      "Batch [30/313], Loss: 2.4779\n",
      "Batch [60/313], Loss: 2.1391\n",
      "Batch [90/313], Loss: 2.1787\n",
      "Batch [120/313], Loss: 2.3460\n",
      "Batch [150/313], Loss: 2.4525\n",
      "Batch [180/313], Loss: 2.2167\n",
      "Batch [210/313], Loss: 2.2158\n",
      "Batch [240/313], Loss: 2.3630\n",
      "Batch [270/313], Loss: 2.1593\n",
      "Batch [300/313], Loss: 2.2953\n",
      "Epoch [6/20], Loss: 2.3053\n",
      "Validation loss decreased (2.328501 --> 2.305340).  Saving model ...\n",
      "Batch [30/313], Loss: 2.2270\n",
      "Batch [60/313], Loss: 2.5081\n",
      "Batch [90/313], Loss: 2.1924\n",
      "Batch [120/313], Loss: 2.2839\n",
      "Batch [150/313], Loss: 2.1283\n",
      "Batch [180/313], Loss: 2.2621\n",
      "Batch [210/313], Loss: 2.0966\n",
      "Batch [240/313], Loss: 2.3557\n",
      "Batch [270/313], Loss: 2.2409\n",
      "Batch [300/313], Loss: 2.2772\n",
      "Epoch [7/20], Loss: 2.2837\n",
      "Validation loss decreased (2.305340 --> 2.283720).  Saving model ...\n",
      "Batch [30/313], Loss: 2.3930\n",
      "Batch [60/313], Loss: 2.2435\n",
      "Batch [90/313], Loss: 2.0909\n",
      "Batch [120/313], Loss: 2.2448\n",
      "Batch [150/313], Loss: 2.0788\n",
      "Batch [180/313], Loss: 2.2175\n",
      "Batch [210/313], Loss: 2.1851\n",
      "Batch [240/313], Loss: 2.1401\n",
      "Batch [270/313], Loss: 2.3215\n",
      "Batch [300/313], Loss: 2.0194\n",
      "Epoch [8/20], Loss: 2.2586\n",
      "Validation loss decreased (2.283720 --> 2.258569).  Saving model ...\n",
      "Batch [30/313], Loss: 2.2719\n",
      "Batch [60/313], Loss: 2.1378\n",
      "Batch [90/313], Loss: 1.9858\n",
      "Batch [120/313], Loss: 1.9670\n",
      "Batch [150/313], Loss: 2.3073\n",
      "Batch [180/313], Loss: 2.0693\n",
      "Batch [210/313], Loss: 2.2392\n",
      "Batch [240/313], Loss: 2.1980\n",
      "Batch [270/313], Loss: 2.0889\n",
      "Batch [300/313], Loss: 2.0120\n",
      "Epoch [9/20], Loss: 2.2251\n",
      "Validation loss decreased (2.258569 --> 2.225084).  Saving model ...\n",
      "Batch [30/313], Loss: 2.2887\n",
      "Batch [60/313], Loss: 2.2241\n",
      "Batch [90/313], Loss: 2.2045\n",
      "Batch [120/313], Loss: 2.3340\n",
      "Batch [150/313], Loss: 2.1581\n",
      "Batch [180/313], Loss: 2.2304\n",
      "Batch [210/313], Loss: 2.3711\n",
      "Batch [240/313], Loss: 2.2001\n",
      "Batch [270/313], Loss: 2.3103\n",
      "Batch [300/313], Loss: 1.9014\n",
      "Epoch [10/20], Loss: 2.2178\n",
      "Validation loss decreased (2.225084 --> 2.217806).  Saving model ...\n",
      "Batch [30/313], Loss: 1.9923\n",
      "Batch [60/313], Loss: 2.1437\n",
      "Batch [90/313], Loss: 2.1633\n",
      "Batch [120/313], Loss: 2.2901\n",
      "Batch [150/313], Loss: 2.1442\n",
      "Batch [180/313], Loss: 2.3013\n",
      "Batch [210/313], Loss: 1.9434\n",
      "Batch [240/313], Loss: 2.3406\n",
      "Batch [270/313], Loss: 2.0062\n",
      "Batch [300/313], Loss: 2.0753\n",
      "Epoch [11/20], Loss: 2.1819\n",
      "Validation loss decreased (2.217806 --> 2.181950).  Saving model ...\n",
      "Batch [30/313], Loss: 2.0032\n",
      "Batch [60/313], Loss: 2.4830\n",
      "Batch [90/313], Loss: 2.1359\n",
      "Batch [120/313], Loss: 2.3471\n",
      "Batch [150/313], Loss: 2.2396\n",
      "Batch [180/313], Loss: 2.1274\n",
      "Batch [210/313], Loss: 2.0273\n",
      "Batch [240/313], Loss: 1.9191\n",
      "Batch [270/313], Loss: 2.2419\n",
      "Batch [300/313], Loss: 2.0342\n",
      "Epoch [12/20], Loss: 2.1665\n",
      "Validation loss decreased (2.181950 --> 2.166465).  Saving model ...\n",
      "Batch [30/313], Loss: 2.1353\n",
      "Batch [60/313], Loss: 2.2859\n",
      "Batch [90/313], Loss: 2.3280\n",
      "Batch [120/313], Loss: 2.2767\n",
      "Batch [150/313], Loss: 2.2538\n",
      "Batch [180/313], Loss: 1.7825\n",
      "Batch [210/313], Loss: 2.1302\n",
      "Batch [240/313], Loss: 2.5294\n",
      "Batch [270/313], Loss: 2.0755\n",
      "Batch [300/313], Loss: 2.0053\n",
      "Epoch [13/20], Loss: 2.1451\n",
      "Validation loss decreased (2.166465 --> 2.145135).  Saving model ...\n",
      "Batch [30/313], Loss: 2.3831\n",
      "Batch [60/313], Loss: 1.7166\n",
      "Batch [90/313], Loss: 2.1217\n",
      "Batch [120/313], Loss: 1.8816\n",
      "Batch [150/313], Loss: 1.8831\n",
      "Batch [180/313], Loss: 2.0864\n",
      "Batch [210/313], Loss: 1.9193\n",
      "Batch [240/313], Loss: 1.9856\n",
      "Batch [270/313], Loss: 1.9849\n",
      "Batch [300/313], Loss: 2.1385\n",
      "Epoch [14/20], Loss: 2.1213\n",
      "Validation loss decreased (2.145135 --> 2.121303).  Saving model ...\n",
      "Batch [30/313], Loss: 1.8852\n",
      "Batch [60/313], Loss: 2.1087\n",
      "Batch [90/313], Loss: 1.8365\n",
      "Batch [120/313], Loss: 1.9935\n",
      "Batch [150/313], Loss: 1.9321\n",
      "Batch [180/313], Loss: 2.1798\n",
      "Batch [210/313], Loss: 2.3545\n",
      "Batch [240/313], Loss: 2.2086\n",
      "Batch [270/313], Loss: 1.9044\n",
      "Batch [300/313], Loss: 2.2729\n",
      "Epoch [15/20], Loss: 2.0994\n",
      "Validation loss decreased (2.121303 --> 2.099419).  Saving model ...\n",
      "Batch [30/313], Loss: 2.0368\n",
      "Batch [60/313], Loss: 2.2066\n",
      "Batch [90/313], Loss: 1.8708\n",
      "Batch [120/313], Loss: 2.3103\n",
      "Batch [150/313], Loss: 2.1886\n",
      "Batch [180/313], Loss: 2.2145\n",
      "Batch [210/313], Loss: 1.9824\n",
      "Batch [240/313], Loss: 2.1547\n",
      "Batch [270/313], Loss: 2.1333\n",
      "Batch [300/313], Loss: 1.9023\n",
      "Epoch [16/20], Loss: 2.0684\n",
      "Validation loss decreased (2.099419 --> 2.068422).  Saving model ...\n",
      "Batch [30/313], Loss: 2.0218\n",
      "Batch [60/313], Loss: 2.0277\n",
      "Batch [90/313], Loss: 2.1846\n",
      "Batch [120/313], Loss: 2.1909\n",
      "Batch [150/313], Loss: 1.8075\n",
      "Batch [180/313], Loss: 2.0493\n",
      "Batch [210/313], Loss: 1.9655\n",
      "Batch [240/313], Loss: 2.0316\n",
      "Batch [270/313], Loss: 2.0995\n",
      "Batch [300/313], Loss: 2.2701\n",
      "Epoch [17/20], Loss: 2.0491\n",
      "Validation loss decreased (2.068422 --> 2.049092).  Saving model ...\n",
      "Batch [30/313], Loss: 2.3367\n",
      "Batch [60/313], Loss: 2.0666\n",
      "Batch [90/313], Loss: 2.2760\n",
      "Batch [120/313], Loss: 2.0957\n",
      "Batch [150/313], Loss: 2.0383\n",
      "Batch [180/313], Loss: 1.9507\n",
      "Batch [210/313], Loss: 2.0845\n",
      "Batch [240/313], Loss: 2.0660\n",
      "Batch [270/313], Loss: 1.9965\n",
      "Batch [300/313], Loss: 2.0615\n",
      "Epoch [18/20], Loss: 2.0213\n",
      "Validation loss decreased (2.049092 --> 2.021350).  Saving model ...\n",
      "Batch [30/313], Loss: 1.7591\n",
      "Batch [60/313], Loss: 1.9916\n",
      "Batch [90/313], Loss: 2.1943\n",
      "Batch [120/313], Loss: 1.7882\n",
      "Batch [150/313], Loss: 2.0860\n",
      "Batch [180/313], Loss: 1.9225\n",
      "Batch [210/313], Loss: 2.1569\n",
      "Batch [240/313], Loss: 2.1911\n",
      "Batch [270/313], Loss: 1.8625\n",
      "Batch [300/313], Loss: 2.0981\n",
      "Epoch [19/20], Loss: 2.0007\n",
      "Validation loss decreased (2.021350 --> 2.000693).  Saving model ...\n",
      "Batch [30/313], Loss: 1.8338\n",
      "Batch [60/313], Loss: 1.9970\n",
      "Batch [90/313], Loss: 1.8550\n",
      "Batch [120/313], Loss: 2.0979\n",
      "Batch [150/313], Loss: 2.0790\n",
      "Batch [180/313], Loss: 2.1641\n",
      "Batch [210/313], Loss: 1.9976\n",
      "Batch [240/313], Loss: 1.9506\n",
      "Batch [270/313], Loss: 1.9177\n",
      "Batch [300/313], Loss: 2.0056\n",
      "Epoch [20/20], Loss: 1.9768\n",
      "Validation loss decreased (2.000693 --> 1.976844).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█████▋                                | 3/20 [1:17:23<7:25:42, 1573.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20]   Loss: 2.5184   Accuracy: 38.16%\n",
      "Batch [30/313], Loss: 2.2099\n",
      "Batch [60/313], Loss: 2.0006\n",
      "Batch [90/313], Loss: 1.9815\n",
      "Batch [120/313], Loss: 1.7880\n",
      "Batch [150/313], Loss: 1.9327\n",
      "Batch [180/313], Loss: 2.3116\n",
      "Batch [210/313], Loss: 2.1416\n",
      "Batch [240/313], Loss: 2.2171\n",
      "Batch [270/313], Loss: 2.0849\n",
      "Batch [300/313], Loss: 2.0495\n",
      "Epoch [1/20], Loss: 1.9739\n",
      "Validation loss decreased (1.976844 --> 1.973937).  Saving model ...\n",
      "Batch [30/313], Loss: 1.8748\n",
      "Batch [60/313], Loss: 2.1915\n",
      "Batch [90/313], Loss: 2.0061\n",
      "Batch [120/313], Loss: 1.8284\n",
      "Batch [150/313], Loss: 2.1469\n",
      "Batch [180/313], Loss: 1.9474\n",
      "Batch [210/313], Loss: 2.0230\n",
      "Batch [240/313], Loss: 1.9431\n",
      "Batch [270/313], Loss: 1.9419\n",
      "Batch [300/313], Loss: 2.2549\n",
      "Epoch [2/20], Loss: 1.9333\n",
      "Validation loss decreased (1.973937 --> 1.933286).  Saving model ...\n",
      "Batch [30/313], Loss: 1.8135\n",
      "Batch [60/313], Loss: 1.8096\n",
      "Batch [90/313], Loss: 1.7466\n",
      "Batch [120/313], Loss: 1.7684\n",
      "Batch [150/313], Loss: 1.8070\n",
      "Batch [180/313], Loss: 2.2398\n",
      "Batch [210/313], Loss: 1.7644\n",
      "Batch [240/313], Loss: 1.8627\n",
      "Batch [270/313], Loss: 1.8425\n",
      "Batch [300/313], Loss: 1.8489\n",
      "Epoch [3/20], Loss: 1.9111\n",
      "Validation loss decreased (1.933286 --> 1.911092).  Saving model ...\n",
      "Batch [30/313], Loss: 1.9721\n",
      "Batch [60/313], Loss: 1.8584\n",
      "Batch [90/313], Loss: 1.7404\n",
      "Batch [120/313], Loss: 1.8270\n",
      "Batch [150/313], Loss: 1.7533\n",
      "Batch [180/313], Loss: 1.6874\n",
      "Batch [210/313], Loss: 1.7380\n",
      "Batch [240/313], Loss: 2.0502\n",
      "Batch [270/313], Loss: 1.8954\n",
      "Batch [300/313], Loss: 2.0751\n",
      "Epoch [4/20], Loss: 1.8817\n",
      "Validation loss decreased (1.911092 --> 1.881720).  Saving model ...\n",
      "Batch [30/313], Loss: 1.8901\n",
      "Batch [60/313], Loss: 1.6824\n",
      "Batch [90/313], Loss: 1.7423\n",
      "Batch [120/313], Loss: 1.9001\n",
      "Batch [150/313], Loss: 2.0655\n",
      "Batch [180/313], Loss: 1.9899\n",
      "Batch [210/313], Loss: 1.7144\n",
      "Batch [240/313], Loss: 1.6836\n",
      "Batch [270/313], Loss: 1.8431\n",
      "Batch [300/313], Loss: 2.0315\n",
      "Epoch [5/20], Loss: 1.8657\n",
      "Validation loss decreased (1.881720 --> 1.865721).  Saving model ...\n",
      "Batch [30/313], Loss: 1.5772\n",
      "Batch [60/313], Loss: 1.7835\n",
      "Batch [90/313], Loss: 2.0125\n",
      "Batch [120/313], Loss: 2.2683\n",
      "Batch [150/313], Loss: 1.5211\n",
      "Batch [180/313], Loss: 1.6893\n",
      "Batch [210/313], Loss: 1.8335\n",
      "Batch [240/313], Loss: 2.1310\n",
      "Batch [270/313], Loss: 1.9653\n",
      "Batch [300/313], Loss: 2.0033\n",
      "Epoch [6/20], Loss: 1.8424\n",
      "Validation loss decreased (1.865721 --> 1.842435).  Saving model ...\n",
      "Batch [30/313], Loss: 1.8104\n",
      "Batch [60/313], Loss: 1.6735\n",
      "Batch [90/313], Loss: 1.8746\n",
      "Batch [120/313], Loss: 1.6497\n",
      "Batch [150/313], Loss: 1.8745\n",
      "Batch [180/313], Loss: 1.8604\n",
      "Batch [210/313], Loss: 1.7891\n",
      "Batch [240/313], Loss: 1.8454\n",
      "Batch [270/313], Loss: 1.7341\n",
      "Batch [300/313], Loss: 1.6348\n",
      "Epoch [7/20], Loss: 1.8160\n",
      "Validation loss decreased (1.842435 --> 1.816008).  Saving model ...\n",
      "Batch [30/313], Loss: 1.6789\n",
      "Batch [60/313], Loss: 1.6920\n",
      "Batch [90/313], Loss: 1.6072\n",
      "Batch [120/313], Loss: 1.6635\n",
      "Batch [150/313], Loss: 1.6551\n",
      "Batch [180/313], Loss: 1.8393\n",
      "Batch [210/313], Loss: 1.9386\n",
      "Batch [240/313], Loss: 1.8986\n",
      "Batch [270/313], Loss: 1.7003\n",
      "Batch [300/313], Loss: 1.8997\n",
      "Epoch [8/20], Loss: 1.7982\n",
      "Validation loss decreased (1.816008 --> 1.798216).  Saving model ...\n",
      "Batch [30/313], Loss: 1.8428\n",
      "Batch [60/313], Loss: 1.7863\n",
      "Batch [90/313], Loss: 1.8184\n",
      "Batch [120/313], Loss: 1.7943\n",
      "Batch [150/313], Loss: 1.8464\n",
      "Batch [180/313], Loss: 1.6749\n",
      "Batch [210/313], Loss: 1.8499\n",
      "Batch [240/313], Loss: 1.8379\n",
      "Batch [270/313], Loss: 1.9827\n",
      "Batch [300/313], Loss: 1.9071\n",
      "Epoch [9/20], Loss: 1.7701\n",
      "Validation loss decreased (1.798216 --> 1.770062).  Saving model ...\n",
      "Batch [30/313], Loss: 1.9863\n",
      "Batch [60/313], Loss: 1.7466\n",
      "Batch [90/313], Loss: 1.6760\n",
      "Batch [120/313], Loss: 1.4618\n",
      "Batch [150/313], Loss: 1.5770\n",
      "Batch [180/313], Loss: 1.5702\n",
      "Batch [210/313], Loss: 1.8014\n",
      "Batch [240/313], Loss: 1.6619\n",
      "Batch [270/313], Loss: 1.7913\n",
      "Batch [300/313], Loss: 1.7377\n",
      "Epoch [10/20], Loss: 1.7407\n",
      "Validation loss decreased (1.770062 --> 1.740676).  Saving model ...\n",
      "Batch [30/313], Loss: 1.7314\n",
      "Batch [60/313], Loss: 1.5987\n",
      "Batch [90/313], Loss: 1.7004\n",
      "Batch [120/313], Loss: 1.6235\n",
      "Batch [150/313], Loss: 1.6438\n",
      "Batch [180/313], Loss: 1.7698\n",
      "Batch [210/313], Loss: 1.6433\n",
      "Batch [240/313], Loss: 1.6636\n",
      "Batch [270/313], Loss: 1.6577\n",
      "Batch [300/313], Loss: 1.8829\n",
      "Epoch [11/20], Loss: 1.7340\n",
      "Validation loss decreased (1.740676 --> 1.733956).  Saving model ...\n",
      "Batch [30/313], Loss: 1.5293\n",
      "Batch [60/313], Loss: 1.5733\n",
      "Batch [90/313], Loss: 1.8457\n",
      "Batch [120/313], Loss: 1.7242\n",
      "Batch [150/313], Loss: 1.4921\n",
      "Batch [180/313], Loss: 1.6715\n",
      "Batch [210/313], Loss: 1.8552\n",
      "Batch [240/313], Loss: 1.7751\n",
      "Batch [270/313], Loss: 1.6783\n",
      "Batch [300/313], Loss: 1.8419\n",
      "Epoch [12/20], Loss: 1.7064\n",
      "Validation loss decreased (1.733956 --> 1.706354).  Saving model ...\n",
      "Batch [30/313], Loss: 1.5524\n",
      "Batch [60/313], Loss: 1.7608\n",
      "Batch [90/313], Loss: 1.6710\n",
      "Batch [120/313], Loss: 1.8034\n",
      "Batch [150/313], Loss: 1.8395\n",
      "Batch [180/313], Loss: 1.5190\n",
      "Batch [210/313], Loss: 1.6580\n",
      "Batch [240/313], Loss: 1.6401\n",
      "Batch [270/313], Loss: 1.7750\n",
      "Batch [300/313], Loss: 1.4387\n",
      "Epoch [13/20], Loss: 1.6784\n",
      "Validation loss decreased (1.706354 --> 1.678389).  Saving model ...\n",
      "Batch [30/313], Loss: 1.6477\n",
      "Batch [60/313], Loss: 1.4667\n",
      "Batch [90/313], Loss: 1.7413\n",
      "Batch [120/313], Loss: 1.5600\n",
      "Batch [150/313], Loss: 1.4504\n",
      "Batch [180/313], Loss: 1.6038\n",
      "Batch [210/313], Loss: 1.4648\n",
      "Batch [240/313], Loss: 1.4707\n",
      "Batch [270/313], Loss: 1.4917\n",
      "Batch [300/313], Loss: 1.7219\n",
      "Epoch [14/20], Loss: 1.6489\n",
      "Validation loss decreased (1.678389 --> 1.648874).  Saving model ...\n",
      "Batch [30/313], Loss: 1.8349\n",
      "Batch [60/313], Loss: 1.4924\n",
      "Batch [90/313], Loss: 1.5768\n",
      "Batch [120/313], Loss: 1.6073\n",
      "Batch [150/313], Loss: 1.6358\n",
      "Batch [180/313], Loss: 1.4523\n",
      "Batch [210/313], Loss: 1.7092\n",
      "Batch [240/313], Loss: 1.7285\n",
      "Batch [270/313], Loss: 1.7414\n",
      "Batch [300/313], Loss: 2.0574\n",
      "Epoch [15/20], Loss: 1.6310\n",
      "Validation loss decreased (1.648874 --> 1.630999).  Saving model ...\n",
      "Batch [30/313], Loss: 1.6845\n",
      "Batch [60/313], Loss: 1.5716\n",
      "Batch [90/313], Loss: 1.5887\n",
      "Batch [120/313], Loss: 1.6374\n",
      "Batch [150/313], Loss: 1.3331\n",
      "Batch [180/313], Loss: 1.6213\n",
      "Batch [210/313], Loss: 1.7923\n",
      "Batch [240/313], Loss: 1.5495\n",
      "Batch [270/313], Loss: 1.8331\n",
      "Batch [300/313], Loss: 1.5499\n",
      "Epoch [16/20], Loss: 1.6082\n",
      "Validation loss decreased (1.630999 --> 1.608212).  Saving model ...\n",
      "Batch [30/313], Loss: 1.3239\n",
      "Batch [60/313], Loss: 1.2003\n",
      "Batch [90/313], Loss: 1.3686\n",
      "Batch [120/313], Loss: 1.5136\n",
      "Batch [150/313], Loss: 1.3665\n",
      "Batch [180/313], Loss: 1.5627\n",
      "Batch [210/313], Loss: 1.4258\n",
      "Batch [240/313], Loss: 1.4315\n",
      "Batch [270/313], Loss: 1.7846\n",
      "Batch [300/313], Loss: 1.4527\n",
      "Epoch [17/20], Loss: 1.5860\n",
      "Validation loss decreased (1.608212 --> 1.586029).  Saving model ...\n",
      "Batch [30/313], Loss: 1.3915\n",
      "Batch [60/313], Loss: 1.4919\n",
      "Batch [90/313], Loss: 1.4963\n",
      "Batch [120/313], Loss: 1.4812\n",
      "Batch [150/313], Loss: 1.5962\n",
      "Batch [180/313], Loss: 1.5651\n",
      "Batch [210/313], Loss: 1.6584\n",
      "Batch [240/313], Loss: 1.7114\n",
      "Batch [270/313], Loss: 1.6145\n",
      "Batch [300/313], Loss: 1.5735\n",
      "Epoch [18/20], Loss: 1.5649\n",
      "Validation loss decreased (1.586029 --> 1.564871).  Saving model ...\n",
      "Batch [30/313], Loss: 1.4254\n",
      "Batch [60/313], Loss: 1.5744\n",
      "Batch [90/313], Loss: 1.5718\n",
      "Batch [120/313], Loss: 1.5609\n",
      "Batch [150/313], Loss: 1.6452\n",
      "Batch [180/313], Loss: 1.4503\n",
      "Batch [210/313], Loss: 1.4782\n",
      "Batch [240/313], Loss: 1.5835\n",
      "Batch [270/313], Loss: 1.6404\n",
      "Batch [300/313], Loss: 1.6324\n",
      "Epoch [19/20], Loss: 1.5455\n",
      "Validation loss decreased (1.564871 --> 1.545456).  Saving model ...\n",
      "Batch [30/313], Loss: 1.4557\n",
      "Batch [60/313], Loss: 1.4165\n",
      "Batch [90/313], Loss: 1.4910\n",
      "Batch [120/313], Loss: 1.5130\n",
      "Batch [150/313], Loss: 1.3466\n",
      "Batch [180/313], Loss: 1.3663\n",
      "Batch [210/313], Loss: 1.6797\n",
      "Batch [240/313], Loss: 1.6326\n",
      "Batch [270/313], Loss: 1.6531\n",
      "Batch [300/313], Loss: 1.5746\n",
      "Epoch [20/20], Loss: 1.5258\n",
      "Validation loss decreased (1.545456 --> 1.525820).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████▌                              | 4/20 [1:44:54<7:07:41, 1603.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20]   Loss: 2.5986   Accuracy: 38.74%\n",
      "Batch [30/313], Loss: 1.4687\n",
      "Batch [60/313], Loss: 1.7680\n",
      "Batch [90/313], Loss: 1.5605\n",
      "Batch [120/313], Loss: 1.5715\n",
      "Batch [150/313], Loss: 1.6598\n",
      "Batch [180/313], Loss: 1.6121\n",
      "Batch [210/313], Loss: 1.4475\n",
      "Batch [240/313], Loss: 1.3664\n",
      "Batch [270/313], Loss: 1.5479\n",
      "Batch [300/313], Loss: 1.6668\n",
      "Epoch [1/20], Loss: 1.5057\n",
      "Validation loss decreased (1.525820 --> 1.505737).  Saving model ...\n",
      "Batch [30/313], Loss: 1.5958\n",
      "Batch [60/313], Loss: 1.3470\n",
      "Batch [90/313], Loss: 1.6995\n",
      "Batch [120/313], Loss: 1.4267\n",
      "Batch [150/313], Loss: 1.4227\n",
      "Batch [180/313], Loss: 1.7008\n",
      "Batch [210/313], Loss: 1.5133\n",
      "Batch [240/313], Loss: 1.5950\n",
      "Batch [270/313], Loss: 1.7831\n",
      "Batch [300/313], Loss: 1.5655\n",
      "Epoch [2/20], Loss: 1.4786\n",
      "Validation loss decreased (1.505737 --> 1.478598).  Saving model ...\n",
      "Batch [30/313], Loss: 1.3382\n",
      "Batch [60/313], Loss: 1.5275\n",
      "Batch [90/313], Loss: 1.2229\n",
      "Batch [120/313], Loss: 1.3069\n",
      "Batch [150/313], Loss: 1.5758\n",
      "Batch [180/313], Loss: 1.6222\n",
      "Batch [210/313], Loss: 1.3931\n",
      "Batch [240/313], Loss: 1.4726\n",
      "Batch [270/313], Loss: 1.8294\n",
      "Batch [300/313], Loss: 1.9035\n",
      "Epoch [3/20], Loss: 1.4484\n",
      "Validation loss decreased (1.478598 --> 1.448416).  Saving model ...\n",
      "Batch [30/313], Loss: 1.2446\n",
      "Batch [60/313], Loss: 1.2643\n",
      "Batch [90/313], Loss: 1.4612\n",
      "Batch [120/313], Loss: 1.4396\n",
      "Batch [150/313], Loss: 1.5044\n",
      "Batch [180/313], Loss: 1.4043\n",
      "Batch [210/313], Loss: 1.6367\n",
      "Batch [240/313], Loss: 1.4172\n",
      "Batch [270/313], Loss: 1.5266\n",
      "Batch [300/313], Loss: 1.2853\n",
      "Epoch [4/20], Loss: 1.4339\n",
      "Validation loss decreased (1.448416 --> 1.433873).  Saving model ...\n",
      "Batch [30/313], Loss: 1.2394\n",
      "Batch [60/313], Loss: 1.5223\n",
      "Batch [90/313], Loss: 1.4650\n",
      "Batch [120/313], Loss: 1.3810\n",
      "Batch [150/313], Loss: 1.2842\n",
      "Batch [180/313], Loss: 1.4327\n",
      "Batch [210/313], Loss: 1.2901\n",
      "Batch [240/313], Loss: 1.4106\n",
      "Batch [270/313], Loss: 1.2722\n",
      "Batch [300/313], Loss: 1.4383\n",
      "Epoch [5/20], Loss: 1.3977\n",
      "Validation loss decreased (1.433873 --> 1.397742).  Saving model ...\n",
      "Batch [30/313], Loss: 1.2706\n",
      "Batch [60/313], Loss: 1.4116\n",
      "Batch [90/313], Loss: 1.2003\n",
      "Batch [120/313], Loss: 1.2999\n",
      "Batch [150/313], Loss: 1.4274\n",
      "Batch [180/313], Loss: 1.2941\n",
      "Batch [210/313], Loss: 1.4792\n",
      "Batch [240/313], Loss: 1.3158\n",
      "Batch [270/313], Loss: 1.2096\n",
      "Batch [300/313], Loss: 1.3678\n",
      "Epoch [6/20], Loss: 1.3876\n",
      "Validation loss decreased (1.397742 --> 1.387563).  Saving model ...\n",
      "Batch [30/313], Loss: 1.5349\n",
      "Batch [60/313], Loss: 1.0883\n",
      "Batch [90/313], Loss: 1.2877\n",
      "Batch [120/313], Loss: 1.6152\n",
      "Batch [150/313], Loss: 1.3084\n",
      "Batch [180/313], Loss: 1.3954\n",
      "Batch [210/313], Loss: 1.2755\n",
      "Batch [240/313], Loss: 1.5320\n",
      "Batch [270/313], Loss: 1.3813\n",
      "Batch [300/313], Loss: 1.5126\n",
      "Epoch [7/20], Loss: 1.3623\n",
      "Validation loss decreased (1.387563 --> 1.362267).  Saving model ...\n",
      "Batch [30/313], Loss: 1.2954\n",
      "Batch [60/313], Loss: 1.1434\n",
      "Batch [90/313], Loss: 1.3686\n",
      "Batch [120/313], Loss: 1.4111\n",
      "Batch [150/313], Loss: 1.5356\n",
      "Batch [180/313], Loss: 1.5277\n",
      "Batch [210/313], Loss: 1.4307\n",
      "Batch [240/313], Loss: 1.3008\n",
      "Batch [270/313], Loss: 1.1527\n",
      "Batch [300/313], Loss: 1.5572\n",
      "Epoch [8/20], Loss: 1.3450\n",
      "Validation loss decreased (1.362267 --> 1.344998).  Saving model ...\n",
      "Batch [30/313], Loss: 1.1321\n",
      "Batch [60/313], Loss: 1.2150\n",
      "Batch [90/313], Loss: 1.1581\n",
      "Batch [120/313], Loss: 1.6424\n",
      "Batch [150/313], Loss: 1.2420\n",
      "Batch [180/313], Loss: 1.4350\n",
      "Batch [210/313], Loss: 1.1483\n",
      "Batch [240/313], Loss: 1.5495\n",
      "Batch [270/313], Loss: 1.6756\n",
      "Batch [300/313], Loss: 1.3318\n",
      "Epoch [9/20], Loss: 1.3303\n",
      "Validation loss decreased (1.344998 --> 1.330330).  Saving model ...\n",
      "Batch [30/313], Loss: 1.1837\n",
      "Batch [60/313], Loss: 1.1637\n",
      "Batch [90/313], Loss: 1.2619\n",
      "Batch [120/313], Loss: 1.2136\n",
      "Batch [150/313], Loss: 1.1229\n",
      "Batch [180/313], Loss: 1.3688\n",
      "Batch [210/313], Loss: 1.3084\n",
      "Batch [240/313], Loss: 1.3688\n",
      "Batch [270/313], Loss: 1.3445\n",
      "Batch [300/313], Loss: 1.3410\n",
      "Epoch [10/20], Loss: 1.2906\n",
      "Validation loss decreased (1.330330 --> 1.290646).  Saving model ...\n",
      "Batch [30/313], Loss: 1.4105\n",
      "Batch [60/313], Loss: 1.4125\n",
      "Batch [90/313], Loss: 1.3592\n",
      "Batch [120/313], Loss: 1.3186\n",
      "Batch [150/313], Loss: 1.2604\n",
      "Batch [180/313], Loss: 1.2272\n",
      "Batch [210/313], Loss: 1.1038\n",
      "Batch [240/313], Loss: 1.6958\n",
      "Batch [270/313], Loss: 1.2070\n",
      "Batch [300/313], Loss: 1.3568\n",
      "Epoch [11/20], Loss: 1.2928\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 1.3489\n",
      "Batch [60/313], Loss: 1.1885\n",
      "Batch [90/313], Loss: 1.2151\n",
      "Batch [120/313], Loss: 1.0847\n",
      "Batch [150/313], Loss: 1.3435\n",
      "Batch [180/313], Loss: 1.3245\n",
      "Batch [210/313], Loss: 1.3203\n",
      "Batch [240/313], Loss: 1.1616\n",
      "Batch [270/313], Loss: 1.3055\n",
      "Batch [300/313], Loss: 1.3031\n",
      "Epoch [12/20], Loss: 1.2677\n",
      "Validation loss decreased (1.290646 --> 1.267653).  Saving model ...\n",
      "Batch [30/313], Loss: 1.3021\n",
      "Batch [60/313], Loss: 1.1943\n",
      "Batch [90/313], Loss: 1.1722\n",
      "Batch [120/313], Loss: 1.1855\n",
      "Batch [150/313], Loss: 1.2267\n",
      "Batch [180/313], Loss: 1.0434\n",
      "Batch [210/313], Loss: 1.3175\n",
      "Batch [240/313], Loss: 1.1926\n",
      "Batch [270/313], Loss: 1.3559\n",
      "Batch [300/313], Loss: 1.0290\n",
      "Epoch [13/20], Loss: 1.2494\n",
      "Validation loss decreased (1.267653 --> 1.249374).  Saving model ...\n",
      "Batch [30/313], Loss: 1.0050\n",
      "Batch [60/313], Loss: 1.0875\n",
      "Batch [90/313], Loss: 1.1367\n",
      "Batch [120/313], Loss: 1.3124\n",
      "Batch [150/313], Loss: 1.2898\n",
      "Batch [180/313], Loss: 1.2699\n",
      "Batch [210/313], Loss: 1.3725\n",
      "Batch [240/313], Loss: 1.2404\n",
      "Batch [270/313], Loss: 1.3005\n",
      "Batch [300/313], Loss: 1.3387\n",
      "Epoch [14/20], Loss: 1.2313\n",
      "Validation loss decreased (1.249374 --> 1.231317).  Saving model ...\n",
      "Batch [30/313], Loss: 1.0541\n",
      "Batch [60/313], Loss: 0.9671\n",
      "Batch [90/313], Loss: 1.2722\n",
      "Batch [120/313], Loss: 1.2301\n",
      "Batch [150/313], Loss: 1.2720\n",
      "Batch [180/313], Loss: 1.3738\n",
      "Batch [210/313], Loss: 1.0915\n",
      "Batch [240/313], Loss: 1.1281\n",
      "Batch [270/313], Loss: 1.2951\n",
      "Batch [300/313], Loss: 0.9309\n",
      "Epoch [15/20], Loss: 1.1991\n",
      "Validation loss decreased (1.231317 --> 1.199080).  Saving model ...\n",
      "Batch [30/313], Loss: 1.0923\n",
      "Batch [60/313], Loss: 0.9875\n",
      "Batch [90/313], Loss: 1.1584\n",
      "Batch [120/313], Loss: 1.0024\n",
      "Batch [150/313], Loss: 1.1723\n",
      "Batch [180/313], Loss: 1.2543\n",
      "Batch [210/313], Loss: 1.2317\n",
      "Batch [240/313], Loss: 1.3464\n",
      "Batch [270/313], Loss: 1.0941\n",
      "Batch [300/313], Loss: 1.3141\n",
      "Epoch [16/20], Loss: 1.1847\n",
      "Validation loss decreased (1.199080 --> 1.184664).  Saving model ...\n",
      "Batch [30/313], Loss: 1.1537\n",
      "Batch [60/313], Loss: 1.4232\n",
      "Batch [90/313], Loss: 1.2507\n",
      "Batch [120/313], Loss: 1.0597\n",
      "Batch [150/313], Loss: 1.5023\n",
      "Batch [180/313], Loss: 1.1560\n",
      "Batch [210/313], Loss: 1.3239\n",
      "Batch [240/313], Loss: 1.3746\n",
      "Batch [270/313], Loss: 1.0391\n",
      "Batch [300/313], Loss: 1.1590\n",
      "Epoch [17/20], Loss: 1.1538\n",
      "Validation loss decreased (1.184664 --> 1.153810).  Saving model ...\n",
      "Batch [30/313], Loss: 1.0200\n",
      "Batch [60/313], Loss: 1.1089\n",
      "Batch [90/313], Loss: 1.0699\n",
      "Batch [120/313], Loss: 1.1057\n",
      "Batch [150/313], Loss: 1.1120\n",
      "Batch [180/313], Loss: 1.1475\n",
      "Batch [210/313], Loss: 1.1234\n",
      "Batch [240/313], Loss: 1.1451\n",
      "Batch [270/313], Loss: 1.2481\n",
      "Batch [300/313], Loss: 1.0690\n",
      "Epoch [18/20], Loss: 1.1276\n",
      "Validation loss decreased (1.153810 --> 1.127614).  Saving model ...\n",
      "Batch [30/313], Loss: 0.9536\n",
      "Batch [60/313], Loss: 1.1123\n",
      "Batch [90/313], Loss: 0.8968\n",
      "Batch [120/313], Loss: 1.0834\n",
      "Batch [150/313], Loss: 0.9128\n",
      "Batch [180/313], Loss: 1.1912\n",
      "Batch [210/313], Loss: 1.0057\n",
      "Batch [240/313], Loss: 1.1012\n",
      "Batch [270/313], Loss: 1.3128\n",
      "Batch [300/313], Loss: 1.0633\n",
      "Epoch [19/20], Loss: 1.1184\n",
      "Validation loss decreased (1.127614 --> 1.118370).  Saving model ...\n",
      "Batch [30/313], Loss: 1.1268\n",
      "Batch [60/313], Loss: 1.1486\n",
      "Batch [90/313], Loss: 0.8661\n",
      "Batch [120/313], Loss: 1.1652\n",
      "Batch [150/313], Loss: 1.1105\n",
      "Batch [180/313], Loss: 0.9520\n",
      "Batch [210/313], Loss: 0.9426\n",
      "Batch [240/313], Loss: 0.9859\n",
      "Batch [270/313], Loss: 1.3850\n",
      "Batch [300/313], Loss: 1.0809\n",
      "Epoch [20/20], Loss: 1.0929\n",
      "Validation loss decreased (1.118370 --> 1.092912).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|█████████▌                            | 5/20 [2:12:21<6:44:52, 1619.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20]   Loss: 2.8221   Accuracy: 38.37%\n",
      "Batch [30/313], Loss: 1.1171\n",
      "Batch [60/313], Loss: 0.9326\n",
      "Batch [90/313], Loss: 1.0395\n",
      "Batch [120/313], Loss: 1.1046\n",
      "Batch [150/313], Loss: 0.9822\n",
      "Batch [180/313], Loss: 0.9391\n",
      "Batch [210/313], Loss: 1.1698\n",
      "Batch [240/313], Loss: 0.9699\n",
      "Batch [270/313], Loss: 1.2291\n",
      "Batch [300/313], Loss: 1.4278\n",
      "Epoch [1/20], Loss: 1.0685\n",
      "Validation loss decreased (1.092912 --> 1.068479).  Saving model ...\n",
      "Batch [30/313], Loss: 0.8838\n",
      "Batch [60/313], Loss: 0.9821\n",
      "Batch [90/313], Loss: 0.9829\n",
      "Batch [120/313], Loss: 1.0970\n",
      "Batch [150/313], Loss: 1.1132\n",
      "Batch [180/313], Loss: 1.2587\n",
      "Batch [210/313], Loss: 1.1191\n",
      "Batch [240/313], Loss: 1.0472\n",
      "Batch [270/313], Loss: 0.9965\n",
      "Batch [300/313], Loss: 1.0418\n",
      "Epoch [2/20], Loss: 1.0459\n",
      "Validation loss decreased (1.068479 --> 1.045882).  Saving model ...\n",
      "Batch [30/313], Loss: 0.9949\n",
      "Batch [60/313], Loss: 1.0973\n",
      "Batch [90/313], Loss: 0.9012\n",
      "Batch [120/313], Loss: 1.1774\n",
      "Batch [150/313], Loss: 0.9089\n",
      "Batch [180/313], Loss: 0.9730\n",
      "Batch [210/313], Loss: 1.0915\n",
      "Batch [240/313], Loss: 0.8697\n",
      "Batch [270/313], Loss: 0.9659\n",
      "Batch [300/313], Loss: 1.1338\n",
      "Epoch [3/20], Loss: 1.0395\n",
      "Validation loss decreased (1.045882 --> 1.039455).  Saving model ...\n",
      "Batch [30/313], Loss: 1.0716\n",
      "Batch [60/313], Loss: 0.8714\n",
      "Batch [90/313], Loss: 1.0144\n",
      "Batch [120/313], Loss: 0.9395\n",
      "Batch [150/313], Loss: 1.0176\n",
      "Batch [180/313], Loss: 0.9561\n",
      "Batch [210/313], Loss: 1.0615\n",
      "Batch [240/313], Loss: 1.1592\n",
      "Batch [270/313], Loss: 0.9504\n",
      "Batch [300/313], Loss: 0.9866\n",
      "Epoch [4/20], Loss: 1.0079\n",
      "Validation loss decreased (1.039455 --> 1.007894).  Saving model ...\n",
      "Batch [30/313], Loss: 1.0287\n",
      "Batch [60/313], Loss: 0.9558\n",
      "Batch [90/313], Loss: 1.0957\n",
      "Batch [120/313], Loss: 1.0705\n",
      "Batch [150/313], Loss: 1.1296\n",
      "Batch [180/313], Loss: 0.9562\n",
      "Batch [210/313], Loss: 1.0041\n",
      "Batch [240/313], Loss: 0.8994\n",
      "Batch [270/313], Loss: 0.8846\n",
      "Batch [300/313], Loss: 1.0436\n",
      "Epoch [5/20], Loss: 1.0011\n",
      "Validation loss decreased (1.007894 --> 1.001096).  Saving model ...\n",
      "Batch [30/313], Loss: 0.7997\n",
      "Batch [60/313], Loss: 1.0917\n",
      "Batch [90/313], Loss: 0.8837\n",
      "Batch [120/313], Loss: 0.9076\n",
      "Batch [150/313], Loss: 0.9761\n",
      "Batch [180/313], Loss: 0.8669\n",
      "Batch [210/313], Loss: 1.0049\n",
      "Batch [240/313], Loss: 1.0522\n",
      "Batch [270/313], Loss: 0.8936\n",
      "Batch [300/313], Loss: 1.1528\n",
      "Epoch [6/20], Loss: 0.9746\n",
      "Validation loss decreased (1.001096 --> 0.974607).  Saving model ...\n",
      "Batch [30/313], Loss: 0.7628\n",
      "Batch [60/313], Loss: 0.7518\n",
      "Batch [90/313], Loss: 0.8554\n",
      "Batch [120/313], Loss: 0.8758\n",
      "Batch [150/313], Loss: 0.7807\n",
      "Batch [180/313], Loss: 0.9609\n",
      "Batch [210/313], Loss: 1.0691\n",
      "Batch [240/313], Loss: 0.9089\n",
      "Batch [270/313], Loss: 1.1500\n",
      "Batch [300/313], Loss: 0.9639\n",
      "Epoch [7/20], Loss: 0.9595\n",
      "Validation loss decreased (0.974607 --> 0.959536).  Saving model ...\n",
      "Batch [30/313], Loss: 0.8817\n",
      "Batch [60/313], Loss: 0.8173\n",
      "Batch [90/313], Loss: 0.9541\n",
      "Batch [120/313], Loss: 1.1168\n",
      "Batch [150/313], Loss: 0.9907\n",
      "Batch [180/313], Loss: 1.0604\n",
      "Batch [210/313], Loss: 1.0626\n",
      "Batch [240/313], Loss: 1.0358\n",
      "Batch [270/313], Loss: 1.0143\n",
      "Batch [300/313], Loss: 1.0067\n",
      "Epoch [8/20], Loss: 0.9553\n",
      "Validation loss decreased (0.959536 --> 0.955309).  Saving model ...\n",
      "Batch [30/313], Loss: 0.8674\n",
      "Batch [60/313], Loss: 1.0342\n",
      "Batch [90/313], Loss: 0.8010\n",
      "Batch [120/313], Loss: 0.9430\n",
      "Batch [150/313], Loss: 0.9532\n",
      "Batch [180/313], Loss: 0.8112\n",
      "Batch [210/313], Loss: 0.9429\n",
      "Batch [240/313], Loss: 0.8888\n",
      "Batch [270/313], Loss: 0.9175\n",
      "Batch [300/313], Loss: 0.9126\n",
      "Epoch [9/20], Loss: 0.9296\n",
      "Validation loss decreased (0.955309 --> 0.929560).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5756\n",
      "Batch [60/313], Loss: 0.8451\n",
      "Batch [90/313], Loss: 0.8912\n",
      "Batch [120/313], Loss: 0.9085\n",
      "Batch [150/313], Loss: 1.1256\n",
      "Batch [180/313], Loss: 0.8021\n",
      "Batch [210/313], Loss: 0.9556\n",
      "Batch [240/313], Loss: 0.9588\n",
      "Batch [270/313], Loss: 1.0269\n",
      "Batch [300/313], Loss: 0.9493\n",
      "Epoch [10/20], Loss: 0.9046\n",
      "Validation loss decreased (0.929560 --> 0.904622).  Saving model ...\n",
      "Batch [30/313], Loss: 0.8856\n",
      "Batch [60/313], Loss: 0.7998\n",
      "Batch [90/313], Loss: 0.9278\n",
      "Batch [120/313], Loss: 0.9576\n",
      "Batch [150/313], Loss: 0.9045\n",
      "Batch [180/313], Loss: 0.8861\n",
      "Batch [210/313], Loss: 0.7765\n",
      "Batch [240/313], Loss: 0.8964\n",
      "Batch [270/313], Loss: 0.9244\n",
      "Batch [300/313], Loss: 0.9753\n",
      "Epoch [11/20], Loss: 0.8926\n",
      "Validation loss decreased (0.904622 --> 0.892605).  Saving model ...\n",
      "Batch [30/313], Loss: 0.6712\n",
      "Batch [60/313], Loss: 1.1410\n",
      "Batch [90/313], Loss: 0.7149\n",
      "Batch [120/313], Loss: 0.9250\n",
      "Batch [150/313], Loss: 1.2254\n",
      "Batch [180/313], Loss: 0.6255\n",
      "Batch [210/313], Loss: 0.7529\n",
      "Batch [240/313], Loss: 0.8473\n",
      "Batch [270/313], Loss: 0.8863\n",
      "Batch [300/313], Loss: 0.9025\n",
      "Epoch [12/20], Loss: 0.8758\n",
      "Validation loss decreased (0.892605 --> 0.875830).  Saving model ...\n",
      "Batch [30/313], Loss: 1.0032\n",
      "Batch [60/313], Loss: 0.8606\n",
      "Batch [90/313], Loss: 0.9467\n",
      "Batch [120/313], Loss: 0.8555\n",
      "Batch [150/313], Loss: 0.8067\n",
      "Batch [180/313], Loss: 0.6683\n",
      "Batch [210/313], Loss: 0.8371\n",
      "Batch [240/313], Loss: 0.7591\n",
      "Batch [270/313], Loss: 0.8896\n",
      "Batch [300/313], Loss: 0.9441\n",
      "Epoch [13/20], Loss: 0.8656\n",
      "Validation loss decreased (0.875830 --> 0.865603).  Saving model ...\n",
      "Batch [30/313], Loss: 0.6158\n",
      "Batch [60/313], Loss: 0.9212\n",
      "Batch [90/313], Loss: 0.8029\n",
      "Batch [120/313], Loss: 0.8744\n",
      "Batch [150/313], Loss: 0.9509\n",
      "Batch [180/313], Loss: 0.8416\n",
      "Batch [210/313], Loss: 0.9236\n",
      "Batch [240/313], Loss: 0.9721\n",
      "Batch [270/313], Loss: 0.9664\n",
      "Batch [300/313], Loss: 0.7758\n",
      "Epoch [14/20], Loss: 0.8532\n",
      "Validation loss decreased (0.865603 --> 0.853227).  Saving model ...\n",
      "Batch [30/313], Loss: 0.6515\n",
      "Batch [60/313], Loss: 0.6622\n",
      "Batch [90/313], Loss: 0.7323\n",
      "Batch [120/313], Loss: 1.0052\n",
      "Batch [150/313], Loss: 0.8861\n",
      "Batch [180/313], Loss: 0.7832\n",
      "Batch [210/313], Loss: 0.9157\n",
      "Batch [240/313], Loss: 0.8829\n",
      "Batch [270/313], Loss: 0.7362\n",
      "Batch [300/313], Loss: 0.7149\n",
      "Epoch [15/20], Loss: 0.8160\n",
      "Validation loss decreased (0.853227 --> 0.816042).  Saving model ...\n",
      "Batch [30/313], Loss: 0.8674\n",
      "Batch [60/313], Loss: 0.7808\n",
      "Batch [90/313], Loss: 0.8927\n",
      "Batch [120/313], Loss: 0.6529\n",
      "Batch [150/313], Loss: 0.8166\n",
      "Batch [180/313], Loss: 0.8006\n",
      "Batch [210/313], Loss: 0.8889\n",
      "Batch [240/313], Loss: 0.9704\n",
      "Batch [270/313], Loss: 0.9144\n",
      "Batch [300/313], Loss: 0.7899\n",
      "Epoch [16/20], Loss: 0.8025\n",
      "Validation loss decreased (0.816042 --> 0.802513).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5782\n",
      "Batch [60/313], Loss: 0.8023\n",
      "Batch [90/313], Loss: 0.6833\n",
      "Batch [120/313], Loss: 0.7400\n",
      "Batch [150/313], Loss: 0.7323\n",
      "Batch [180/313], Loss: 0.6341\n",
      "Batch [210/313], Loss: 0.7633\n",
      "Batch [240/313], Loss: 0.8841\n",
      "Batch [270/313], Loss: 0.8333\n",
      "Batch [300/313], Loss: 0.9389\n",
      "Epoch [17/20], Loss: 0.8029\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.6678\n",
      "Batch [60/313], Loss: 0.6739\n",
      "Batch [90/313], Loss: 0.7348\n",
      "Batch [120/313], Loss: 0.6439\n",
      "Batch [150/313], Loss: 0.7411\n",
      "Batch [180/313], Loss: 0.6828\n",
      "Batch [210/313], Loss: 0.7066\n",
      "Batch [240/313], Loss: 0.8517\n",
      "Batch [270/313], Loss: 0.9484\n",
      "Batch [300/313], Loss: 0.9291\n",
      "Epoch [18/20], Loss: 0.7865\n",
      "Validation loss decreased (0.802513 --> 0.786492).  Saving model ...\n",
      "Batch [30/313], Loss: 0.6944\n",
      "Batch [60/313], Loss: 0.8546\n",
      "Batch [90/313], Loss: 0.8822\n",
      "Batch [120/313], Loss: 0.6985\n",
      "Batch [150/313], Loss: 0.6337\n",
      "Batch [180/313], Loss: 0.9009\n",
      "Batch [210/313], Loss: 0.9010\n",
      "Batch [240/313], Loss: 0.7724\n",
      "Batch [270/313], Loss: 0.8867\n",
      "Batch [300/313], Loss: 0.7954\n",
      "Epoch [19/20], Loss: 0.7591\n",
      "Validation loss decreased (0.786492 --> 0.759102).  Saving model ...\n",
      "Batch [30/313], Loss: 0.7829\n",
      "Batch [60/313], Loss: 0.8165\n",
      "Batch [90/313], Loss: 0.7598\n",
      "Batch [120/313], Loss: 0.8253\n",
      "Batch [150/313], Loss: 0.8469\n",
      "Batch [180/313], Loss: 0.6608\n",
      "Batch [210/313], Loss: 0.7587\n",
      "Batch [240/313], Loss: 0.7590\n",
      "Batch [270/313], Loss: 0.7441\n",
      "Batch [300/313], Loss: 1.0578\n",
      "Epoch [20/20], Loss: 0.7480\n",
      "Validation loss decreased (0.759102 --> 0.747992).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████▍                          | 6/20 [2:39:43<6:19:40, 1627.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20]   Loss: 3.0497   Accuracy: 38.36%\n",
      "Batch [30/313], Loss: 0.7334\n",
      "Batch [60/313], Loss: 0.6480\n",
      "Batch [90/313], Loss: 0.6982\n",
      "Batch [120/313], Loss: 0.6827\n",
      "Batch [150/313], Loss: 0.7052\n",
      "Batch [180/313], Loss: 0.7789\n",
      "Batch [210/313], Loss: 0.6161\n",
      "Batch [240/313], Loss: 0.8150\n",
      "Batch [270/313], Loss: 0.6413\n",
      "Batch [300/313], Loss: 0.7497\n",
      "Epoch [1/20], Loss: 0.7375\n",
      "Validation loss decreased (0.747992 --> 0.737529).  Saving model ...\n",
      "Batch [30/313], Loss: 0.6344\n",
      "Batch [60/313], Loss: 0.6364\n",
      "Batch [90/313], Loss: 0.5545\n",
      "Batch [120/313], Loss: 0.6557\n",
      "Batch [150/313], Loss: 0.6687\n",
      "Batch [180/313], Loss: 0.6918\n",
      "Batch [210/313], Loss: 0.6982\n",
      "Batch [240/313], Loss: 0.6202\n",
      "Batch [270/313], Loss: 0.7817\n",
      "Batch [300/313], Loss: 0.8140\n",
      "Epoch [2/20], Loss: 0.7231\n",
      "Validation loss decreased (0.737529 --> 0.723133).  Saving model ...\n",
      "Batch [30/313], Loss: 0.7692\n",
      "Batch [60/313], Loss: 0.7757\n",
      "Batch [90/313], Loss: 0.7295\n",
      "Batch [120/313], Loss: 0.6876\n",
      "Batch [150/313], Loss: 0.5996\n",
      "Batch [180/313], Loss: 0.6968\n",
      "Batch [210/313], Loss: 0.8094\n",
      "Batch [240/313], Loss: 0.6213\n",
      "Batch [270/313], Loss: 0.7189\n",
      "Batch [300/313], Loss: 0.8006\n",
      "Epoch [3/20], Loss: 0.7123\n",
      "Validation loss decreased (0.723133 --> 0.712291).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5719\n",
      "Batch [60/313], Loss: 0.9216\n",
      "Batch [90/313], Loss: 0.6883\n",
      "Batch [120/313], Loss: 0.6563\n",
      "Batch [150/313], Loss: 0.6448\n",
      "Batch [180/313], Loss: 0.5372\n",
      "Batch [210/313], Loss: 0.6505\n",
      "Batch [240/313], Loss: 0.6863\n",
      "Batch [270/313], Loss: 0.6461\n",
      "Batch [300/313], Loss: 0.6404\n",
      "Epoch [4/20], Loss: 0.6946\n",
      "Validation loss decreased (0.712291 --> 0.694579).  Saving model ...\n",
      "Batch [30/313], Loss: 0.6547\n",
      "Batch [60/313], Loss: 0.5908\n",
      "Batch [90/313], Loss: 0.5650\n",
      "Batch [120/313], Loss: 0.7225\n",
      "Batch [150/313], Loss: 0.9353\n",
      "Batch [180/313], Loss: 0.7009\n",
      "Batch [210/313], Loss: 0.8511\n",
      "Batch [240/313], Loss: 0.7568\n",
      "Batch [270/313], Loss: 0.5315\n",
      "Batch [300/313], Loss: 0.7069\n",
      "Epoch [5/20], Loss: 0.6810\n",
      "Validation loss decreased (0.694579 --> 0.680963).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5206\n",
      "Batch [60/313], Loss: 0.7049\n",
      "Batch [90/313], Loss: 0.8270\n",
      "Batch [120/313], Loss: 0.7299\n",
      "Batch [150/313], Loss: 0.6128\n",
      "Batch [180/313], Loss: 0.5282\n",
      "Batch [210/313], Loss: 0.6356\n",
      "Batch [240/313], Loss: 0.7155\n",
      "Batch [270/313], Loss: 0.6759\n",
      "Batch [300/313], Loss: 0.7597\n",
      "Epoch [6/20], Loss: 0.6776\n",
      "Validation loss decreased (0.680963 --> 0.677563).  Saving model ...\n",
      "Batch [30/313], Loss: 0.7356\n",
      "Batch [60/313], Loss: 0.6258\n",
      "Batch [90/313], Loss: 0.8240\n",
      "Batch [120/313], Loss: 0.8175\n",
      "Batch [150/313], Loss: 0.7600\n",
      "Batch [180/313], Loss: 0.6191\n",
      "Batch [210/313], Loss: 0.7192\n",
      "Batch [240/313], Loss: 0.6126\n",
      "Batch [270/313], Loss: 0.6874\n",
      "Batch [300/313], Loss: 0.8479\n",
      "Epoch [7/20], Loss: 0.6582\n",
      "Validation loss decreased (0.677563 --> 0.658158).  Saving model ...\n",
      "Batch [30/313], Loss: 0.4960\n",
      "Batch [60/313], Loss: 0.6195\n",
      "Batch [90/313], Loss: 0.6133\n",
      "Batch [120/313], Loss: 0.6347\n",
      "Batch [150/313], Loss: 0.6683\n",
      "Batch [180/313], Loss: 0.7326\n",
      "Batch [210/313], Loss: 0.7556\n",
      "Batch [240/313], Loss: 0.7324\n",
      "Batch [270/313], Loss: 0.8174\n",
      "Batch [300/313], Loss: 0.6950\n",
      "Epoch [8/20], Loss: 0.6491\n",
      "Validation loss decreased (0.658158 --> 0.649114).  Saving model ...\n",
      "Batch [30/313], Loss: 0.6302\n",
      "Batch [60/313], Loss: 0.5664\n",
      "Batch [90/313], Loss: 0.3980\n",
      "Batch [120/313], Loss: 0.7894\n",
      "Batch [150/313], Loss: 0.8048\n",
      "Batch [180/313], Loss: 0.7664\n",
      "Batch [210/313], Loss: 0.7417\n",
      "Batch [240/313], Loss: 0.5122\n",
      "Batch [270/313], Loss: 0.6721\n",
      "Batch [300/313], Loss: 0.6559\n",
      "Epoch [9/20], Loss: 0.6448\n",
      "Validation loss decreased (0.649114 --> 0.644815).  Saving model ...\n",
      "Batch [30/313], Loss: 0.6125\n",
      "Batch [60/313], Loss: 0.5489\n",
      "Batch [90/313], Loss: 0.7847\n",
      "Batch [120/313], Loss: 0.6620\n",
      "Batch [150/313], Loss: 0.6762\n",
      "Batch [180/313], Loss: 0.6420\n",
      "Batch [210/313], Loss: 0.7025\n",
      "Batch [240/313], Loss: 0.5869\n",
      "Batch [270/313], Loss: 0.6200\n",
      "Batch [300/313], Loss: 0.6868\n",
      "Epoch [10/20], Loss: 0.6218\n",
      "Validation loss decreased (0.644815 --> 0.621784).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5488\n",
      "Batch [60/313], Loss: 0.4958\n",
      "Batch [90/313], Loss: 0.5756\n",
      "Batch [120/313], Loss: 0.5659\n",
      "Batch [150/313], Loss: 0.6115\n",
      "Batch [180/313], Loss: 0.6823\n",
      "Batch [210/313], Loss: 0.6831\n",
      "Batch [240/313], Loss: 0.5534\n",
      "Batch [270/313], Loss: 0.5577\n",
      "Batch [300/313], Loss: 0.5795\n",
      "Epoch [11/20], Loss: 0.6139\n",
      "Validation loss decreased (0.621784 --> 0.613936).  Saving model ...\n",
      "Batch [30/313], Loss: 0.7820\n",
      "Batch [60/313], Loss: 0.5919\n",
      "Batch [90/313], Loss: 0.6542\n",
      "Batch [120/313], Loss: 0.6142\n",
      "Batch [150/313], Loss: 0.6656\n",
      "Batch [180/313], Loss: 0.5839\n",
      "Batch [210/313], Loss: 0.6103\n",
      "Batch [240/313], Loss: 0.7000\n",
      "Batch [270/313], Loss: 0.6656\n",
      "Batch [300/313], Loss: 0.5813\n",
      "Epoch [12/20], Loss: 0.6012\n",
      "Validation loss decreased (0.613936 --> 0.601217).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5865\n",
      "Batch [60/313], Loss: 0.5352\n",
      "Batch [90/313], Loss: 0.4823\n",
      "Batch [120/313], Loss: 0.5633\n",
      "Batch [150/313], Loss: 0.6435\n",
      "Batch [180/313], Loss: 0.5465\n",
      "Batch [210/313], Loss: 0.6283\n",
      "Batch [240/313], Loss: 0.5215\n",
      "Batch [270/313], Loss: 0.6090\n",
      "Batch [300/313], Loss: 0.6914\n",
      "Epoch [13/20], Loss: 0.5861\n",
      "Validation loss decreased (0.601217 --> 0.586108).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5797\n",
      "Batch [60/313], Loss: 0.4691\n",
      "Batch [90/313], Loss: 0.4882\n",
      "Batch [120/313], Loss: 0.7568\n",
      "Batch [150/313], Loss: 0.6450\n",
      "Batch [180/313], Loss: 0.6244\n",
      "Batch [210/313], Loss: 0.5379\n",
      "Batch [240/313], Loss: 0.5868\n",
      "Batch [270/313], Loss: 0.3618\n",
      "Batch [300/313], Loss: 0.6029\n",
      "Epoch [14/20], Loss: 0.5707\n",
      "Validation loss decreased (0.586108 --> 0.570663).  Saving model ...\n",
      "Batch [30/313], Loss: 0.6306\n",
      "Batch [60/313], Loss: 0.5956\n",
      "Batch [90/313], Loss: 0.5288\n",
      "Batch [120/313], Loss: 0.5958\n",
      "Batch [150/313], Loss: 0.6178\n",
      "Batch [180/313], Loss: 0.5398\n",
      "Batch [210/313], Loss: 0.5998\n",
      "Batch [240/313], Loss: 0.5369\n",
      "Batch [270/313], Loss: 0.6129\n",
      "Batch [300/313], Loss: 0.4771\n",
      "Epoch [15/20], Loss: 0.5719\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.4217\n",
      "Batch [60/313], Loss: 0.5481\n",
      "Batch [90/313], Loss: 0.6071\n",
      "Batch [120/313], Loss: 0.5598\n",
      "Batch [150/313], Loss: 0.5060\n",
      "Batch [180/313], Loss: 0.6991\n",
      "Batch [210/313], Loss: 0.5307\n",
      "Batch [240/313], Loss: 0.6256\n",
      "Batch [270/313], Loss: 0.5057\n",
      "Batch [300/313], Loss: 0.8072\n",
      "Epoch [16/20], Loss: 0.5601\n",
      "Validation loss decreased (0.570663 --> 0.560129).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5485\n",
      "Batch [60/313], Loss: 0.4857\n",
      "Batch [90/313], Loss: 0.4349\n",
      "Batch [120/313], Loss: 0.4776\n",
      "Batch [150/313], Loss: 0.5600\n",
      "Batch [180/313], Loss: 0.4321\n",
      "Batch [210/313], Loss: 0.3662\n",
      "Batch [240/313], Loss: 0.4017\n",
      "Batch [270/313], Loss: 0.5162\n",
      "Batch [300/313], Loss: 0.4350\n",
      "Epoch [17/20], Loss: 0.5410\n",
      "Validation loss decreased (0.560129 --> 0.540961).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5566\n",
      "Batch [60/313], Loss: 0.5366\n",
      "Batch [90/313], Loss: 0.3859\n",
      "Batch [120/313], Loss: 0.5039\n",
      "Batch [150/313], Loss: 0.4474\n",
      "Batch [180/313], Loss: 0.5212\n",
      "Batch [210/313], Loss: 0.6435\n",
      "Batch [240/313], Loss: 0.6640\n",
      "Batch [270/313], Loss: 0.5590\n",
      "Batch [300/313], Loss: 0.4391\n",
      "Epoch [18/20], Loss: 0.5459\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.3941\n",
      "Batch [60/313], Loss: 0.3985\n",
      "Batch [90/313], Loss: 0.4369\n",
      "Batch [120/313], Loss: 0.4931\n",
      "Batch [150/313], Loss: 0.6198\n",
      "Batch [180/313], Loss: 0.5750\n",
      "Batch [210/313], Loss: 0.6365\n",
      "Batch [240/313], Loss: 0.4521\n",
      "Batch [270/313], Loss: 0.5300\n",
      "Batch [300/313], Loss: 0.5215\n",
      "Epoch [19/20], Loss: 0.5320\n",
      "Validation loss decreased (0.540961 --> 0.531964).  Saving model ...\n",
      "Batch [30/313], Loss: 0.4108\n",
      "Batch [60/313], Loss: 0.4280\n",
      "Batch [90/313], Loss: 0.6470\n",
      "Batch [120/313], Loss: 0.4823\n",
      "Batch [150/313], Loss: 0.5156\n",
      "Batch [180/313], Loss: 0.5998\n",
      "Batch [210/313], Loss: 0.4140\n",
      "Batch [240/313], Loss: 0.4652\n",
      "Batch [270/313], Loss: 0.6317\n",
      "Batch [300/313], Loss: 0.5082\n",
      "Epoch [20/20], Loss: 0.5152\n",
      "Validation loss decreased (0.531964 --> 0.515204).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|█████████████▎                        | 7/20 [3:06:59<5:53:09, 1629.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20]   Loss: 3.2684   Accuracy: 38.34%\n",
      "Batch [30/313], Loss: 0.5243\n",
      "Batch [60/313], Loss: 0.3359\n",
      "Batch [90/313], Loss: 0.6163\n",
      "Batch [120/313], Loss: 0.5561\n",
      "Batch [150/313], Loss: 0.6413\n",
      "Batch [180/313], Loss: 0.5703\n",
      "Batch [210/313], Loss: 0.4572\n",
      "Batch [240/313], Loss: 0.5474\n",
      "Batch [270/313], Loss: 0.3445\n",
      "Batch [300/313], Loss: 0.8108\n",
      "Epoch [1/20], Loss: 0.5019\n",
      "Validation loss decreased (0.515204 --> 0.501902).  Saving model ...\n",
      "Batch [30/313], Loss: 0.4333\n",
      "Batch [60/313], Loss: 0.4054\n",
      "Batch [90/313], Loss: 0.6990\n",
      "Batch [120/313], Loss: 0.3248\n",
      "Batch [150/313], Loss: 0.7065\n",
      "Batch [180/313], Loss: 0.5708\n",
      "Batch [210/313], Loss: 0.4668\n",
      "Batch [240/313], Loss: 0.4546\n",
      "Batch [270/313], Loss: 0.7125\n",
      "Batch [300/313], Loss: 0.5010\n",
      "Epoch [2/20], Loss: 0.4913\n",
      "Validation loss decreased (0.501902 --> 0.491320).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5161\n",
      "Batch [60/313], Loss: 0.3317\n",
      "Batch [90/313], Loss: 0.4561\n",
      "Batch [120/313], Loss: 0.4982\n",
      "Batch [150/313], Loss: 0.4153\n",
      "Batch [180/313], Loss: 0.4409\n",
      "Batch [210/313], Loss: 0.5201\n",
      "Batch [240/313], Loss: 0.5054\n",
      "Batch [270/313], Loss: 0.7823\n",
      "Batch [300/313], Loss: 0.3169\n",
      "Epoch [3/20], Loss: 0.4892\n",
      "Validation loss decreased (0.491320 --> 0.489206).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5686\n",
      "Batch [60/313], Loss: 0.4750\n",
      "Batch [90/313], Loss: 0.5502\n",
      "Batch [120/313], Loss: 0.5032\n",
      "Batch [150/313], Loss: 0.4402\n",
      "Batch [180/313], Loss: 0.4243\n",
      "Batch [210/313], Loss: 0.3931\n",
      "Batch [240/313], Loss: 0.4595\n",
      "Batch [270/313], Loss: 0.4663\n",
      "Batch [300/313], Loss: 0.4211\n",
      "Epoch [4/20], Loss: 0.4809\n",
      "Validation loss decreased (0.489206 --> 0.480894).  Saving model ...\n",
      "Batch [30/313], Loss: 0.4843\n",
      "Batch [60/313], Loss: 0.4708\n",
      "Batch [90/313], Loss: 0.5212\n",
      "Batch [120/313], Loss: 0.6274\n",
      "Batch [150/313], Loss: 0.4819\n",
      "Batch [180/313], Loss: 0.3620\n",
      "Batch [210/313], Loss: 0.3481\n",
      "Batch [240/313], Loss: 0.3977\n",
      "Batch [270/313], Loss: 0.4792\n",
      "Batch [300/313], Loss: 0.5270\n",
      "Epoch [5/20], Loss: 0.4773\n",
      "Validation loss decreased (0.480894 --> 0.477291).  Saving model ...\n",
      "Batch [30/313], Loss: 0.3845\n",
      "Batch [60/313], Loss: 0.5727\n",
      "Batch [90/313], Loss: 0.3068\n",
      "Batch [120/313], Loss: 0.3684\n",
      "Batch [150/313], Loss: 0.5743\n",
      "Batch [180/313], Loss: 0.4206\n",
      "Batch [210/313], Loss: 0.5934\n",
      "Batch [240/313], Loss: 0.5758\n",
      "Batch [270/313], Loss: 0.3910\n",
      "Batch [300/313], Loss: 0.5207\n",
      "Epoch [6/20], Loss: 0.4632\n",
      "Validation loss decreased (0.477291 --> 0.463243).  Saving model ...\n",
      "Batch [30/313], Loss: 0.4703\n",
      "Batch [60/313], Loss: 0.5192\n",
      "Batch [90/313], Loss: 0.5060\n",
      "Batch [120/313], Loss: 0.5578\n",
      "Batch [150/313], Loss: 0.4772\n",
      "Batch [180/313], Loss: 0.3817\n",
      "Batch [210/313], Loss: 0.4553\n",
      "Batch [240/313], Loss: 0.4494\n",
      "Batch [270/313], Loss: 0.4640\n",
      "Batch [300/313], Loss: 0.5067\n",
      "Epoch [7/20], Loss: 0.4648\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.3940\n",
      "Batch [60/313], Loss: 0.4332\n",
      "Batch [90/313], Loss: 0.4504\n",
      "Batch [120/313], Loss: 0.3778\n",
      "Batch [150/313], Loss: 0.5098\n",
      "Batch [180/313], Loss: 0.4391\n",
      "Batch [210/313], Loss: 0.5390\n",
      "Batch [240/313], Loss: 0.4430\n",
      "Batch [270/313], Loss: 0.5139\n",
      "Batch [300/313], Loss: 0.5218\n",
      "Epoch [8/20], Loss: 0.4505\n",
      "Validation loss decreased (0.463243 --> 0.450473).  Saving model ...\n",
      "Batch [30/313], Loss: 0.3353\n",
      "Batch [60/313], Loss: 0.3882\n",
      "Batch [90/313], Loss: 0.3992\n",
      "Batch [120/313], Loss: 0.4744\n",
      "Batch [150/313], Loss: 0.5689\n",
      "Batch [180/313], Loss: 0.5634\n",
      "Batch [210/313], Loss: 0.3897\n",
      "Batch [240/313], Loss: 0.4408\n",
      "Batch [270/313], Loss: 0.4756\n",
      "Batch [300/313], Loss: 0.5064\n",
      "Epoch [9/20], Loss: 0.4361\n",
      "Validation loss decreased (0.450473 --> 0.436063).  Saving model ...\n",
      "Batch [30/313], Loss: 0.4958\n",
      "Batch [60/313], Loss: 0.3669\n",
      "Batch [90/313], Loss: 0.4253\n",
      "Batch [120/313], Loss: 0.4045\n",
      "Batch [150/313], Loss: 0.3344\n",
      "Batch [180/313], Loss: 0.4395\n",
      "Batch [210/313], Loss: 0.4556\n",
      "Batch [240/313], Loss: 0.5738\n",
      "Batch [270/313], Loss: 0.5981\n",
      "Batch [300/313], Loss: 0.5547\n",
      "Epoch [10/20], Loss: 0.4361\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.2348\n",
      "Batch [60/313], Loss: 0.4902\n",
      "Batch [90/313], Loss: 0.3024\n",
      "Batch [120/313], Loss: 0.4068\n",
      "Batch [150/313], Loss: 0.5325\n",
      "Batch [180/313], Loss: 0.4576\n",
      "Batch [210/313], Loss: 0.4388\n",
      "Batch [240/313], Loss: 0.5306\n",
      "Batch [270/313], Loss: 0.3204\n",
      "Batch [300/313], Loss: 0.5448\n",
      "Epoch [11/20], Loss: 0.4389\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Batch [30/313], Loss: 0.4845\n",
      "Batch [60/313], Loss: 0.2535\n",
      "Batch [90/313], Loss: 0.4277\n",
      "Batch [120/313], Loss: 0.3388\n",
      "Batch [150/313], Loss: 0.4283\n",
      "Batch [180/313], Loss: 0.4209\n",
      "Batch [210/313], Loss: 0.4663\n",
      "Batch [240/313], Loss: 0.3394\n",
      "Batch [270/313], Loss: 0.4815\n",
      "Batch [300/313], Loss: 0.3743\n",
      "Epoch [12/20], Loss: 0.4289\n",
      "Validation loss decreased (0.436063 --> 0.428915).  Saving model ...\n",
      "Batch [30/313], Loss: 0.4833\n",
      "Batch [60/313], Loss: 0.4744\n",
      "Batch [90/313], Loss: 0.3269\n",
      "Batch [120/313], Loss: 0.3846\n",
      "Batch [150/313], Loss: 0.4178\n",
      "Batch [180/313], Loss: 0.4428\n",
      "Batch [210/313], Loss: 0.5064\n",
      "Batch [240/313], Loss: 0.4020\n",
      "Batch [270/313], Loss: 0.5565\n",
      "Batch [300/313], Loss: 0.4842\n",
      "Epoch [13/20], Loss: 0.4110\n",
      "Validation loss decreased (0.428915 --> 0.411030).  Saving model ...\n",
      "Batch [30/313], Loss: 0.4492\n",
      "Batch [60/313], Loss: 0.2517\n",
      "Batch [90/313], Loss: 0.3593\n",
      "Batch [120/313], Loss: 0.4610\n",
      "Batch [150/313], Loss: 0.5606\n",
      "Batch [180/313], Loss: 0.3341\n",
      "Batch [210/313], Loss: 0.3644\n",
      "Batch [240/313], Loss: 0.4799\n",
      "Batch [270/313], Loss: 0.3407\n",
      "Batch [300/313], Loss: 0.3643\n",
      "Epoch [14/20], Loss: 0.4096\n",
      "Validation loss decreased (0.411030 --> 0.409647).  Saving model ...\n",
      "Batch [30/313], Loss: 0.3486\n",
      "Batch [60/313], Loss: 0.4240\n",
      "Batch [90/313], Loss: 0.3763\n",
      "Batch [120/313], Loss: 0.3494\n",
      "Batch [150/313], Loss: 0.4815\n",
      "Batch [180/313], Loss: 0.5467\n",
      "Batch [210/313], Loss: 0.5089\n",
      "Batch [240/313], Loss: 0.5651\n",
      "Batch [270/313], Loss: 0.4188\n",
      "Batch [300/313], Loss: 0.3815\n",
      "Epoch [15/20], Loss: 0.3961\n",
      "Validation loss decreased (0.409647 --> 0.396072).  Saving model ...\n",
      "Batch [30/313], Loss: 0.2872\n",
      "Batch [60/313], Loss: 0.2308\n",
      "Batch [90/313], Loss: 0.3882\n",
      "Batch [120/313], Loss: 0.3369\n",
      "Batch [150/313], Loss: 0.4802\n",
      "Batch [180/313], Loss: 0.5013\n",
      "Batch [210/313], Loss: 0.4398\n",
      "Batch [240/313], Loss: 0.3552\n",
      "Batch [270/313], Loss: 0.4083\n",
      "Batch [300/313], Loss: 0.3774\n",
      "Epoch [16/20], Loss: 0.3879\n",
      "Validation loss decreased (0.396072 --> 0.387881).  Saving model ...\n",
      "Batch [30/313], Loss: 0.3813\n",
      "Batch [60/313], Loss: 0.4304\n",
      "Batch [90/313], Loss: 0.3656\n",
      "Batch [120/313], Loss: 0.3213\n",
      "Batch [150/313], Loss: 0.5421\n",
      "Batch [180/313], Loss: 0.3037\n",
      "Batch [210/313], Loss: 0.4009\n",
      "Batch [240/313], Loss: 0.3740\n",
      "Batch [270/313], Loss: 0.3706\n",
      "Batch [300/313], Loss: 0.5411\n",
      "Epoch [17/20], Loss: 0.3849\n",
      "Validation loss decreased (0.387881 --> 0.384859).  Saving model ...\n",
      "Batch [30/313], Loss: 0.3811\n",
      "Batch [60/313], Loss: 0.2316\n",
      "Batch [90/313], Loss: 0.2949\n",
      "Batch [120/313], Loss: 0.4132\n",
      "Batch [150/313], Loss: 0.4844\n",
      "Batch [180/313], Loss: 0.4236\n",
      "Batch [210/313], Loss: 0.4096\n",
      "Batch [240/313], Loss: 0.6199\n",
      "Batch [270/313], Loss: 0.5299\n",
      "Batch [300/313], Loss: 0.3713\n",
      "Epoch [18/20], Loss: 0.3792\n",
      "Validation loss decreased (0.384859 --> 0.379156).  Saving model ...\n",
      "Batch [30/313], Loss: 0.3172\n",
      "Batch [60/313], Loss: 0.4604\n",
      "Batch [90/313], Loss: 0.3804\n",
      "Batch [120/313], Loss: 0.3360\n",
      "Batch [150/313], Loss: 0.2565\n",
      "Batch [180/313], Loss: 0.4039\n",
      "Batch [210/313], Loss: 0.3196\n",
      "Batch [240/313], Loss: 0.3695\n",
      "Batch [270/313], Loss: 0.4672\n",
      "Batch [300/313], Loss: 0.4637\n",
      "Epoch [19/20], Loss: 0.3821\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.3530\n",
      "Batch [60/313], Loss: 0.4096\n",
      "Batch [90/313], Loss: 0.3982\n",
      "Batch [120/313], Loss: 0.3769\n",
      "Batch [150/313], Loss: 0.2747\n",
      "Batch [180/313], Loss: 0.5744\n",
      "Batch [210/313], Loss: 0.3935\n",
      "Batch [240/313], Loss: 0.4697\n",
      "Batch [270/313], Loss: 0.4019\n",
      "Batch [300/313], Loss: 0.3372\n",
      "Epoch [20/20], Loss: 0.3699\n",
      "Validation loss decreased (0.379156 --> 0.369885).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███████████████▏                      | 8/20 [3:34:27<5:27:07, 1635.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20]   Loss: 3.4139   Accuracy: 38.29%\n",
      "Batch [30/313], Loss: 0.3533\n",
      "Batch [60/313], Loss: 0.3304\n",
      "Batch [90/313], Loss: 0.3290\n",
      "Batch [120/313], Loss: 0.2873\n",
      "Batch [150/313], Loss: 0.3917\n",
      "Batch [180/313], Loss: 0.4412\n",
      "Batch [210/313], Loss: 0.3472\n",
      "Batch [240/313], Loss: 0.4083\n",
      "Batch [270/313], Loss: 0.2992\n",
      "Batch [300/313], Loss: 0.3372\n",
      "Epoch [1/20], Loss: 0.3582\n",
      "Validation loss decreased (0.369885 --> 0.358242).  Saving model ...\n",
      "Batch [30/313], Loss: 0.4498\n",
      "Batch [60/313], Loss: 0.3538\n",
      "Batch [90/313], Loss: 0.3856\n",
      "Batch [120/313], Loss: 0.2703\n",
      "Batch [150/313], Loss: 0.3754\n",
      "Batch [180/313], Loss: 0.2993\n",
      "Batch [210/313], Loss: 0.3084\n",
      "Batch [240/313], Loss: 0.3889\n",
      "Batch [270/313], Loss: 0.3833\n",
      "Batch [300/313], Loss: 0.2748\n",
      "Epoch [2/20], Loss: 0.3694\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.4304\n",
      "Batch [60/313], Loss: 0.2539\n",
      "Batch [90/313], Loss: 0.4343\n",
      "Batch [120/313], Loss: 0.2631\n",
      "Batch [150/313], Loss: 0.4817\n",
      "Batch [180/313], Loss: 0.2657\n",
      "Batch [210/313], Loss: 0.2789\n",
      "Batch [240/313], Loss: 0.3696\n",
      "Batch [270/313], Loss: 0.4118\n",
      "Batch [300/313], Loss: 0.4190\n",
      "Epoch [3/20], Loss: 0.3517\n",
      "Validation loss decreased (0.358242 --> 0.351662).  Saving model ...\n",
      "Batch [30/313], Loss: 0.3507\n",
      "Batch [60/313], Loss: 0.2906\n",
      "Batch [90/313], Loss: 0.3960\n",
      "Batch [120/313], Loss: 0.3193\n",
      "Batch [150/313], Loss: 0.5422\n",
      "Batch [180/313], Loss: 0.2855\n",
      "Batch [210/313], Loss: 0.3241\n",
      "Batch [240/313], Loss: 0.2951\n",
      "Batch [270/313], Loss: 0.3809\n",
      "Batch [300/313], Loss: 0.2846\n",
      "Epoch [4/20], Loss: 0.3463\n",
      "Validation loss decreased (0.351662 --> 0.346325).  Saving model ...\n",
      "Batch [30/313], Loss: 0.3926\n",
      "Batch [60/313], Loss: 0.4198\n",
      "Batch [90/313], Loss: 0.2931\n",
      "Batch [120/313], Loss: 0.2378\n",
      "Batch [150/313], Loss: 0.1728\n",
      "Batch [180/313], Loss: 0.2796\n",
      "Batch [210/313], Loss: 0.4343\n",
      "Batch [240/313], Loss: 0.3701\n",
      "Batch [270/313], Loss: 0.3183\n",
      "Batch [300/313], Loss: 0.2553\n",
      "Epoch [5/20], Loss: 0.3442\n",
      "Validation loss decreased (0.346325 --> 0.344206).  Saving model ...\n",
      "Batch [30/313], Loss: 0.3302\n",
      "Batch [60/313], Loss: 0.2567\n",
      "Batch [90/313], Loss: 0.4724\n",
      "Batch [120/313], Loss: 0.4535\n",
      "Batch [150/313], Loss: 0.3391\n",
      "Batch [180/313], Loss: 0.3660\n",
      "Batch [210/313], Loss: 0.2153\n",
      "Batch [240/313], Loss: 0.3137\n",
      "Batch [270/313], Loss: 0.4332\n",
      "Batch [300/313], Loss: 0.2572\n",
      "Epoch [6/20], Loss: 0.3340\n",
      "Validation loss decreased (0.344206 --> 0.334043).  Saving model ...\n",
      "Batch [30/313], Loss: 0.5568\n",
      "Batch [60/313], Loss: 0.3360\n",
      "Batch [90/313], Loss: 0.3049\n",
      "Batch [120/313], Loss: 0.4025\n",
      "Batch [150/313], Loss: 0.3505\n",
      "Batch [180/313], Loss: 0.3155\n",
      "Batch [210/313], Loss: 0.3222\n",
      "Batch [240/313], Loss: 0.4132\n",
      "Batch [270/313], Loss: 0.3366\n",
      "Batch [300/313], Loss: 0.4656\n",
      "Epoch [7/20], Loss: 0.3440\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.3717\n",
      "Batch [60/313], Loss: 0.2568\n",
      "Batch [90/313], Loss: 0.3035\n",
      "Batch [120/313], Loss: 0.2384\n",
      "Batch [150/313], Loss: 0.3230\n",
      "Batch [180/313], Loss: 0.5843\n",
      "Batch [210/313], Loss: 0.3322\n",
      "Batch [240/313], Loss: 0.3703\n",
      "Batch [270/313], Loss: 0.3146\n",
      "Batch [300/313], Loss: 0.4070\n",
      "Epoch [8/20], Loss: 0.3342\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Batch [30/313], Loss: 0.2707\n",
      "Batch [60/313], Loss: 0.3205\n",
      "Batch [90/313], Loss: 0.3381\n",
      "Batch [120/313], Loss: 0.2370\n",
      "Batch [150/313], Loss: 0.4064\n",
      "Batch [180/313], Loss: 0.3667\n",
      "Batch [210/313], Loss: 0.2735\n",
      "Batch [240/313], Loss: 0.2644\n",
      "Batch [270/313], Loss: 0.2803\n",
      "Batch [300/313], Loss: 0.3181\n",
      "Epoch [9/20], Loss: 0.3251\n",
      "Validation loss decreased (0.334043 --> 0.325093).  Saving model ...\n",
      "Batch [30/313], Loss: 0.2829\n",
      "Batch [60/313], Loss: 0.2001\n",
      "Batch [90/313], Loss: 0.2389\n",
      "Batch [120/313], Loss: 0.4370\n",
      "Batch [150/313], Loss: 0.4614\n",
      "Batch [180/313], Loss: 0.2882\n",
      "Batch [210/313], Loss: 0.3128\n",
      "Batch [240/313], Loss: 0.2978\n",
      "Batch [270/313], Loss: 0.2761\n",
      "Batch [300/313], Loss: 0.2399\n",
      "Epoch [10/20], Loss: 0.3138\n",
      "Validation loss decreased (0.325093 --> 0.313801).  Saving model ...\n",
      "Batch [30/313], Loss: 0.2576\n",
      "Batch [60/313], Loss: 0.3674\n",
      "Batch [90/313], Loss: 0.2374\n",
      "Batch [120/313], Loss: 0.2787\n",
      "Batch [150/313], Loss: 0.3375\n",
      "Batch [180/313], Loss: 0.2889\n",
      "Batch [210/313], Loss: 0.3635\n",
      "Batch [240/313], Loss: 0.5359\n",
      "Batch [270/313], Loss: 0.2734\n",
      "Batch [300/313], Loss: 0.3858\n",
      "Epoch [11/20], Loss: 0.3213\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.3167\n",
      "Batch [60/313], Loss: 0.2409\n",
      "Batch [90/313], Loss: 0.2507\n",
      "Batch [120/313], Loss: 0.2395\n",
      "Batch [150/313], Loss: 0.1249\n",
      "Batch [180/313], Loss: 0.4417\n",
      "Batch [210/313], Loss: 0.2621\n",
      "Batch [240/313], Loss: 0.2665\n",
      "Batch [270/313], Loss: 0.3249\n",
      "Batch [300/313], Loss: 0.4091\n",
      "Epoch [12/20], Loss: 0.3069\n",
      "Validation loss decreased (0.313801 --> 0.306892).  Saving model ...\n",
      "Batch [30/313], Loss: 0.2335\n",
      "Batch [60/313], Loss: 0.2823\n",
      "Batch [90/313], Loss: 0.4266\n",
      "Batch [120/313], Loss: 0.2896\n",
      "Batch [150/313], Loss: 0.2976\n",
      "Batch [180/313], Loss: 0.2462\n",
      "Batch [210/313], Loss: 0.3113\n",
      "Batch [240/313], Loss: 0.2991\n",
      "Batch [270/313], Loss: 0.3036\n",
      "Batch [300/313], Loss: 0.2132\n",
      "Epoch [13/20], Loss: 0.3002\n",
      "Validation loss decreased (0.306892 --> 0.300214).  Saving model ...\n",
      "Batch [30/313], Loss: 0.2113\n",
      "Batch [60/313], Loss: 0.2251\n",
      "Batch [90/313], Loss: 0.2411\n",
      "Batch [120/313], Loss: 0.3120\n",
      "Batch [150/313], Loss: 0.3747\n",
      "Batch [180/313], Loss: 0.3484\n",
      "Batch [210/313], Loss: 0.3405\n",
      "Batch [240/313], Loss: 0.2050\n",
      "Batch [270/313], Loss: 0.3728\n",
      "Batch [300/313], Loss: 0.2748\n",
      "Epoch [14/20], Loss: 0.3022\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.2929\n",
      "Batch [60/313], Loss: 0.2973\n",
      "Batch [90/313], Loss: 0.2290\n",
      "Batch [120/313], Loss: 0.3137\n",
      "Batch [150/313], Loss: 0.3176\n",
      "Batch [180/313], Loss: 0.3070\n",
      "Batch [210/313], Loss: 0.1889\n",
      "Batch [240/313], Loss: 0.3233\n",
      "Batch [270/313], Loss: 0.3381\n",
      "Batch [300/313], Loss: 0.4260\n",
      "Epoch [15/20], Loss: 0.3023\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Batch [30/313], Loss: 0.3636\n",
      "Batch [60/313], Loss: 0.2779\n",
      "Batch [90/313], Loss: 0.3741\n",
      "Batch [120/313], Loss: 0.1951\n",
      "Batch [150/313], Loss: 0.3044\n",
      "Batch [180/313], Loss: 0.2674\n",
      "Batch [210/313], Loss: 0.2639\n",
      "Batch [240/313], Loss: 0.2670\n",
      "Batch [270/313], Loss: 0.2726\n",
      "Batch [300/313], Loss: 0.3282\n",
      "Epoch [16/20], Loss: 0.2842\n",
      "Validation loss decreased (0.300214 --> 0.284180).  Saving model ...\n",
      "Batch [30/313], Loss: 0.2159\n",
      "Batch [60/313], Loss: 0.1683\n",
      "Batch [90/313], Loss: 0.2445\n",
      "Batch [120/313], Loss: 0.3178\n",
      "Batch [150/313], Loss: 0.5126\n",
      "Batch [180/313], Loss: 0.3435\n",
      "Batch [210/313], Loss: 0.2784\n",
      "Batch [240/313], Loss: 0.2910\n",
      "Batch [270/313], Loss: 0.3401\n",
      "Batch [300/313], Loss: 0.3338\n",
      "Epoch [17/20], Loss: 0.2895\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.2401\n",
      "Batch [60/313], Loss: 0.3035\n",
      "Batch [90/313], Loss: 0.3106\n",
      "Batch [120/313], Loss: 0.2477\n",
      "Batch [150/313], Loss: 0.1900\n",
      "Batch [180/313], Loss: 0.3395\n",
      "Batch [210/313], Loss: 0.2440\n",
      "Batch [240/313], Loss: 0.1973\n",
      "Batch [270/313], Loss: 0.3931\n",
      "Batch [300/313], Loss: 0.3239\n",
      "Epoch [18/20], Loss: 0.2833\n",
      "Validation loss decreased (0.284180 --> 0.283310).  Saving model ...\n",
      "Batch [30/313], Loss: 0.1859\n",
      "Batch [60/313], Loss: 0.2418\n",
      "Batch [90/313], Loss: 0.2690\n",
      "Batch [120/313], Loss: 0.2212\n",
      "Batch [150/313], Loss: 0.3031\n",
      "Batch [180/313], Loss: 0.2613\n",
      "Batch [210/313], Loss: 0.3305\n",
      "Batch [240/313], Loss: 0.3693\n",
      "Batch [270/313], Loss: 0.2952\n",
      "Batch [300/313], Loss: 0.2110\n",
      "Epoch [19/20], Loss: 0.2753\n",
      "Validation loss decreased (0.283310 --> 0.275279).  Saving model ...\n",
      "Batch [30/313], Loss: 0.3431\n",
      "Batch [60/313], Loss: 0.3320\n",
      "Batch [90/313], Loss: 0.2342\n",
      "Batch [120/313], Loss: 0.3096\n",
      "Batch [150/313], Loss: 0.2719\n",
      "Batch [180/313], Loss: 0.2392\n",
      "Batch [210/313], Loss: 0.2727\n",
      "Batch [240/313], Loss: 0.1732\n",
      "Batch [270/313], Loss: 0.3095\n",
      "Batch [300/313], Loss: 0.3060\n",
      "Epoch [20/20], Loss: 0.2772\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████                     | 9/20 [4:01:42<4:59:51, 1635.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/20]   Loss: 3.5230   Accuracy: 38.97%\n",
      "Batch [30/313], Loss: 0.1977\n",
      "Batch [60/313], Loss: 0.2696\n",
      "Batch [90/313], Loss: 0.2064\n",
      "Batch [120/313], Loss: 0.3741\n",
      "Batch [150/313], Loss: 0.2577\n",
      "Batch [180/313], Loss: 0.3001\n",
      "Batch [210/313], Loss: 0.3438\n",
      "Batch [240/313], Loss: 0.1860\n",
      "Batch [270/313], Loss: 0.2343\n",
      "Batch [300/313], Loss: 0.3911\n",
      "Epoch [1/20], Loss: 0.2765\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Batch [30/313], Loss: 0.2384\n",
      "Batch [60/313], Loss: 0.2926\n",
      "Batch [90/313], Loss: 0.2747\n",
      "Batch [120/313], Loss: 0.2965\n",
      "Batch [150/313], Loss: 0.2567\n",
      "Batch [180/313], Loss: 0.3483\n",
      "Batch [210/313], Loss: 0.3754\n",
      "Batch [240/313], Loss: 0.3477\n",
      "Batch [270/313], Loss: 0.2220\n",
      "Batch [300/313], Loss: 0.3504\n",
      "Epoch [2/20], Loss: 0.2741\n",
      "Validation loss decreased (0.275279 --> 0.274129).  Saving model ...\n",
      "Batch [30/313], Loss: 0.2070\n",
      "Batch [60/313], Loss: 0.2076\n",
      "Batch [90/313], Loss: 0.2475\n",
      "Batch [120/313], Loss: 0.2266\n",
      "Batch [150/313], Loss: 0.4084\n",
      "Batch [180/313], Loss: 0.3262\n",
      "Batch [210/313], Loss: 0.2228\n",
      "Batch [240/313], Loss: 0.3293\n",
      "Batch [270/313], Loss: 0.2019\n",
      "Batch [300/313], Loss: 0.4180\n",
      "Epoch [3/20], Loss: 0.2665\n",
      "Validation loss decreased (0.274129 --> 0.266518).  Saving model ...\n",
      "Batch [30/313], Loss: 0.2719\n",
      "Batch [60/313], Loss: 0.1901\n",
      "Batch [90/313], Loss: 0.2692\n",
      "Batch [120/313], Loss: 0.2008\n",
      "Batch [150/313], Loss: 0.3740\n",
      "Batch [180/313], Loss: 0.2376\n",
      "Batch [210/313], Loss: 0.4558\n",
      "Batch [240/313], Loss: 0.2721\n",
      "Batch [270/313], Loss: 0.4231\n",
      "Batch [300/313], Loss: 0.3295\n",
      "Epoch [4/20], Loss: 0.2647\n",
      "Validation loss decreased (0.266518 --> 0.264749).  Saving model ...\n",
      "Batch [30/313], Loss: 0.2938\n",
      "Batch [60/313], Loss: 0.1893\n",
      "Batch [90/313], Loss: 0.1687\n",
      "Batch [120/313], Loss: 0.2899\n",
      "Batch [150/313], Loss: 0.1877\n",
      "Batch [180/313], Loss: 0.2824\n",
      "Batch [210/313], Loss: 0.2332\n",
      "Batch [240/313], Loss: 0.2362\n",
      "Batch [270/313], Loss: 0.1692\n",
      "Batch [300/313], Loss: 0.2347\n",
      "Epoch [5/20], Loss: 0.2531\n",
      "Validation loss decreased (0.264749 --> 0.253099).  Saving model ...\n",
      "Batch [30/313], Loss: 0.2477\n",
      "Batch [60/313], Loss: 0.1871\n",
      "Batch [90/313], Loss: 0.2495\n",
      "Batch [120/313], Loss: 0.2819\n",
      "Batch [150/313], Loss: 0.2377\n",
      "Batch [180/313], Loss: 0.1531\n",
      "Batch [210/313], Loss: 0.2208\n",
      "Batch [240/313], Loss: 0.2080\n",
      "Batch [270/313], Loss: 0.3571\n",
      "Batch [300/313], Loss: 0.3097\n",
      "Epoch [6/20], Loss: 0.2532\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.2009\n",
      "Batch [60/313], Loss: 0.1651\n",
      "Batch [90/313], Loss: 0.3087\n",
      "Batch [120/313], Loss: 0.1576\n",
      "Batch [150/313], Loss: 0.3524\n",
      "Batch [180/313], Loss: 0.3397\n",
      "Batch [210/313], Loss: 0.2463\n",
      "Batch [240/313], Loss: 0.1596\n",
      "Batch [270/313], Loss: 0.2408\n",
      "Batch [300/313], Loss: 0.3507\n",
      "Epoch [7/20], Loss: 0.2583\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Batch [30/313], Loss: 0.3211\n",
      "Batch [60/313], Loss: 0.3085\n",
      "Batch [90/313], Loss: 0.2719\n",
      "Batch [120/313], Loss: 0.1981\n",
      "Batch [150/313], Loss: 0.3218\n",
      "Batch [180/313], Loss: 0.2755\n",
      "Batch [210/313], Loss: 0.2244\n",
      "Batch [240/313], Loss: 0.2386\n",
      "Batch [270/313], Loss: 0.2816\n",
      "Batch [300/313], Loss: 0.3110\n",
      "Epoch [8/20], Loss: 0.2614\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Batch [30/313], Loss: 0.2240\n",
      "Batch [60/313], Loss: 0.1994\n",
      "Batch [90/313], Loss: 0.2196\n",
      "Batch [120/313], Loss: 0.3240\n",
      "Batch [150/313], Loss: 0.2373\n",
      "Batch [180/313], Loss: 0.3438\n",
      "Batch [210/313], Loss: 0.2935\n",
      "Batch [240/313], Loss: 0.1339\n",
      "Batch [270/313], Loss: 0.2177\n",
      "Batch [300/313], Loss: 0.2004\n",
      "Epoch [9/20], Loss: 0.2458\n",
      "Validation loss decreased (0.253099 --> 0.245837).  Saving model ...\n",
      "Batch [30/313], Loss: 0.3077\n",
      "Batch [60/313], Loss: 0.2632\n",
      "Batch [90/313], Loss: 0.3238\n",
      "Batch [120/313], Loss: 0.1223\n",
      "Batch [150/313], Loss: 0.1990\n",
      "Batch [180/313], Loss: 0.2182\n",
      "Batch [210/313], Loss: 0.2433\n",
      "Batch [240/313], Loss: 0.2621\n",
      "Batch [270/313], Loss: 0.3421\n",
      "Batch [300/313], Loss: 0.3822\n",
      "Epoch [10/20], Loss: 0.2497\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.1275\n",
      "Batch [60/313], Loss: 0.3150\n",
      "Batch [90/313], Loss: 0.2627\n",
      "Batch [120/313], Loss: 0.2805\n",
      "Batch [150/313], Loss: 0.2828\n",
      "Batch [180/313], Loss: 0.2208\n",
      "Batch [210/313], Loss: 0.2416\n",
      "Batch [240/313], Loss: 0.3866\n",
      "Batch [270/313], Loss: 0.2994\n",
      "Batch [300/313], Loss: 0.1936\n",
      "Epoch [11/20], Loss: 0.2458\n",
      "Validation loss decreased (0.245837 --> 0.245773).  Saving model ...\n",
      "Batch [30/313], Loss: 0.2597\n",
      "Batch [60/313], Loss: 0.3538\n",
      "Batch [90/313], Loss: 0.2999\n",
      "Batch [120/313], Loss: 0.3574\n",
      "Batch [150/313], Loss: 0.1881\n",
      "Batch [180/313], Loss: 0.2109\n",
      "Batch [210/313], Loss: 0.2829\n",
      "Batch [240/313], Loss: 0.1516\n",
      "Batch [270/313], Loss: 0.1845\n",
      "Batch [300/313], Loss: 0.3104\n",
      "Epoch [12/20], Loss: 0.2416\n",
      "Validation loss decreased (0.245773 --> 0.241621).  Saving model ...\n",
      "Batch [30/313], Loss: 0.2870\n",
      "Batch [60/313], Loss: 0.2351\n",
      "Batch [90/313], Loss: 0.2365\n",
      "Batch [120/313], Loss: 0.2451\n",
      "Batch [150/313], Loss: 0.2706\n",
      "Batch [180/313], Loss: 0.3230\n",
      "Batch [210/313], Loss: 0.3628\n",
      "Batch [240/313], Loss: 0.1601\n",
      "Batch [270/313], Loss: 0.3616\n",
      "Batch [300/313], Loss: 0.1940\n",
      "Epoch [13/20], Loss: 0.2460\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.1651\n",
      "Batch [60/313], Loss: 0.2295\n",
      "Batch [90/313], Loss: 0.2803\n",
      "Batch [120/313], Loss: 0.1548\n",
      "Batch [150/313], Loss: 0.2290\n",
      "Batch [180/313], Loss: 0.2755\n",
      "Batch [210/313], Loss: 0.3625\n",
      "Batch [240/313], Loss: 0.4269\n",
      "Batch [270/313], Loss: 0.2738\n",
      "Batch [300/313], Loss: 0.1941\n",
      "Epoch [14/20], Loss: 0.2313\n",
      "Validation loss decreased (0.241621 --> 0.231320).  Saving model ...\n",
      "Batch [30/313], Loss: 0.2801\n",
      "Batch [60/313], Loss: 0.2198\n",
      "Batch [90/313], Loss: 0.1680\n",
      "Batch [120/313], Loss: 0.3200\n",
      "Batch [150/313], Loss: 0.2126\n",
      "Batch [180/313], Loss: 0.2292\n",
      "Batch [210/313], Loss: 0.2423\n",
      "Batch [240/313], Loss: 0.2281\n",
      "Batch [270/313], Loss: 0.2493\n",
      "Batch [300/313], Loss: 0.2363\n",
      "Epoch [15/20], Loss: 0.2314\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.2830\n",
      "Batch [60/313], Loss: 0.2888\n",
      "Batch [90/313], Loss: 0.2436\n",
      "Batch [120/313], Loss: 0.1609\n",
      "Batch [150/313], Loss: 0.2019\n",
      "Batch [180/313], Loss: 0.1999\n",
      "Batch [210/313], Loss: 0.3654\n",
      "Batch [240/313], Loss: 0.3300\n",
      "Batch [270/313], Loss: 0.2397\n",
      "Batch [300/313], Loss: 0.3323\n",
      "Epoch [16/20], Loss: 0.2346\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Batch [30/313], Loss: 0.1447\n",
      "Batch [60/313], Loss: 0.3435\n",
      "Batch [90/313], Loss: 0.2453\n",
      "Batch [120/313], Loss: 0.1253\n",
      "Batch [150/313], Loss: 0.2823\n",
      "Batch [180/313], Loss: 0.2083\n",
      "Batch [210/313], Loss: 0.2408\n",
      "Batch [240/313], Loss: 0.1981\n",
      "Batch [270/313], Loss: 0.2341\n",
      "Batch [300/313], Loss: 0.1420\n",
      "Epoch [17/20], Loss: 0.2272\n",
      "Validation loss decreased (0.231320 --> 0.227210).  Saving model ...\n",
      "Batch [30/313], Loss: 0.2461\n",
      "Batch [60/313], Loss: 0.1260\n",
      "Batch [90/313], Loss: 0.1604\n",
      "Batch [120/313], Loss: 0.1816\n",
      "Batch [150/313], Loss: 0.2727\n",
      "Batch [180/313], Loss: 0.1781\n",
      "Batch [210/313], Loss: 0.1527\n",
      "Batch [240/313], Loss: 0.1720\n",
      "Batch [270/313], Loss: 0.2513\n",
      "Batch [300/313], Loss: 0.3039\n",
      "Epoch [18/20], Loss: 0.2232\n",
      "Validation loss decreased (0.227210 --> 0.223191).  Saving model ...\n",
      "Batch [30/313], Loss: 0.3704\n",
      "Batch [60/313], Loss: 0.2799\n",
      "Batch [90/313], Loss: 0.3658\n",
      "Batch [120/313], Loss: 0.2089\n",
      "Batch [150/313], Loss: 0.2998\n",
      "Batch [180/313], Loss: 0.1199\n",
      "Batch [210/313], Loss: 0.1616\n",
      "Batch [240/313], Loss: 0.2007\n",
      "Batch [270/313], Loss: 0.2448\n",
      "Batch [300/313], Loss: 0.2510\n",
      "Epoch [19/20], Loss: 0.2241\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.1790\n",
      "Batch [60/313], Loss: 0.2017\n",
      "Batch [90/313], Loss: 0.1596\n",
      "Batch [120/313], Loss: 0.2249\n",
      "Batch [150/313], Loss: 0.2423\n",
      "Batch [180/313], Loss: 0.1739\n",
      "Batch [210/313], Loss: 0.2362\n",
      "Batch [240/313], Loss: 0.1804\n",
      "Batch [270/313], Loss: 0.2871\n",
      "Batch [300/313], Loss: 0.3612\n",
      "Epoch [20/20], Loss: 0.2206\n",
      "Validation loss decreased (0.223191 --> 0.220639).  Saving model ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████▌                  | 10/20 [4:29:03<4:32:52, 1637.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/20]   Loss: 3.6415   Accuracy: 39.11%\n",
      "Batch [30/313], Loss: 0.0947\n",
      "Batch [60/313], Loss: 0.2006\n",
      "Batch [90/313], Loss: 0.1822\n",
      "Batch [120/313], Loss: 0.1364\n",
      "Batch [150/313], Loss: 0.1827\n",
      "Batch [180/313], Loss: 0.2669\n",
      "Batch [210/313], Loss: 0.1532\n",
      "Batch [240/313], Loss: 0.1669\n",
      "Batch [270/313], Loss: 0.2641\n",
      "Batch [300/313], Loss: 0.2310\n",
      "Epoch [1/20], Loss: 0.2182\n",
      "Validation loss decreased (0.220639 --> 0.218168).  Saving model ...\n",
      "Batch [30/313], Loss: 0.2168\n",
      "Batch [60/313], Loss: 0.2091\n",
      "Batch [90/313], Loss: 0.1774\n",
      "Batch [120/313], Loss: 0.1865\n",
      "Batch [150/313], Loss: 0.1759\n",
      "Batch [180/313], Loss: 0.1421\n",
      "Batch [210/313], Loss: 0.1786\n",
      "Batch [240/313], Loss: 0.2280\n",
      "Batch [270/313], Loss: 0.2121\n",
      "Batch [300/313], Loss: 0.2091\n",
      "Epoch [2/20], Loss: 0.2062\n",
      "Validation loss decreased (0.218168 --> 0.206172).  Saving model ...\n",
      "Batch [30/313], Loss: 0.2061\n",
      "Batch [60/313], Loss: 0.1987\n",
      "Batch [90/313], Loss: 0.1190\n",
      "Batch [120/313], Loss: 0.2421\n",
      "Batch [150/313], Loss: 0.2574\n",
      "Batch [180/313], Loss: 0.1701\n",
      "Batch [210/313], Loss: 0.2075\n",
      "Batch [240/313], Loss: 0.2168\n",
      "Batch [270/313], Loss: 0.2714\n",
      "Batch [300/313], Loss: 0.1789\n",
      "Epoch [3/20], Loss: 0.2111\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.2335\n",
      "Batch [60/313], Loss: 0.1128\n",
      "Batch [90/313], Loss: 0.2305\n",
      "Batch [120/313], Loss: 0.1750\n",
      "Batch [150/313], Loss: 0.2524\n",
      "Batch [180/313], Loss: 0.1515\n",
      "Batch [210/313], Loss: 0.1478\n",
      "Batch [240/313], Loss: 0.3129\n",
      "Batch [270/313], Loss: 0.1329\n",
      "Batch [300/313], Loss: 0.1641\n",
      "Epoch [4/20], Loss: 0.2134\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Batch [30/313], Loss: 0.1316\n",
      "Batch [60/313], Loss: 0.2083\n",
      "Batch [90/313], Loss: 0.1304\n",
      "Batch [120/313], Loss: 0.2611\n",
      "Batch [150/313], Loss: 0.1406\n",
      "Batch [180/313], Loss: 0.2704\n",
      "Batch [210/313], Loss: 0.1377\n",
      "Batch [240/313], Loss: 0.1685\n",
      "Batch [270/313], Loss: 0.1342\n",
      "Batch [300/313], Loss: 0.2146\n",
      "Epoch [5/20], Loss: 0.1972\n",
      "Validation loss decreased (0.206172 --> 0.197153).  Saving model ...\n",
      "Batch [30/313], Loss: 0.2416\n",
      "Batch [60/313], Loss: 0.1871\n",
      "Batch [90/313], Loss: 0.1744\n",
      "Batch [120/313], Loss: 0.1633\n",
      "Batch [150/313], Loss: 0.1969\n",
      "Batch [180/313], Loss: 0.2392\n",
      "Batch [210/313], Loss: 0.0566\n",
      "Batch [240/313], Loss: 0.1539\n",
      "Batch [270/313], Loss: 0.2190\n",
      "Batch [300/313], Loss: 0.2121\n",
      "Epoch [6/20], Loss: 0.1994\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.1743\n",
      "Batch [60/313], Loss: 0.1401\n",
      "Batch [90/313], Loss: 0.1465\n",
      "Batch [120/313], Loss: 0.2084\n",
      "Batch [150/313], Loss: 0.1398\n",
      "Batch [180/313], Loss: 0.2012\n",
      "Batch [210/313], Loss: 0.2317\n",
      "Batch [240/313], Loss: 0.1571\n",
      "Batch [270/313], Loss: 0.1133\n",
      "Batch [300/313], Loss: 0.1748\n",
      "Epoch [7/20], Loss: 0.1966\n",
      "Validation loss decreased (0.197153 --> 0.196635).  Saving model ...\n",
      "Batch [30/313], Loss: 0.1608\n",
      "Batch [60/313], Loss: 0.2780\n",
      "Batch [90/313], Loss: 0.2372\n",
      "Batch [120/313], Loss: 0.1435\n",
      "Batch [150/313], Loss: 0.1554\n",
      "Batch [180/313], Loss: 0.1579\n",
      "Batch [210/313], Loss: 0.2196\n",
      "Batch [240/313], Loss: 0.2494\n",
      "Batch [270/313], Loss: 0.2241\n",
      "Batch [300/313], Loss: 0.1879\n",
      "Epoch [8/20], Loss: 0.1959\n",
      "Validation loss decreased (0.196635 --> 0.195895).  Saving model ...\n",
      "Batch [30/313], Loss: 0.2066\n",
      "Batch [60/313], Loss: 0.3244\n",
      "Batch [90/313], Loss: 0.1062\n",
      "Batch [120/313], Loss: 0.2302\n",
      "Batch [150/313], Loss: 0.1706\n",
      "Batch [180/313], Loss: 0.2411\n",
      "Batch [210/313], Loss: 0.1907\n",
      "Batch [240/313], Loss: 0.1624\n",
      "Batch [270/313], Loss: 0.1121\n",
      "Batch [300/313], Loss: 0.2611\n",
      "Epoch [9/20], Loss: 0.1953\n",
      "Validation loss decreased (0.195895 --> 0.195283).  Saving model ...\n",
      "Batch [30/313], Loss: 0.1410\n",
      "Batch [60/313], Loss: 0.1560\n",
      "Batch [90/313], Loss: 0.2613\n",
      "Batch [120/313], Loss: 0.1357\n",
      "Batch [150/313], Loss: 0.1162\n",
      "Batch [180/313], Loss: 0.2172\n",
      "Batch [210/313], Loss: 0.1622\n",
      "Batch [240/313], Loss: 0.2025\n",
      "Batch [270/313], Loss: 0.2295\n",
      "Batch [300/313], Loss: 0.2679\n",
      "Epoch [10/20], Loss: 0.1909\n",
      "Validation loss decreased (0.195283 --> 0.190852).  Saving model ...\n",
      "Batch [30/313], Loss: 0.1746\n",
      "Batch [60/313], Loss: 0.2053\n",
      "Batch [90/313], Loss: 0.2040\n",
      "Batch [120/313], Loss: 0.2476\n",
      "Batch [150/313], Loss: 0.1974\n",
      "Batch [180/313], Loss: 0.1746\n",
      "Batch [210/313], Loss: 0.1506\n",
      "Batch [240/313], Loss: 0.2102\n",
      "Batch [270/313], Loss: 0.2856\n",
      "Batch [300/313], Loss: 0.2529\n",
      "Epoch [11/20], Loss: 0.1936\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.1844\n",
      "Batch [60/313], Loss: 0.1688\n",
      "Batch [90/313], Loss: 0.1669\n",
      "Batch [120/313], Loss: 0.1458\n",
      "Batch [150/313], Loss: 0.1245\n",
      "Batch [180/313], Loss: 0.1799\n",
      "Batch [210/313], Loss: 0.1186\n",
      "Batch [240/313], Loss: 0.2289\n",
      "Batch [270/313], Loss: 0.1863\n",
      "Batch [300/313], Loss: 0.1308\n",
      "Epoch [12/20], Loss: 0.1904\n",
      "Validation loss decreased (0.190852 --> 0.190388).  Saving model ...\n",
      "Batch [30/313], Loss: 0.1835\n",
      "Batch [60/313], Loss: 0.3681\n",
      "Batch [90/313], Loss: 0.1723\n",
      "Batch [120/313], Loss: 0.2345\n",
      "Batch [150/313], Loss: 0.1397\n",
      "Batch [180/313], Loss: 0.1651\n",
      "Batch [210/313], Loss: 0.1623\n",
      "Batch [240/313], Loss: 0.1940\n",
      "Batch [270/313], Loss: 0.1251\n",
      "Batch [300/313], Loss: 0.1315\n",
      "Epoch [13/20], Loss: 0.1923\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.1111\n",
      "Batch [60/313], Loss: 0.1817\n",
      "Batch [90/313], Loss: 0.1739\n",
      "Batch [120/313], Loss: 0.1368\n",
      "Batch [150/313], Loss: 0.1760\n",
      "Batch [180/313], Loss: 0.1226\n",
      "Batch [210/313], Loss: 0.2622\n",
      "Batch [240/313], Loss: 0.2209\n",
      "Batch [270/313], Loss: 0.1271\n",
      "Batch [300/313], Loss: 0.1334\n",
      "Epoch [14/20], Loss: 0.1850\n",
      "Validation loss decreased (0.190388 --> 0.184960).  Saving model ...\n",
      "Batch [30/313], Loss: 0.1168\n",
      "Batch [60/313], Loss: 0.1882\n",
      "Batch [90/313], Loss: 0.1772\n",
      "Batch [120/313], Loss: 0.2186\n",
      "Batch [150/313], Loss: 0.1600\n",
      "Batch [180/313], Loss: 0.3055\n",
      "Batch [210/313], Loss: 0.1568\n",
      "Batch [240/313], Loss: 0.2256\n",
      "Batch [270/313], Loss: 0.3407\n",
      "Batch [300/313], Loss: 0.2926\n",
      "Epoch [15/20], Loss: 0.1891\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.1135\n",
      "Batch [60/313], Loss: 0.2082\n",
      "Batch [90/313], Loss: 0.1372\n",
      "Batch [120/313], Loss: 0.1424\n",
      "Batch [150/313], Loss: 0.1450\n",
      "Batch [180/313], Loss: 0.1372\n",
      "Batch [210/313], Loss: 0.2165\n",
      "Batch [240/313], Loss: 0.1402\n",
      "Batch [270/313], Loss: 0.2373\n",
      "Batch [300/313], Loss: 0.2317\n",
      "Epoch [16/20], Loss: 0.1823\n",
      "Validation loss decreased (0.184960 --> 0.182343).  Saving model ...\n",
      "Batch [30/313], Loss: 0.0867\n",
      "Batch [60/313], Loss: 0.1442\n",
      "Batch [90/313], Loss: 0.1727\n",
      "Batch [120/313], Loss: 0.1734\n",
      "Batch [150/313], Loss: 0.1998\n",
      "Batch [180/313], Loss: 0.1804\n",
      "Batch [210/313], Loss: 0.1536\n",
      "Batch [240/313], Loss: 0.2164\n",
      "Batch [270/313], Loss: 0.1831\n",
      "Batch [300/313], Loss: 0.2182\n",
      "Epoch [17/20], Loss: 0.1840\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.1591\n",
      "Batch [60/313], Loss: 0.2127\n",
      "Batch [90/313], Loss: 0.1412\n",
      "Batch [120/313], Loss: 0.1915\n",
      "Batch [150/313], Loss: 0.1679\n",
      "Batch [180/313], Loss: 0.1507\n",
      "Batch [210/313], Loss: 0.2799\n",
      "Batch [240/313], Loss: 0.2056\n",
      "Batch [270/313], Loss: 0.2080\n",
      "Batch [300/313], Loss: 0.3407\n",
      "Epoch [18/20], Loss: 0.1791\n",
      "Validation loss decreased (0.182343 --> 0.179052).  Saving model ...\n",
      "Batch [30/313], Loss: 0.1626\n",
      "Batch [60/313], Loss: 0.1085\n",
      "Batch [90/313], Loss: 0.2111\n",
      "Batch [120/313], Loss: 0.1294\n",
      "Batch [150/313], Loss: 0.2664\n",
      "Batch [180/313], Loss: 0.2331\n",
      "Batch [210/313], Loss: 0.1260\n",
      "Batch [240/313], Loss: 0.2615\n",
      "Batch [270/313], Loss: 0.1622\n",
      "Batch [300/313], Loss: 0.2805\n",
      "Epoch [19/20], Loss: 0.1741\n",
      "Validation loss decreased (0.179052 --> 0.174125).  Saving model ...\n",
      "Batch [30/313], Loss: 0.0873\n",
      "Batch [60/313], Loss: 0.1510\n",
      "Batch [90/313], Loss: 0.1476\n",
      "Batch [120/313], Loss: 0.2385\n",
      "Batch [150/313], Loss: 0.4585\n",
      "Batch [180/313], Loss: 0.1679\n",
      "Batch [210/313], Loss: 0.0584\n",
      "Batch [240/313], Loss: 0.2610\n",
      "Batch [270/313], Loss: 0.1612\n",
      "Batch [300/313], Loss: 0.2687\n",
      "Epoch [20/20], Loss: 0.1775\n",
      "EarlyStopping counter: 1 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████████████████▎                | 11/20 [4:56:25<4:05:49, 1638.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/20]   Loss: 3.7433   Accuracy: 38.66%\n",
      "Batch [30/313], Loss: 0.0839\n",
      "Batch [60/313], Loss: 0.1664\n",
      "Batch [90/313], Loss: 0.2175\n",
      "Batch [120/313], Loss: 0.1891\n",
      "Batch [150/313], Loss: 0.1708\n",
      "Batch [180/313], Loss: 0.1681\n",
      "Batch [210/313], Loss: 0.1548\n",
      "Batch [240/313], Loss: 0.1629\n",
      "Batch [270/313], Loss: 0.1563\n",
      "Batch [300/313], Loss: 0.1511\n",
      "Epoch [1/20], Loss: 0.1759\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Batch [30/313], Loss: 0.1133\n",
      "Batch [60/313], Loss: 0.1813\n",
      "Batch [90/313], Loss: 0.2061\n",
      "Batch [120/313], Loss: 0.1848\n",
      "Batch [150/313], Loss: 0.2317\n",
      "Batch [180/313], Loss: 0.1332\n",
      "Batch [210/313], Loss: 0.1675\n",
      "Batch [240/313], Loss: 0.2455\n",
      "Batch [270/313], Loss: 0.1545\n",
      "Batch [300/313], Loss: 0.1815\n",
      "Epoch [2/20], Loss: 0.1724\n",
      "Validation loss decreased (0.174125 --> 0.172390).  Saving model ...\n",
      "Batch [30/313], Loss: 0.1296\n",
      "Batch [60/313], Loss: 0.1465\n",
      "Batch [90/313], Loss: 0.1479\n",
      "Batch [120/313], Loss: 0.2312\n",
      "Batch [150/313], Loss: 0.1861\n",
      "Batch [180/313], Loss: 0.1464\n",
      "Batch [210/313], Loss: 0.1540\n",
      "Batch [240/313], Loss: 0.1543\n",
      "Batch [270/313], Loss: 0.1027\n",
      "Batch [300/313], Loss: 0.1429\n",
      "Epoch [3/20], Loss: 0.1694\n",
      "Validation loss decreased (0.172390 --> 0.169372).  Saving model ...\n",
      "Batch [30/313], Loss: 0.1396\n",
      "Batch [60/313], Loss: 0.1183\n",
      "Batch [90/313], Loss: 0.1377\n",
      "Batch [120/313], Loss: 0.2416\n",
      "Batch [150/313], Loss: 0.2106\n",
      "Batch [180/313], Loss: 0.1722\n",
      "Batch [210/313], Loss: 0.1343\n",
      "Batch [240/313], Loss: 0.0916\n",
      "Batch [270/313], Loss: 0.2109\n",
      "Batch [300/313], Loss: 0.0978\n",
      "Epoch [4/20], Loss: 0.1629\n",
      "Validation loss decreased (0.169372 --> 0.162949).  Saving model ...\n",
      "Batch [30/313], Loss: 0.1740\n",
      "Batch [60/313], Loss: 0.1725\n",
      "Batch [90/313], Loss: 0.2165\n",
      "Batch [120/313], Loss: 0.1194\n",
      "Batch [150/313], Loss: 0.1098\n",
      "Batch [180/313], Loss: 0.2736\n",
      "Batch [210/313], Loss: 0.0942\n",
      "Batch [240/313], Loss: 0.0841\n",
      "Batch [270/313], Loss: 0.1196\n",
      "Batch [300/313], Loss: 0.1210\n",
      "Epoch [5/20], Loss: 0.1652\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.1430\n",
      "Batch [60/313], Loss: 0.1471\n",
      "Batch [90/313], Loss: 0.1104\n",
      "Batch [120/313], Loss: 0.3240\n",
      "Batch [150/313], Loss: 0.0843\n",
      "Batch [180/313], Loss: 0.3019\n",
      "Batch [210/313], Loss: 0.1954\n",
      "Batch [240/313], Loss: 0.2622\n",
      "Batch [270/313], Loss: 0.1317\n",
      "Batch [300/313], Loss: 0.1500\n",
      "Epoch [6/20], Loss: 0.1707\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Batch [30/313], Loss: 0.1012\n",
      "Batch [60/313], Loss: 0.1316\n",
      "Batch [90/313], Loss: 0.1444\n",
      "Batch [120/313], Loss: 0.1788\n",
      "Batch [150/313], Loss: 0.1729\n",
      "Batch [180/313], Loss: 0.2207\n",
      "Batch [210/313], Loss: 0.1309\n",
      "Batch [240/313], Loss: 0.1794\n",
      "Batch [270/313], Loss: 0.1178\n",
      "Batch [300/313], Loss: 0.0916\n",
      "Epoch [7/20], Loss: 0.1644\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Batch [30/313], Loss: 0.1416\n",
      "Batch [60/313], Loss: 0.1412\n",
      "Batch [90/313], Loss: 0.2349\n",
      "Batch [120/313], Loss: 0.1435\n",
      "Batch [150/313], Loss: 0.2316\n",
      "Batch [180/313], Loss: 0.2000\n",
      "Batch [210/313], Loss: 0.1489\n",
      "Batch [240/313], Loss: 0.1191\n",
      "Batch [270/313], Loss: 0.1501\n",
      "Batch [300/313], Loss: 0.1888\n",
      "Epoch [8/20], Loss: 0.1664\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Batch [30/313], Loss: 0.2015\n",
      "Batch [60/313], Loss: 0.1667\n",
      "Batch [90/313], Loss: 0.1054\n",
      "Batch [120/313], Loss: 0.1881\n",
      "Batch [150/313], Loss: 0.1603\n",
      "Batch [180/313], Loss: 0.1495\n",
      "Batch [210/313], Loss: 0.2497\n",
      "Batch [240/313], Loss: 0.2387\n",
      "Batch [270/313], Loss: 0.1226\n",
      "Batch [300/313], Loss: 0.1957\n",
      "Epoch [9/20], Loss: 0.1564\n",
      "Validation loss decreased (0.162949 --> 0.156422).  Saving model ...\n",
      "Batch [30/313], Loss: 0.0636\n",
      "Batch [60/313], Loss: 0.3032\n",
      "Batch [90/313], Loss: 0.0923\n",
      "Batch [120/313], Loss: 0.1378\n",
      "Batch [150/313], Loss: 0.1381\n",
      "Batch [180/313], Loss: 0.0852\n",
      "Batch [210/313], Loss: 0.1756\n",
      "Batch [240/313], Loss: 0.0674\n",
      "Batch [270/313], Loss: 0.1011\n",
      "Batch [300/313], Loss: 0.1708\n",
      "Epoch [10/20], Loss: 0.1646\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.0986\n",
      "Batch [60/313], Loss: 0.1758\n",
      "Batch [90/313], Loss: 0.1900\n",
      "Batch [120/313], Loss: 0.1439\n",
      "Batch [150/313], Loss: 0.2679\n",
      "Batch [180/313], Loss: 0.1037\n",
      "Batch [210/313], Loss: 0.1596\n",
      "Batch [240/313], Loss: 0.2793\n",
      "Batch [270/313], Loss: 0.0973\n",
      "Batch [300/313], Loss: 0.0939\n",
      "Epoch [11/20], Loss: 0.1646\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Batch [30/313], Loss: 0.1080\n",
      "Batch [60/313], Loss: 0.1893\n",
      "Batch [90/313], Loss: 0.1078\n",
      "Batch [120/313], Loss: 0.1184\n",
      "Batch [150/313], Loss: 0.1180\n",
      "Batch [180/313], Loss: 0.2321\n",
      "Batch [210/313], Loss: 0.1589\n",
      "Batch [240/313], Loss: 0.1912\n",
      "Batch [270/313], Loss: 0.1692\n",
      "Batch [300/313], Loss: 0.2638\n",
      "Epoch [12/20], Loss: 0.1618\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Batch [30/313], Loss: 0.0754\n",
      "Batch [60/313], Loss: 0.1747\n",
      "Batch [90/313], Loss: 0.1177\n",
      "Batch [120/313], Loss: 0.0825\n",
      "Batch [150/313], Loss: 0.2372\n",
      "Batch [180/313], Loss: 0.1810\n",
      "Batch [210/313], Loss: 0.1303\n",
      "Batch [240/313], Loss: 0.1209\n",
      "Batch [270/313], Loss: 0.0940\n",
      "Batch [300/313], Loss: 0.1533\n",
      "Epoch [13/20], Loss: 0.1575\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Batch [30/313], Loss: 0.1282\n",
      "Batch [60/313], Loss: 0.1721\n",
      "Batch [90/313], Loss: 0.1798\n",
      "Batch [120/313], Loss: 0.1739\n",
      "Batch [150/313], Loss: 0.1456\n",
      "Batch [180/313], Loss: 0.0991\n",
      "Batch [210/313], Loss: 0.1508\n",
      "Batch [240/313], Loss: 0.1041\n",
      "Batch [270/313], Loss: 0.1489\n",
      "Batch [300/313], Loss: 0.2243\n",
      "Epoch [14/20], Loss: 0.1560\n",
      "Validation loss decreased (0.156422 --> 0.156046).  Saving model ...\n",
      "Batch [30/313], Loss: 0.1308\n",
      "Batch [60/313], Loss: 0.1318\n",
      "Batch [90/313], Loss: 0.1160\n",
      "Batch [120/313], Loss: 0.1597\n",
      "Batch [150/313], Loss: 0.1460\n",
      "Batch [180/313], Loss: 0.1510\n",
      "Batch [210/313], Loss: 0.1967\n",
      "Batch [240/313], Loss: 0.0961\n",
      "Batch [270/313], Loss: 0.2098\n",
      "Batch [300/313], Loss: 0.1657\n",
      "Epoch [15/20], Loss: 0.1576\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.1553\n",
      "Batch [60/313], Loss: 0.1232\n",
      "Batch [90/313], Loss: 0.2000\n",
      "Batch [120/313], Loss: 0.1770\n",
      "Batch [150/313], Loss: 0.0664\n",
      "Batch [180/313], Loss: 0.1272\n",
      "Batch [210/313], Loss: 0.1333\n",
      "Batch [240/313], Loss: 0.1732\n",
      "Batch [270/313], Loss: 0.1071\n",
      "Batch [300/313], Loss: 0.1440\n",
      "Epoch [16/20], Loss: 0.1532\n",
      "Validation loss decreased (0.156046 --> 0.153209).  Saving model ...\n",
      "Batch [30/313], Loss: 0.1255\n",
      "Batch [60/313], Loss: 0.1542\n",
      "Batch [90/313], Loss: 0.1552\n",
      "Batch [120/313], Loss: 0.1261\n",
      "Batch [150/313], Loss: 0.1996\n",
      "Batch [180/313], Loss: 0.1028\n",
      "Batch [210/313], Loss: 0.1836\n",
      "Batch [240/313], Loss: 0.1054\n",
      "Batch [270/313], Loss: 0.1084\n",
      "Batch [300/313], Loss: 0.0900\n",
      "Epoch [17/20], Loss: 0.1483\n",
      "Validation loss decreased (0.153209 --> 0.148347).  Saving model ...\n",
      "Batch [30/313], Loss: 0.1789\n",
      "Batch [60/313], Loss: 0.1431\n",
      "Batch [90/313], Loss: 0.0985\n",
      "Batch [120/313], Loss: 0.1943\n",
      "Batch [150/313], Loss: 0.1477\n",
      "Batch [180/313], Loss: 0.1065\n",
      "Batch [210/313], Loss: 0.1062\n",
      "Batch [240/313], Loss: 0.0881\n",
      "Batch [270/313], Loss: 0.1749\n",
      "Batch [300/313], Loss: 0.1730\n",
      "Epoch [18/20], Loss: 0.1473\n",
      "Validation loss decreased (0.148347 --> 0.147251).  Saving model ...\n",
      "Batch [30/313], Loss: 0.1436\n",
      "Batch [60/313], Loss: 0.1592\n",
      "Batch [90/313], Loss: 0.1482\n",
      "Batch [120/313], Loss: 0.1133\n",
      "Batch [150/313], Loss: 0.1927\n",
      "Batch [180/313], Loss: 0.0698\n",
      "Batch [210/313], Loss: 0.1405\n",
      "Batch [240/313], Loss: 0.1327\n",
      "Batch [270/313], Loss: 0.1542\n",
      "Batch [300/313], Loss: 0.0845\n",
      "Epoch [19/20], Loss: 0.1506\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.1036\n",
      "Batch [60/313], Loss: 0.1518\n",
      "Batch [90/313], Loss: 0.1898\n",
      "Batch [120/313], Loss: 0.0929\n",
      "Batch [150/313], Loss: 0.1689\n",
      "Batch [180/313], Loss: 0.1480\n",
      "Batch [210/313], Loss: 0.1236\n",
      "Batch [240/313], Loss: 0.1179\n",
      "Batch [270/313], Loss: 0.1609\n",
      "Batch [300/313], Loss: 0.1372\n",
      "Epoch [20/20], Loss: 0.1557\n",
      "EarlyStopping counter: 2 out of 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████▏              | 12/20 [5:23:49<3:38:41, 1640.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/20]   Loss: 3.8292   Accuracy: 38.30%\n",
      "Batch [30/313], Loss: 0.1222\n",
      "Batch [60/313], Loss: 0.1129\n",
      "Batch [90/313], Loss: 0.1651\n",
      "Batch [120/313], Loss: 0.0627\n",
      "Batch [150/313], Loss: 0.1268\n",
      "Batch [180/313], Loss: 0.1073\n",
      "Batch [210/313], Loss: 0.2135\n",
      "Batch [240/313], Loss: 0.1061\n",
      "Batch [270/313], Loss: 0.1536\n",
      "Batch [300/313], Loss: 0.1576\n",
      "Epoch [1/20], Loss: 0.1467\n",
      "Validation loss decreased (0.147251 --> 0.146691).  Saving model ...\n",
      "Batch [30/313], Loss: 0.0711\n",
      "Batch [60/313], Loss: 0.1556\n",
      "Batch [90/313], Loss: 0.1686\n",
      "Batch [120/313], Loss: 0.1744\n",
      "Batch [150/313], Loss: 0.1115\n",
      "Batch [180/313], Loss: 0.1591\n",
      "Batch [210/313], Loss: 0.2943\n",
      "Batch [240/313], Loss: 0.1607\n",
      "Batch [270/313], Loss: 0.0586\n",
      "Batch [300/313], Loss: 0.1673\n",
      "Epoch [2/20], Loss: 0.1447\n",
      "Validation loss decreased (0.146691 --> 0.144736).  Saving model ...\n",
      "Batch [30/313], Loss: 0.0965\n",
      "Batch [60/313], Loss: 0.1120\n",
      "Batch [90/313], Loss: 0.0763\n",
      "Batch [120/313], Loss: 0.1504\n",
      "Batch [150/313], Loss: 0.1550\n",
      "Batch [180/313], Loss: 0.1204\n",
      "Batch [210/313], Loss: 0.1372\n",
      "Batch [240/313], Loss: 0.1459\n",
      "Batch [270/313], Loss: 0.1435\n",
      "Batch [300/313], Loss: 0.0699\n",
      "Epoch [3/20], Loss: 0.1346\n",
      "Validation loss decreased (0.144736 --> 0.134610).  Saving model ...\n",
      "Batch [30/313], Loss: 0.1069\n",
      "Batch [60/313], Loss: 0.0658\n",
      "Batch [90/313], Loss: 0.0842\n",
      "Batch [120/313], Loss: 0.1441\n",
      "Batch [150/313], Loss: 0.1619\n",
      "Batch [180/313], Loss: 0.0814\n",
      "Batch [210/313], Loss: 0.1309\n",
      "Batch [240/313], Loss: 0.1827\n",
      "Batch [270/313], Loss: 0.2611\n",
      "Batch [300/313], Loss: 0.1938\n",
      "Epoch [4/20], Loss: 0.1469\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Batch [30/313], Loss: 0.1351\n",
      "Batch [60/313], Loss: 0.2087\n",
      "Batch [90/313], Loss: 0.1388\n",
      "Batch [120/313], Loss: 0.1689\n",
      "Batch [150/313], Loss: 0.0969\n",
      "Batch [180/313], Loss: 0.2215\n",
      "Batch [210/313], Loss: 0.1022\n",
      "Batch [240/313], Loss: 0.1642\n",
      "Batch [270/313], Loss: 0.1804\n",
      "Batch [300/313], Loss: 0.1013\n",
      "Epoch [5/20], Loss: 0.1440\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Batch [30/313], Loss: 0.0949\n",
      "Batch [60/313], Loss: 0.1179\n",
      "Batch [90/313], Loss: 0.0803\n",
      "Batch [120/313], Loss: 0.1123\n",
      "Batch [150/313], Loss: 0.1654\n",
      "Batch [180/313], Loss: 0.1856\n",
      "Batch [210/313], Loss: 0.1666\n",
      "Batch [240/313], Loss: 0.1638\n",
      "Batch [270/313], Loss: 0.1738\n",
      "Batch [300/313], Loss: 0.0967\n",
      "Epoch [6/20], Loss: 0.1404\n",
      "EarlyStopping counter: 3 out of 5\n",
      "Batch [30/313], Loss: 0.1573\n",
      "Batch [60/313], Loss: 0.1245\n",
      "Batch [90/313], Loss: 0.1126\n",
      "Batch [120/313], Loss: 0.1703\n",
      "Batch [150/313], Loss: 0.1690\n",
      "Batch [180/313], Loss: 0.0832\n",
      "Batch [210/313], Loss: 0.1339\n",
      "Batch [240/313], Loss: 0.1181\n",
      "Batch [270/313], Loss: 0.1646\n",
      "Batch [300/313], Loss: 0.1361\n",
      "Epoch [7/20], Loss: 0.1409\n",
      "EarlyStopping counter: 4 out of 5\n",
      "Batch [30/313], Loss: 0.0993\n",
      "Batch [60/313], Loss: 0.1408\n",
      "Batch [90/313], Loss: 0.1504\n",
      "Batch [120/313], Loss: 0.1170\n",
      "Batch [150/313], Loss: 0.1268\n",
      "Batch [180/313], Loss: 0.1701\n",
      "Batch [210/313], Loss: 0.0938\n",
      "Batch [240/313], Loss: 0.0953\n",
      "Batch [270/313], Loss: 0.1115\n",
      "Batch [300/313], Loss: 0.1833\n",
      "Epoch [8/20], Loss: 0.1370\n",
      "EarlyStopping counter: 5 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|████████████████████████             | 13/20 [5:34:46<2:36:37, 1342.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/20]   Loss: 3.8217   Accuracy: 39.21%\n",
      "Batch [30/313], Loss: 0.0782\n",
      "Batch [60/313], Loss: 0.1285\n",
      "Batch [90/313], Loss: 0.1492\n",
      "Batch [120/313], Loss: 0.1418\n",
      "Batch [150/313], Loss: 0.1693\n",
      "Batch [180/313], Loss: 0.2058\n",
      "Batch [210/313], Loss: 0.2146\n",
      "Batch [240/313], Loss: 0.1258\n",
      "Batch [270/313], Loss: 0.1151\n",
      "Batch [300/313], Loss: 0.1466\n",
      "Epoch [1/20], Loss: 0.1370\n",
      "EarlyStopping counter: 6 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████▌           | 14/20 [5:36:10<1:36:15, 962.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/20]   Loss: 3.8288   Accuracy: 39.39%\n",
      "Batch [30/313], Loss: 0.1375\n",
      "Batch [60/313], Loss: 0.0598\n",
      "Batch [90/313], Loss: 0.1459\n",
      "Batch [120/313], Loss: 0.1285\n",
      "Batch [150/313], Loss: 0.0773\n",
      "Batch [180/313], Loss: 0.0824\n",
      "Batch [210/313], Loss: 0.1330\n",
      "Batch [240/313], Loss: 0.1809\n",
      "Batch [270/313], Loss: 0.2017\n",
      "Batch [300/313], Loss: 0.1414\n",
      "Epoch [1/20], Loss: 0.1332\n",
      "Validation loss decreased (0.134610 --> 0.133232).  Saving model ...\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|██████████████████████████████          | 15/20 [5:37:35<58:09, 697.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/20]   Loss: 3.8648   Accuracy: 38.74%\n",
      "Batch [30/313], Loss: 0.1389\n",
      "Batch [60/313], Loss: 0.1287\n",
      "Batch [90/313], Loss: 0.0929\n",
      "Batch [120/313], Loss: 0.0723\n",
      "Batch [150/313], Loss: 0.1991\n",
      "Batch [180/313], Loss: 0.2438\n",
      "Batch [210/313], Loss: 0.2098\n",
      "Batch [240/313], Loss: 0.1451\n",
      "Batch [270/313], Loss: 0.1318\n",
      "Batch [300/313], Loss: 0.1851\n",
      "Epoch [1/20], Loss: 0.1384\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████████████████████████████        | 16/20 [5:39:00<34:13, 513.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/20]   Loss: 3.8202   Accuracy: 39.52%\n",
      "Batch [30/313], Loss: 0.1008\n",
      "Batch [60/313], Loss: 0.0939\n",
      "Batch [90/313], Loss: 0.2563\n",
      "Batch [120/313], Loss: 0.1023\n",
      "Batch [150/313], Loss: 0.0992\n",
      "Batch [180/313], Loss: 0.1445\n",
      "Batch [210/313], Loss: 0.0864\n",
      "Batch [240/313], Loss: 0.0701\n",
      "Batch [270/313], Loss: 0.1068\n",
      "Batch [300/313], Loss: 0.2649\n",
      "Epoch [1/20], Loss: 0.1272\n",
      "Validation loss decreased (0.133232 --> 0.127155).  Saving model ...\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|██████████████████████████████████      | 17/20 [5:40:25<19:13, 384.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/20]   Loss: 3.8331   Accuracy: 39.28%\n",
      "Batch [30/313], Loss: 0.1753\n",
      "Batch [60/313], Loss: 0.1456\n",
      "Batch [90/313], Loss: 0.0973\n",
      "Batch [120/313], Loss: 0.1081\n",
      "Batch [150/313], Loss: 0.1312\n",
      "Batch [180/313], Loss: 0.1315\n",
      "Batch [210/313], Loss: 0.1312\n",
      "Batch [240/313], Loss: 0.1354\n",
      "Batch [270/313], Loss: 0.1773\n",
      "Batch [300/313], Loss: 0.1029\n",
      "Epoch [1/20], Loss: 0.1312\n",
      "EarlyStopping counter: 1 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████████████████████████████████    | 18/20 [5:41:50<09:48, 294.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/20]   Loss: 3.8154   Accuracy: 39.46%\n",
      "Batch [30/313], Loss: 0.0997\n",
      "Batch [60/313], Loss: 0.1699\n",
      "Batch [90/313], Loss: 0.1385\n",
      "Batch [120/313], Loss: 0.0623\n",
      "Batch [150/313], Loss: 0.1387\n",
      "Batch [180/313], Loss: 0.0818\n",
      "Batch [210/313], Loss: 0.1232\n",
      "Batch [240/313], Loss: 0.1408\n",
      "Batch [270/313], Loss: 0.1259\n",
      "Batch [300/313], Loss: 0.1557\n",
      "Epoch [1/20], Loss: 0.1283\n",
      "EarlyStopping counter: 2 out of 5\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|██████████████████████████████████████  | 19/20 [5:43:14<03:51, 231.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/20]   Loss: 3.8482   Accuracy: 38.86%\n",
      "Batch [30/313], Loss: 0.2397\n",
      "Batch [60/313], Loss: 0.1151\n",
      "Batch [90/313], Loss: 0.1351\n",
      "Batch [120/313], Loss: 0.1264\n",
      "Batch [150/313], Loss: 0.1543\n",
      "Batch [180/313], Loss: 0.0960\n",
      "Batch [210/313], Loss: 0.0780\n",
      "Batch [240/313], Loss: 0.1369\n",
      "Batch [270/313], Loss: 0.0656\n",
      "Batch [300/313], Loss: 0.1221\n",
      "Epoch [1/20], Loss: 0.1252\n",
      "Validation loss decreased (0.127155 --> 0.125208).  Saving model ...\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 20/20 [5:44:39<00:00, 1033.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/20]   Loss: 3.8599   Accuracy: 39.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Per-Epoch Activity 코드\n",
    "for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "    # 모델 학습\n",
    "    train_model(net, train_loader, criterion, optimizer, num_epochs=num_epochs)\n",
    "    \n",
    "    # 테스트 평가\n",
    "    test_loss, test_accuracy = test_model(net, test_loader, criterion, epoch)\n",
    "    \n",
    "    # TensorBoard에 테스트 결과 기록\n",
    "    writer.add_scalar(\"Test Loss\", test_loss, epoch)\n",
    "    writer.add_scalar(\"Test Accuracy\", test_accuracy, epoch)\n",
    "\n",
    "    # 현재 epoch 결과 출력\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}]   Loss: {test_loss:.4f}   Accuracy: {test_accuracy*100:.2f}%\")\n",
    "\n",
    "# TensorBoard writer 닫기\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6271219f-96f2-4967-939d-a24d6def8eb0",
   "metadata": {},
   "source": [
    "### **Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17f94ca2-f1e7-4452-b501-9b36e7025a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Result of ResNet = Epoch : 20   Loss : 3.859919033432007   Accuracy : 0.39\n"
     ]
    }
   ],
   "source": [
    "print(f\" Result of ResNet = Epoch : {epoch}   Loss : {test_loss}   Accuracy : {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb5d42f-9728-4e40-94eb-997ad195b5b2",
   "metadata": {},
   "source": [
    "# Test - 나영(Accuracy) 현욱(Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c59d485-f2bb-4dc0-85ac-55a5f7ef1289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000개 테스트 이미지에서 모델 정확도: 39 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('10000개 테스트 이미지에서 모델 정확도: %d %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b41eb1-d372-4e64-b1d9-d2552ed44d99",
   "metadata": {},
   "source": [
    "**Visualization of average loss(수정 필요)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ea3df5-03bd-4f43-8b2b-d251da3ea733",
   "metadata": {},
   "source": [
    "**Top-1 Accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ece3fe-506f-4fda-86bd-b7f32112a23d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b0993a9-bc4f-40e1-909c-c63d6c7dd36e",
   "metadata": {},
   "source": [
    "**Top-5 Accuracy**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6503fa03-bac6-4758-a623-8cec95a40985",
   "metadata": {},
   "source": [
    "# Data Analysis - 현욱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1321873-e871-4ee1-883e-637fe9aa843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=./runs/resnet_18/tensorboard --port=8202 --host=0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cce3873-fb7d-447b-9e9e-2c2c68a8320f",
   "metadata": {},
   "source": [
    "### **Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459fcbfd-53ee-4489-a204-c366745ff4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = train_data.classes\n",
    "coarse_classes = [\n",
    "    'aquatic mammals', 'fish', 'flowers', 'food containers', 'fruit and vegetables', 'household electrical devices', \n",
    "    'household furniture', 'insects', 'large carnivores', 'large man-made outdoor things', \n",
    "    'large natural outdoor scenes', 'large omnivores and herbivores', 'medium-sized mammals', \n",
    "    'non-insect invertebrates', 'people', 'reptiles', 'small mammals', 'trees', 'vehicles 1', 'vehicles 2'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12998ea-5e92-47e9-8f07-5506b467b682",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(classes), len(coarse_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65824d7-6382-46c5-a654-ce9c579aaee2",
   "metadata": {},
   "source": [
    "##### **Fine_to_coarse_mapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22925779-77ff-457d-bac1-1daa7f1746ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CIFAR-100 세부 클래스(fine classes)와 상위 클래스(coarse classes) 매핑\n",
    "fine_to_coarse_mapping = {\n",
    "    # aquatic mammals\n",
    "    'beaver': 'aquatic mammals',\n",
    "    'dolphin': 'aquatic mammals',\n",
    "    'otter': 'aquatic mammals',\n",
    "    'seal': 'aquatic mammals',\n",
    "    'whale': 'aquatic mammals',\n",
    "    \n",
    "    # fish\n",
    "    'aquarium fish': 'fish',\n",
    "    'flatfish': 'fish',\n",
    "    'ray': 'fish',\n",
    "    'shark': 'fish',\n",
    "    'trout': 'fish',\n",
    "    \n",
    "    # flowers\n",
    "    'orchids': 'flowers',\n",
    "    'poppies': 'flowers',\n",
    "    'roses': 'flowers',\n",
    "    'sunflowers': 'flowers',\n",
    "    'tulips': 'flowers',\n",
    "    \n",
    "    # food containers\n",
    "    'bottles': 'food containers',\n",
    "    'bowls': 'food containers',\n",
    "    'cans': 'food containers',\n",
    "    'cups': 'food containers',\n",
    "    'plates': 'food containers',\n",
    "    \n",
    "    # fruit and vegetables\n",
    "    'apples': 'fruit and vegetables',\n",
    "    'mushrooms': 'fruit and vegetables',\n",
    "    'oranges': 'fruit and vegetables',\n",
    "    'pears': 'fruit and vegetables',\n",
    "    'sweet peppers': 'fruit and vegetables',\n",
    "    \n",
    "    # household electrical devices\n",
    "    'clock': 'household electrical devices',\n",
    "    'computer keyboard': 'household electrical devices',\n",
    "    'lamp': 'household electrical devices',\n",
    "    'telephone': 'household electrical devices',\n",
    "    'television': 'household electrical devices',\n",
    "    \n",
    "    # household furniture\n",
    "    'bed': 'household furniture',\n",
    "    'chair': 'household furniture',\n",
    "    'couch': 'household furniture',\n",
    "    'table': 'household furniture',\n",
    "    'wardrobe': 'household furniture',\n",
    "    \n",
    "    # insects\n",
    "    'bee': 'insects',\n",
    "    'beetle': 'insects',\n",
    "    'butterfly': 'insects',\n",
    "    'caterpillar': 'insects',\n",
    "    'cockroach': 'insects',\n",
    "    \n",
    "    # large carnivores\n",
    "    'bear': 'large carnivores',\n",
    "    'leopard': 'large carnivores',\n",
    "    'lion': 'large carnivores',\n",
    "    'tiger': 'large carnivores',\n",
    "    'wolf': 'large carnivores',\n",
    "    \n",
    "    # large man-made outdoor things\n",
    "    'bridge': 'large man-made outdoor things',\n",
    "    'castle': 'large man-made outdoor things',\n",
    "    'house': 'large man-made outdoor things',\n",
    "    'road': 'large man-made outdoor things',\n",
    "    'skyscraper': 'large man-made outdoor things',\n",
    "    \n",
    "    # large natural outdoor scenes\n",
    "    'cloud': 'large natural outdoor scenes',\n",
    "    'forest': 'large natural outdoor scenes',\n",
    "    'mountain': 'large natural outdoor scenes',\n",
    "    'plain': 'large natural outdoor scenes',\n",
    "    'sea': 'large natural outdoor scenes',\n",
    "    \n",
    "    # large omnivores and herbivores\n",
    "    'camel': 'large omnivores and herbivores',\n",
    "    'cattle': 'large omnivores and herbivores',\n",
    "    'chimpanzee': 'large omnivores and herbivores',\n",
    "    'elephant': 'large omnivores and herbivores',\n",
    "    'kangaroo': 'large omnivores and herbivores',\n",
    "    \n",
    "    # medium-sized mammals\n",
    "    'fox': 'medium-sized mammals',\n",
    "    'porcupine': 'medium-sized mammals',\n",
    "    'possum': 'medium-sized mammals',\n",
    "    'raccoon': 'medium-sized mammals',\n",
    "    'skunk': 'medium-sized mammals',\n",
    "    \n",
    "    # non-insect invertebrates\n",
    "    'crab': 'non-insect invertebrates',\n",
    "    'lobster': 'non-insect invertebrates',\n",
    "    'snail': 'non-insect invertebrates',\n",
    "    'spider': 'non-insect invertebrates',\n",
    "    'worm': 'non-insect invertebrates',\n",
    "    \n",
    "    # people\n",
    "    'baby': 'people',\n",
    "    'boy': 'people',\n",
    "    'girl': 'people',\n",
    "    'man': 'people',\n",
    "    'woman': 'people',\n",
    "    \n",
    "    # reptiles\n",
    "    'crocodile': 'reptiles',\n",
    "    'dinosaur': 'reptiles',\n",
    "    'lizard': 'reptiles',\n",
    "    'snake': 'reptiles',\n",
    "    'turtle': 'reptiles',\n",
    "    \n",
    "    # small mammals\n",
    "    'hamster': 'small mammals',\n",
    "    'mouse': 'small mammals',\n",
    "    'rabbit': 'small mammals',\n",
    "    'shrew': 'small mammals',\n",
    "    'squirrel': 'small mammals',\n",
    "    \n",
    "    # trees\n",
    "    'maple': 'trees',\n",
    "    'oak': 'trees',\n",
    "    'palm': 'trees',\n",
    "    'pine': 'trees',\n",
    "    'willow': 'trees',\n",
    "    \n",
    "    # vehicles 1\n",
    "    'bicycle': 'vehicles 1',\n",
    "    'bus': 'vehicles 1',\n",
    "    'motorcycle': 'vehicles 1',\n",
    "    'pickup truck': 'vehicles 1',\n",
    "    'train': 'vehicles 1',\n",
    "    \n",
    "    # vehicles 2\n",
    "    'lawn-mower': 'vehicles 2',\n",
    "    'rocket': 'vehicles 2',\n",
    "    'streetcar': 'vehicles 2',\n",
    "    'tank': 'vehicles 2',\n",
    "    'tractor': 'vehicles 2'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defd13fc-9e00-4f14-8ce4-c568bf583a0b",
   "metadata": {},
   "source": [
    "### **Confusion Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08932ed-cd47-4196-ae9c-187bd20f02a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = []\n",
    "# y_true = []\n",
    "\n",
    "# # iterate over test data\n",
    "# for x, y in torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size):\n",
    "    \n",
    "#     #print('iter val', i)\n",
    "#     x = x.to(device)\n",
    "#     y = y.to(device)\n",
    "#     z = net(x)\n",
    "#     _, yhat = torch.max(z, 1)\n",
    "#     pred = yhat.data.cpu().numpy()\n",
    "#     y_pred.extend(pred) # Save Prediction\n",
    "\n",
    "#     labels = y.data.cpu().numpy()\n",
    "#     y_true.extend(labels) # Save Truth\n",
    "\n",
    "# # Build confusion matrix\n",
    "# cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "# df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) *10, index = [i for i in classes],\n",
    "#                      columns = [i for i in classes])\n",
    "# plt.figure(figsize = (128,70))\n",
    "# sns.heatmap(df_cm, annot=True)\n",
    "# plt.title('Confusion Matrix of ResNet (CIFAR100)')\n",
    "# plt.savefig('./runs/resnet_18/Confusion_matrix_ResNet_Cifar100.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bdc8f9-0c1e-4787-aaea-0f00e2fa18b3",
   "metadata": {},
   "source": [
    "### **Confusion Matrix - Coarse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773340a7-fea7-41e1-87cf-b57e6bb8a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over test data\n",
    "for x, y in torch.utils.data.DataLoader(dataset=test_data, batch_size=batch_size):\n",
    "    x = x.to(device)\n",
    "    y = y.to(device)\n",
    "    z = net(x)\n",
    "    _, yhat = torch.max(z, 1)\n",
    "    \n",
    "    # Save Prediction and convert to superclasses\n",
    "    pred = yhat.data.cpu().numpy()\n",
    "    super_pred = [fine_to_superclass[p] for p in pred]\n",
    "    y_pred.extend(super_pred)\n",
    "    \n",
    "    # Save Truth and convert to superclasses\n",
    "    labels = y.data.cpu().numpy()\n",
    "    super_labels = [fine_to_superclass[l] for l in labels]\n",
    "    y_true.extend(super_labels)\n",
    "\n",
    "# Build confusion matrix for superclasses\n",
    "cf_matrix = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "\n",
    "# Normalize confusion matrix\n",
    "df_cm = pd.DataFrame(cf_matrix, index=coarse_classes,\n",
    "                     columns=coarse_classes)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(df_cm, annot=True, fmt='.2f', cmap='Blues')\n",
    "plt.title('Confusion Matrix of ResNet (CIFAR-100 Superclass)')\n",
    "plt.savefig('./runs/resnet_18/Confusion_matrix_ResNet_Cifar100_superclass.jpg')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Real')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be2bacf-9f08-4158-a1f5-388c2b93bd1b",
   "metadata": {},
   "source": [
    "### **Classification Report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdae401-61f1-47fc-a281-fcc45a3be062",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Super Classification Report of ResNet(CIFAR100)  \\n { classification_report(y_true, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e68792b2-ebd9-4531-ae7a-a687a78b0ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish!\n"
     ]
    }
   ],
   "source": [
    "# 모델 가중치를 runs 폴더에 저장\n",
    "torch.save(net.state_dict(), f'runs/efficientnetb0_model.pth')\n",
    "print(\"finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28199a8-2509-482f-bb02-52522a18a903",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
