{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da55e5a3-cdd0-4b19-a405-6d25853d33f9",
   "metadata": {
    "id": "da55e5a3-cdd0-4b19-a405-6d25853d33f9"
   },
   "source": [
    "**Load Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83038a2e-883a-4da6-97bd-c5371f13c182",
   "metadata": {
    "id": "83038a2e-883a-4da6-97bd-c5371f13c182"
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from utility.utils import EarlyStopping, WarmUpLR, most_recent_folder, most_recent_weights, best_acc_weights, last_epoch\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, Subset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "# Analysis\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# import wandb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdc13d1-90ff-4301-8dac-07977aaaddcf",
   "metadata": {
    "id": "bbdc13d1-90ff-4301-8dac-07977aaaddcf"
   },
   "source": [
    "**Seed & Device Setting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8539ad-3c66-4c94-abb2-fcc33f97d3ca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b8539ad-3c66-4c94-abb2-fcc33f97d3ca",
    "outputId": "ed6e73ba-1056-4f62-fc55-15b56acfe3c2"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # GPU 여러 개 사용할 경우 사용\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # 완벽한 재현성을 위해선 False로 하는 게 맞지만 성능의 저하를 일으킬 수 있음\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "# 디바이스 설정\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35b5e51",
   "metadata": {
    "id": "a35b5e51"
   },
   "source": [
    "**Wandb Setting & Set Hyperparameters (수정 구간)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ce35ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "id": "d5ce35ad",
    "outputId": "26c52ce7-bcef-496e-fd7d-d9ea23426c27"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\" : 128,\n",
    "    \"num_epochs\" : 250,\n",
    "    \"learning_rate\" : 0.1,\n",
    "    \"momentum\" : 0.9,\n",
    "    \"weight_decay\" : 5e-4,\n",
    "    \"nesterov\" : True,\n",
    "    \"gamma\" : 0.2,\n",
    "    \"warm\" : 1,\n",
    "    \"patience\" : 150,\n",
    "    \"plateau_patience\" : 15,\n",
    "    \"milestones\" : [100, 150, 200],\n",
    "    \"pin_memory\" : True,\n",
    "    \"depth\" : 110,\n",
    "    \"alpha\" : 270,\n",
    "    \"beta\" : 1.0,\n",
    "\n",
    "    \"resume\" : False,\n",
    "    \"model_name\" : \"resnet_18_cutmix\",\n",
    "    \"stratified_data\" : True,\n",
    "    \"scheduler\" : \"MultiStepLR\",\n",
    "    \"train_50000\" : True\n",
    "}\n",
    "if config[\"resume\"] == True:\n",
    "    config[\"warm\"] = 0\n",
    "model_name = config[\"model_name\"]\n",
    "# wandb.init(project=\"CIFAR-100_Classification\", name=config[\"model_name\"], config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8d1e41f",
   "metadata": {
    "id": "a8d1e41f"
   },
   "outputs": [],
   "source": [
    "fine_to_superclass = {\n",
    "    4: 0, 30: 0, 55: 0, 72: 0, 95: 0,       # aquatic mammals\n",
    "    1: 1, 32: 1, 67: 1, 73: 1, 91: 1,       # fish\n",
    "    54: 2, 62: 2, 70: 2, 82: 2, 92: 2,      # flowers\n",
    "    9: 3, 10: 3, 16: 3, 28: 3, 61: 3,       # food containers\n",
    "    0: 4, 51: 4, 53: 4, 57: 4, 83: 4,       # fruit and vegetables\n",
    "    22: 5, 39: 5, 40: 5, 86: 5, 87: 5,      # household electrical devices\n",
    "    5: 6, 20: 6, 25: 6, 84: 6, 94: 6,       # household furniture\n",
    "    6: 7, 7: 7, 14: 7, 18: 7, 24: 7,        # insects\n",
    "    3: 8, 42: 8, 43: 8, 88: 8, 97: 8,       # large carnivores\n",
    "    12: 9, 17: 9, 37: 9, 68: 9, 76: 9,      # large man-made outdoor things\n",
    "    23: 10, 33: 10, 49: 10, 60: 10, 71: 10, # large natural outdoor scenes\n",
    "    15: 11, 19: 11, 21: 11, 31: 11, 38: 11, # large omnivores and herbivores\n",
    "    34: 12, 63: 12, 64: 12, 66: 12, 75: 12, # medium-sized mammals\n",
    "    26: 13, 45: 13, 77: 13, 79: 13, 99: 13, # non-insect invertebrates\n",
    "    2: 14, 11: 14, 35: 14, 46: 14, 98: 14,  # people\n",
    "    27: 15, 29: 15, 44: 15, 78: 15, 93: 15, # reptiles\n",
    "    36: 16, 50: 16, 65: 16, 74: 16, 80: 16, # small mammals\n",
    "    47: 17, 52: 17, 56: 17, 59: 17, 96: 17, # trees\n",
    "    8: 18, 13: 18, 48: 18, 58: 18, 90: 18,  # vehicles 1\n",
    "    41: 19, 69: 19, 81: 19, 85: 19, 89: 19  # vehicles 2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e04e253-49c0-4c37-8260-5e02f2c09452",
   "metadata": {
    "id": "3e04e253-49c0-4c37-8260-5e02f2c09452"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZKl6oKBffJYA",
   "metadata": {
    "id": "ZKl6oKBffJYA"
   },
   "source": [
    "**Data Augementation (수정 구간)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93758c0c-b1d2-4ab9-afb4-bdd60a04dbd3",
   "metadata": {
    "id": "93758c0c-b1d2-4ab9-afb4-bdd60a04dbd3"
   },
   "outputs": [],
   "source": [
    "CIFAR100_TRAIN_MEAN = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
    "CIFAR100_TRAIN_STD = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
    "\n",
    "train_val_transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(CIFAR100_TRAIN_MEAN, CIFAR100_TRAIN_STD)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ade0a7-66ce-48f4-96b0-f4719beaf090",
   "metadata": {
    "id": "13ade0a7-66ce-48f4-96b0-f4719beaf090"
   },
   "source": [
    "**Splitting the training data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b4a6e5-9118-449a-bfb8-d05f9d339304",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09b4a6e5-9118-449a-bfb8-d05f9d339304",
    "outputId": "f8325035-cd21-4715-b7a6-1e0d8d0dfcd5"
   },
   "outputs": [],
   "source": [
    "train_val_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=train_val_transform)\n",
    "val_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=test_transform)\n",
    "test_data = datasets.CIFAR100(root='./data', train=False, download=True, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82c9ab44-2b89-4d17-b90d-f42d1e8fe426",
   "metadata": {
    "id": "82c9ab44-2b89-4d17-b90d-f42d1e8fe426"
   },
   "outputs": [],
   "source": [
    "# Stratified 방식으로 train 데이터를 train/val로 나누기\n",
    "if config[\"train_50000\"]:\n",
    "   pass\n",
    "elif config[\"stratified_data\"]:\n",
    "  labels = train_val_data.targets\n",
    "  stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "\n",
    "  for train_index, val_index in stratified_split.split(train_val_data.data, labels):\n",
    "      train_data = Subset(train_val_data, train_index)\n",
    "      val_data = Subset(val_data, val_index)\n",
    "else:\n",
    "  # RandomSampler 방식으로 train 데이터를 train/val로 나누기\n",
    "  num_train = len(train_val_data)\n",
    "  indices = list(range(num_train))\n",
    "  split = int(np.floor(0.2 * num_train))  # validation 데이터를 20%로 설정\n",
    "\n",
    "  np.random.shuffle(indices)\n",
    "  train_idx, val_idx = indices[split:], indices[:split]\n",
    "\n",
    "  train_sampler = SubsetRandomSampler(train_idx)\n",
    "  val_sampler = SubsetRandomSampler(val_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce676124-c073-4aec-a000-cb061ac3e10a",
   "metadata": {
    "id": "ce676124-c073-4aec-a000-cb061ac3e10a"
   },
   "source": [
    "**Define Dataloader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "824ddf7d-d749-456a-b717-e257489431b9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "824ddf7d-d749-456a-b717-e257489431b9",
    "outputId": "8bfd89af-a57a-441c-9a14-d549ff2ef41d"
   },
   "outputs": [],
   "source": [
    "if config[\"train_50000\"]:\n",
    "  train_loader = DataLoader(train_val_data, batch_size=config[\"batch_size\"], shuffle=True, num_workers=4, pin_memory=config[\"pin_memory\"])\n",
    "  test_loader = DataLoader(test_data, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=config[\"pin_memory\"])\n",
    "elif config[\"stratified_data\"]:\n",
    "  train_loader = DataLoader(train_data, batch_size=config[\"batch_size\"], shuffle=True, num_workers=4, pin_memory=config[\"pin_memory\"])\n",
    "  val_loader = DataLoader(val_data, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=config[\"pin_memory\"])\n",
    "  test_loader = DataLoader(test_data, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=config[\"pin_memory\"])\n",
    "else:\n",
    "  train_loader = DataLoader(train_val_data, batch_size=config[\"batch_size\"], sampler=train_sampler, num_workers=4, pin_memory=config[\"pin_memory\"])\n",
    "  val_loader = DataLoader(train_val_data, batch_size=config[\"batch_size\"], shuffle=False, sampler=val_sampler, num_workers=4, pin_memory=config[\"pin_memory\"])\n",
    "  test_loader = DataLoader(test_data, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=config[\"pin_memory\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e300067-8688-4cb1-b34b-a9dcd504c860",
   "metadata": {
    "id": "0e300067-8688-4cb1-b34b-a9dcd504c860"
   },
   "source": [
    "**Model Initialize (수정 부분)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a58ae19-69f4-49a5-b704-d878caa5d7c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a58ae19-69f4-49a5-b704-d878caa5d7c7",
    "outputId": "70f5d330-681b-4ffd-de0b-16232491cdc4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from models import resnet, shake_pyramidnet\n",
    "\n",
    "print(\"use:\", device)\n",
    "\n",
    "# 모델 초기화\n",
    "model = resnet.resnet18()\n",
    "net = model\n",
    "# 모델을 GPU로 이동\n",
    "net.to(device)\n",
    "\n",
    "# print(summary(net, (3,224,224)))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=config[\"learning_rate\"], momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"], nesterov=config[\"nesterov\"])\n",
    "\n",
    "if config[\"scheduler\"] == \"MultiStepLR\":\n",
    "    train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, config[\"milestones\"], gamma=config[\"gamma\"]) #learning rate decay\n",
    "elif config[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "    train_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", factor=config[\"gamma\"], patience=config[\"plateau_patience\"])\n",
    "else:\n",
    "    print(\"No scheduler!!\")\n",
    "iter_per_epoch = len(train_loader)\n",
    "warmup_scheduler = WarmUpLR(optimizer, iter_per_epoch * config[\"warm\"])\n",
    "early_stopping = EarlyStopping(patience=config[\"patience\"], verbose=True)\n",
    "\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c1329699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int8(W * cut_rat)\n",
    "    cut_h = np.int8(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b49f37a-e104-42df-a9a6-6ba473a9881f",
   "metadata": {
    "id": "8b49f37a-e104-42df-a9a6-6ba473a9881f"
   },
   "source": [
    "**Model Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96ba42f4-6da7-4f0c-983b-b8b82da34879",
   "metadata": {
    "id": "96ba42f4-6da7-4f0c-983b-b8b82da34879"
   },
   "outputs": [],
   "source": [
    "# 모델 학습 함수\n",
    "def train_model(net, trainloader, criterion, optimizer, epoch):\n",
    "    net.train()\n",
    "    start = time.time()\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    superclass_correct = 0\n",
    "    superclass_total = 0\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (inputs, labels) in enumerate(trainloader):\n",
    "        labels, inputs = labels.to(device), inputs.to(device)\n",
    "\n",
    "        r = np.random.rand(1)\n",
    "        if config[\"beta\"] > 0 and r < 0.5:\n",
    "            lam = np.random.beta(config[\"beta\"], config[\"beta\"])\n",
    "            rand_index = torch.randperm(inputs.size()[0]).cuda()\n",
    "            target_a = labels\n",
    "            target_b = labels[rand_index]\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(inputs.size(), lam)\n",
    "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "\n",
    "            lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (inputs.size()[-1] * inputs.size()[-2]))\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, target_a) * lam + criterion(outputs, target_b) * (1. - lam)\n",
    "        else:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        top1_correct += torch.sum(preds == labels).item()\n",
    "\n",
    "        _, top5_preds = outputs.topk(5, 1, True, True)\n",
    "        top5_correct += torch.sum(top5_preds.eq(labels.view(-1, 1).expand_as(top5_preds))).item()\n",
    "\n",
    "        total += labels.size(0)\n",
    "\n",
    "        # Superclass accuracy\n",
    "        super_preds = torch.tensor([fine_to_superclass[p.item()] for p in preds], dtype=torch.long)\n",
    "        super_labels = torch.tensor([fine_to_superclass[t.item()] for t in labels], dtype=torch.long)\n",
    "        superclass_correct += torch.sum(super_preds == super_labels).item()\n",
    "        superclass_total += super_labels.size(0)\n",
    "\n",
    "        # 역전파 및 최적화\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # 30번째 배치마다 상태 출력\n",
    "        if (batch_idx + 1) % 30 == 0:\n",
    "            print(f\"Batch [{batch_idx+1}/{len(trainloader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # LR에 warmup 되는 게 안 보이지만 실제론 이루어지고 있음. batch로 돌 때 이루어짐.\n",
    "        if epoch <= config[\"warm\"]:\n",
    "            warmup_scheduler.step()\n",
    "\n",
    "\n",
    "    # Epoch당 평균 손실 계산 및 출력\n",
    "    epoch_loss = running_loss / total\n",
    "\n",
    "    finish = time.time()\n",
    "\n",
    "    # 세부 클래스 및 슈퍼 클래스 정확도 계산\n",
    "    top1_acc = top1_correct / total\n",
    "    top5_acc = top5_correct / total\n",
    "    superclass_acc = superclass_correct / superclass_total\n",
    "    accuracy = [top1_acc, top5_acc, superclass_acc]\n",
    "\n",
    "    print(\"Train set: Epoch: {}, Average loss:{:.4f}, LR: {:.6f} Top-1 Accuracy: {:.4f}, Top-5 Accuracy: {:.4f}, SuperClass Accuracy: {:.4f}, Time consumed:{:.2f}s\".format(\n",
    "        epoch,\n",
    "        epoch_loss,\n",
    "        optimizer.param_groups[0]['lr'],\n",
    "        top1_acc,\n",
    "        top5_acc,\n",
    "        superclass_acc,\n",
    "        finish - start\n",
    "    ))\n",
    "\n",
    "    return epoch_loss, accuracy\n",
    "\n",
    "def eval_training(net, testloader, criterion, epoch):\n",
    "    net.eval()\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    superclass_correct = 0\n",
    "    superclass_total = 0\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in testloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            top1_correct += torch.sum(preds == labels).item()\n",
    "\n",
    "            _, top5_preds = outputs.topk(5, 1, True, True)\n",
    "            top5_correct += torch.sum(top5_preds.eq(labels.view(-1, 1).expand_as(top5_preds))).item()\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "            test_loss += criterion(outputs, labels)\n",
    "\n",
    "            # Superclass accuracy\n",
    "            super_preds = torch.tensor([fine_to_superclass[p.item()] for p in preds], dtype=torch.long)\n",
    "            super_labels = torch.tensor([fine_to_superclass[t.item()] for t in labels], dtype=torch.long)\n",
    "            superclass_correct += torch.sum(super_preds == super_labels).item()\n",
    "            superclass_total += super_labels.size(0)\n",
    "\n",
    "    finish = time.time()\n",
    "    average_loss = test_loss / total\n",
    "\n",
    "    # 세부 클래스 및 슈퍼 클래스 정확도 계산\n",
    "    top1_acc = top1_correct / total\n",
    "    top5_acc = top5_correct / total\n",
    "    superclass_acc = superclass_correct / superclass_total\n",
    "    accuracy = [top1_acc, top5_acc, superclass_acc]\n",
    "\n",
    "    print(\"Test set: Epoch: {}, Average loss:{:.4f}, Top-1 Accuracy: {:.4f}, Top-5 Accuracy: {:.4f}, SuperClass Accuracy: {:.4f}, Time consumed:{:.2f}s\".format(\n",
    "        epoch,\n",
    "        average_loss,\n",
    "        top1_acc,\n",
    "        top5_acc,\n",
    "        superclass_acc,\n",
    "        finish - start\n",
    "    ))\n",
    "    print()\n",
    "\n",
    "    return average_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e079b97-a215-4b55-aedd-c1297773426d",
   "metadata": {
    "id": "7e079b97-a215-4b55-aedd-c1297773426d"
   },
   "outputs": [],
   "source": [
    "DATE_FORMAT = '%A_%d_%B_%Y_%Hh_%Mm_%Ss'\n",
    "TIME_NOW = datetime.now().strftime(DATE_FORMAT)\n",
    "\n",
    "if config[\"resume\"]:\n",
    "    recent_folder = most_recent_folder(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\"), DATE_FORMAT)\n",
    "    if not recent_folder:\n",
    "        raise Exception(\"no recent folder were found\")\n",
    "\n",
    "    checkpoint_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder)\n",
    "else:\n",
    "    checkpoint_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", TIME_NOW)\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    os.makedirs(checkpoint_path)\n",
    "checkpoint_path = os.path.join(checkpoint_path, '{net}-{epoch}-{type}.pth')\n",
    "\n",
    "best_acc = [0.0, 0.0, 0.0]\n",
    "sum_best_acc = 0.0\n",
    "epoch = 0\n",
    "weights_path = False\n",
    "\n",
    "if config[\"resume\"]:\n",
    "    best_weights = best_acc_weights(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder))\n",
    "    if best_weights:\n",
    "        weights_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder, best_weights)\n",
    "        print('found best acc weights file:{}'.format(weights_path))\n",
    "        print('load best training file to test acc...')\n",
    "        net.load_state_dict(torch.load(weights_path))\n",
    "        _, best_acc = eval_training(net, test_loader, criterion, epoch)\n",
    "        print(\"best acc is Top-1:{:0.2f} Top-5:{:0.2f} Super:{:0.2f} Total:{:0.2f}\".format(best_acc[0], best_acc[1], best_acc[2], sum(best_acc)))\n",
    "\n",
    "    recent_weights_file = most_recent_weights(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder))\n",
    "    if not recent_weights_file:\n",
    "        raise Exception(\"no recent weights file were found\")\n",
    "    weights_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder, recent_weights_file)\n",
    "    print(\"loading weights file {} to resume training......\".format(weights_path))\n",
    "    net.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "    resume_epoch = last_epoch(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder))\n",
    "    print(\"resume_epoch is\", resume_epoch)\n",
    "    optimizer = optim.SGD(net.parameters(), lr=config[\"learning_rate\"], momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"], nesterov=config[\"nesterov\"])\n",
    "\n",
    "    if config[\"scheduler\"] == \"MultiStepLR\":\n",
    "        train_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, config[\"milestones\"], gamma=config[\"gamma\"]) #learning rate decay\n",
    "    elif config[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "        train_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", factor=config[\"gamma\"], patience=config[\"plateau_patience\"])\n",
    "    else:\n",
    "        print(\"No scheduler!!\")\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2e2e73-edc7-4368-910e-e6a71133dd81",
   "metadata": {
    "id": "4a2e2e73-edc7-4368-910e-e6a71133dd81"
   },
   "source": [
    "**Per-Epoch Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c463ddea-8731-4e48-8ba1-23e2961b164a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c463ddea-8731-4e48-8ba1-23e2961b164a",
    "outputId": "c22e6d65-3707-478e-ceb6-b56881f0c945",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for epoch in tqdm(range(1, config[\"num_epochs\"] + 1)):\n",
    "    if epoch > config[\"warm\"]:\n",
    "        if config[\"scheduler\"] == \"MultiStepLR\":\n",
    "            train_scheduler.step()\n",
    "        elif config[\"scheduler\"] == \"ReduceLROnPlateau\":\n",
    "            train_scheduler.step(test_loss)\n",
    "\n",
    "    if config[\"resume\"]:\n",
    "        if epoch <= resume_epoch:\n",
    "            continue\n",
    "\n",
    "    epoch_loss, train_accuracy = train_model(net, train_loader, criterion, optimizer, epoch=epoch)\n",
    "    if config[\"train_50000\"] == False:\n",
    "        test_loss, test_accuracy = eval_training(net, val_loader, criterion, epoch)\n",
    "    else:\n",
    "        test_loss, test_accuracy = eval_training(net, test_loader, criterion, epoch)\n",
    "\n",
    "    if sum_best_acc < sum(test_accuracy):\n",
    "        if weights_path:  # 모델의 용량 이슈로 새로운 best 모델이 갱신되면 이전 best 모델 삭제\n",
    "            os.remove(weights_path)\n",
    "        weights_path = checkpoint_path.format(net=config[\"model_name\"], epoch=epoch, type=\"best\")\n",
    "        print(\"saving weights file to {}\".format(weights_path))\n",
    "        torch.save(net.state_dict(), weights_path)\n",
    "        best_acc = test_accuracy\n",
    "        sum_best_acc = sum(best_acc)\n",
    "\n",
    "    early_stopping(test_loss, net)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    # wandb.log({\n",
    "    #     \"epoch\": epoch,\n",
    "    #     \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "    #     \"train_loss\": epoch_loss,\n",
    "    #     \"train top-1 accuracy\": train_accuracy[0],\n",
    "    #     \"train top-5 accuracy\": train_accuracy[1],\n",
    "    #     \"train super-class accuracy\": train_accuracy[2],\n",
    "    #     \"val_loss\": test_loss,\n",
    "    #     \"valid top-1 accuracy\": test_accuracy[0],\n",
    "    #     \"valid top-5 accuracy\": test_accuracy[1],\n",
    "    #     \"valid super-class accuracy\": test_accuracy[2]\n",
    "    # })\n",
    "\n",
    "end_time = time.time()\n",
    "# wandb.log({\"train_time\": end_time - start_time})\n",
    "\n",
    "# Result\n",
    "print(f\"Result of best {model_name} = Epoch : {epoch}   Loss : {test_loss:.4f}   Top-1 Accuracy : {best_acc[0]*100:.4f}%  Top-5 Accuracy : {best_acc[1]*100:.4f}%   Super Accuracy : {best_acc[2]*100:.4f}%   Total_Accuracy : {sum(best_acc)*100:.4f}    Time : {end_time - start_time:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p5YcRgjxvB92",
   "metadata": {
    "id": "p5YcRgjxvB92"
   },
   "source": [
    "**Best Model Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "VM1-xrA_3Ug6",
   "metadata": {
    "id": "VM1-xrA_3Ug6"
   },
   "outputs": [],
   "source": [
    "def all_accuracy(net, test_loader, device):\n",
    "    net2.eval()\n",
    "    top1_correct = 0\n",
    "    top5_correct = 0\n",
    "    total = 0\n",
    "\n",
    "    superclass_correct = 0\n",
    "    superclass_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (images, labels) in enumerate(test_loader):\n",
    "\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net2(images)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            top1_correct += torch.sum(preds == labels).item()\n",
    "\n",
    "            _, top5_preds = outputs.topk(5, 1, True, True)\n",
    "            top5_correct += torch.sum(top5_preds.eq(labels.view(-1, 1).expand_as(top5_preds))).item()\n",
    "\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Superclass accuracy\n",
    "            super_preds = torch.tensor([fine_to_superclass[p.item()] for p in preds], dtype=torch.long)\n",
    "            super_labels = torch.tensor([fine_to_superclass[t.item()] for t in labels], dtype=torch.long)\n",
    "            superclass_correct += torch.sum(super_preds == super_labels).item()\n",
    "            superclass_total += super_labels.size(0)\n",
    "\n",
    "    # 세부 클래스 및 슈퍼 클래스 정확도 계산\n",
    "    top1_acc = top1_correct / total\n",
    "    top5_acc = top5_correct / total\n",
    "    superclass_acc = superclass_correct / superclass_total\n",
    "    accuracy = [top1_acc, top5_acc, superclass_acc]\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rVneR6Xh_XfY",
   "metadata": {
    "id": "rVneR6Xh_XfY"
   },
   "outputs": [],
   "source": [
    "net2 = model\n",
    "net2.to(device)\n",
    "net2.eval()\n",
    "\n",
    "# 이 코드로 best 모델이 load 되지 않을 경우 아래 코드에서 경로 직접 지정\n",
    "recent_folder = most_recent_folder(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\"), DATE_FORMAT)\n",
    "if not recent_folder:\n",
    "    raise Exception(\"no recent folder were found\")\n",
    "\n",
    "best_weights = best_acc_weights(os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder))\n",
    "if best_weights:\n",
    "    weights_path = os.path.join(\"runs\", config[\"model_name\"], \"savepoints\", recent_folder, best_weights)\n",
    "    print('found best acc weights file:{}'.format(weights_path))\n",
    "    print('load best training file to test acc...')\n",
    "    net2.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "acc = all_accuracy(net2, test_loader, device)\n",
    "print(\"Top-1 accuracy: {:.4f}\".format(acc[0]))\n",
    "print(\"Top-5 accuracy: {:.4f}\".format(acc[1]))\n",
    "print(\"Super-Class accuracy: {:.4f}\".format(acc[2]))\n",
    "\n",
    "# wandb.log({\n",
    "#     \"Test Top-1 accuracy\": acc[0],\n",
    "#     \"Test Top-5 accuracy\": acc[1],\n",
    "#     \"Test Super-Class accuracy\": acc[2],\n",
    "#     \"Total Score\": sum(acc)\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3exQ4RCI34-1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3exQ4RCI34-1",
    "outputId": "6c819a3d-91ce-48d0-e7f1-495bed210b91"
   },
   "outputs": [],
   "source": [
    "# # 위 코드에서 에러가 발생해 경로 지정이 제대로 되지 않을 경우 사용\n",
    "\n",
    "# net2 = model\n",
    "# net2.to(device)\n",
    "# net2.load_state_dict(torch.load(\"./runs/MyPyramidNet/savepoints/Saturday_12_October_2024_16h_08m_28s/MyPyramidNet-194-best.pth\"))\n",
    "\n",
    "# acc = all_accuracy(net2, test_loader, device)\n",
    "# print(\"Top-1 accuracy: {:.4f}\".format(acc[0]))\n",
    "# print(\"Top-5 accuracy: {:.4f}\".format(acc[1]))\n",
    "# print(\"Super-Class accuracy: {:.4f}\".format(acc[2]))\n",
    "# # wandb.log({\n",
    "# #     \"Test Top-1 accuracy\": acc[0],\n",
    "# #     \"Test Top-5 accuracy\": acc[1],\n",
    "# #     \"Test Super-Class accuracy\": acc[2],\n",
    "# #     \"Total Score\": sum(acc)\n",
    "# #     })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc7f408",
   "metadata": {
    "id": "cdc7f408"
   },
   "source": [
    "**Data Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8404d10-a77a-4544-96d1-70cb306e2a05",
   "metadata": {
    "id": "d8404d10-a77a-4544-96d1-70cb306e2a05"
   },
   "outputs": [],
   "source": [
    "# import wandb.sklearn\n",
    "\n",
    "# classes = train_val_data.classes\n",
    "# coarse_classes = [\n",
    "#     'aquatic mammals', 'fish', 'flowers', 'food containers', 'fruit and vegetables', 'household electrical devices',\n",
    "#     'household furniture', 'insects', 'large carnivores', 'large man-made outdoor things',\n",
    "#     'large natural outdoor scenes', 'large omnivores and herbivores', 'medium-sized mammals',\n",
    "#     'non-insect invertebrates', 'people', 'reptiles', 'small mammals', 'trees', 'vehicles 1', 'vehicles 2'\n",
    "# ]\n",
    "\n",
    "# y_pred = []\n",
    "# y_true = []\n",
    "# y_probs = []\n",
    "\n",
    "# for x, y in torch.utils.data.DataLoader(dataset=test_data, batch_size=config[\"batch_size\"]):\n",
    "#     x = x.to(device)\n",
    "#     y = y.to(device)\n",
    "#     z = net2(x)\n",
    "#     _, yhat = torch.max(z, 1)\n",
    "#     probs = z.softmax(dim=1).cpu().detach().numpy()\n",
    "\n",
    "#     pred = yhat.data.cpu().numpy()\n",
    "    \n",
    "#     super_pred = [coarse_classes[fine_to_superclass[p]] for p in pred]\n",
    "#     y_pred.extend(super_pred)\n",
    "\n",
    "#     labels = y.data.cpu().numpy()\n",
    "#     super_labels = [coarse_classes[fine_to_superclass[l]] for l in labels]\n",
    "#     y_true.extend(super_labels)\n",
    "\n",
    "#     y_probs.extend(probs)\n",
    "    \n",
    "# cf_matrix = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "\n",
    "# df_cm = pd.DataFrame(cf_matrix, index=coarse_classes,\n",
    "#                      columns=coarse_classes)\n",
    "\n",
    "# plt.figure(figsize=(10, 7))\n",
    "# sns.heatmap(df_cm, annot=True, fmt='.2f', cmap='Blues')\n",
    "# plt.title(f'Confusion Matrix of {config[\"model_name\"]} (CIFAR-100 Superclass)')\n",
    "# plt.xlabel('Predicted')\n",
    "# plt.ylabel('Real')\n",
    "\n",
    "# conf_matrix_img = wandb.Image(plt.gcf())\n",
    "# plt.close()\n",
    "\n",
    "# # class_report = classification_report(y_true, y_pred, labels=coarse_classes, zero_division=0)\n",
    "\n",
    "# # plt.figure(figsize=(10, 7))\n",
    "# # plt.text(0.01, 0.05, str(class_report), {'fontsize': 12}, fontproperties='monospace')  # monospaced font\n",
    "# # plt.axis('off')  # 축 제거\n",
    "# # plt.tight_layout()\n",
    "\n",
    "# # class_report_img = wandb.Image(plt.gcf())\n",
    "# # plt.close()\n",
    "\n",
    "# wandb.log({\n",
    "#     \"Confusion Matrix\": conf_matrix_img,\n",
    "#     \"Classification Report\": class_report_img\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9076a5-82f4-4ec9-9903-5f20c093aa6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03e76e1-9b06-45a3-9abc-274949428750",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
